{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build predictive model of genes -> STP\n",
    "\n",
    "\n",
    "1) Use ABA VISp,Hipp gene expression and STP data to fit RF for STP. \n",
    "\n",
    "2) test predictions - crossvalidation, test on hippocampal data, test on exc. neurons data?\n",
    "\n",
    "3) feature selection - compare datasets? - compare with Paul analysis\n",
    "\n",
    "4) stp factors genes - find better factors using interactions networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) fit RF ABA ge -> STP : Hippocampus, VISp\n",
    "\n",
    "a. select gene sets\n",
    "\n",
    "b. load gene expression data Hipp, Ent \n",
    "\n",
    "c. hippocampus STP data \n",
    "\n",
    "d. combine STP and gene expression data for Hippocampus\n",
    "\n",
    "e.load gene expression data VISp\n",
    "\n",
    "f.  coretex STP data\n",
    "\n",
    "g. combine STP and gene expression data for VISp cortex\n",
    "\n",
    "h. fit RF from sklit-learn\n",
    "\n",
    "f. test predictions :  cross validation - part of genes, part of STP labels, genes from different cortical areas, genes from hippocampus (Harris 2018)\n",
    "\n",
    "g. compare gene sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "do_laptop=1   # Laptop or desctop version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a. select gene sets - take Paul 2017 best pre-synaptic + CAMs\n",
    "# Paul 2017\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "##\n",
    "##  SELECT GENE SET\n",
    "##\n",
    "###\n",
    "gs = '15656' #'5188' #'1512'#'5188' #'1512'#'219' #'1512' #'219'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prot_list(df_stp_ds, name):\n",
    "    pre_main=df_stp_ds.loc[:,['Protein','Gene rat']]\n",
    "    pre_main_nm = pre_main.loc[pre_main.loc[:,'Protein'].str.contains(':',na=False),'Protein']\n",
    "\n",
    "\n",
    "    pre_main_nm=pre_main_nm.reset_index()\n",
    "    i = pre_main_nm.loc[pre_main_nm.loc[:,'Protein']==name,:]\n",
    "    i1 = i.loc[:,'index']\n",
    "    i2 = pre_main_nm.loc[i.index[0]+1,'index']\n",
    "\n",
    "    Cas = df_stp_ds.loc[np.arange(i1.values[0]+1,i2),'Gene rat']\n",
    "    Cas=Cas.loc[Cas.loc[:].isna()==False]\n",
    "    Cas = Cas.str.split('\\W+',expand=True).iloc[:,0].values.tolist()\n",
    "    return Cas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d8 = '~/Documents/references/transcriptomes to STP/'\n",
    "if do_laptop:\n",
    "    d8 = '/Users/stepaniu/Documents/jan_2020/'\n",
    "\n",
    "df_stp_ds = pd.read_excel(d8+'Datasets_STP_hippocampus.xlsx', sheet_name='Sheet8')\n",
    "\n",
    "Cas = get_prot_list(df_stp_ds, 'Calcium sensors:')\n",
    "AZ2 = get_prot_list(df_stp_ds, 'active zone:')\n",
    "\n",
    "print(Cas)\n",
    "print(AZ2)\n",
    "\n",
    "\n",
    "d6=''\n",
    "if do_laptop:\n",
    "    d6 = '/Users/stepaniu/Documents/references/lists_of_proteins_2/Analysis_of_proteins/genetic_types/'\n",
    "\n",
    "fn6 = 'Paul_2017.xlsx'\n",
    "df_Paul_gs=pd.read_excel(d6 + fn6,header=2)\n",
    "\n",
    "d7 = '~/Documents/references/transcriptomes to STP/transcriptomes/Linnerson_2018/'\n",
    "fn7 = 'GO: synaptic vesicle cycle, mouse.xlsx'\n",
    "if do_laptop:\n",
    "    d7 = '/Users/stepaniu/Documents/jan_2020/'\n",
    "    fn7 = 'GO_ synaptic vesicle cycle, mouse.xlsx'\n",
    "df_go_priming = pd.read_excel(d7+fn7)\n",
    "SV_cycle=df_go_priming.loc[:,'Gene/product label'].value_counts().index.tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(df_Paul_gs.head())\n",
    "\n",
    "#select presynaptic :\n",
    "\n",
    "if (gs=='1512')|(gs=='5188')|(gs=='15656'):\n",
    "    pre = ['Netrin-Unc5-Slit-Robo',\n",
    "    'Nrxn+Nlgn+Nphx+Dystroglycan+Cbln',\n",
    "    'neuronal_IgCAMs',\n",
    "    'Contactins',\n",
    "    'Semaphorin+Plexin',\n",
    "    'LRR+Slitrk+Elfn+Lphn',\n",
    "    'Eph+EphR',\n",
    "    'Calcium channel regulatory subunits',\n",
    "    'Calcium channel poreforming subunits',\n",
    "    'Calcium channel gamma/TARP subunits',\n",
    "    'Orphan GPCRs (based off HGNC)',\n",
    "    'Metabotropic neurotransmitter receptors',\n",
    "    'Neuropeptide receptors',\n",
    "    'Trimeric G_protein_alpha',\n",
    "    'Trimeric G_protein_beta',\n",
    "    'Trimeric G_protein_gamma',\n",
    "    'Rho-GEF',\n",
    "    'Dock-GEF',\n",
    "    'Dock + Rho-GEF',\n",
    "    'Complexin-Syt',\n",
    "    'SNAP-complex',\n",
    "    'Syntaxin',\n",
    "    'RIM-proteins/Active zone',\n",
    "    'bHLH',\n",
    "    'Nuclear receptors with C4 zinc fingers',\n",
    "    'Homeo domain factors',\n",
    "    'High-mobility group (HMG) domain factors',\n",
    "    'C2H2 zinc finger factors',\n",
    "    'bZIP',\n",
    "    'ARID domain factors',\n",
    "    'Runt domain factors',\n",
    "    'Other C4 zinc finger-type',\n",
    "    'C2HC zinc finger factors',\n",
    "    'SMAD/NF-1 DNA-binding domain factors',\n",
    "    'TATA-binding proteins',\n",
    "    'Rel homology region (RHR) factors',\n",
    "    'Tryptophan cluster factors',\n",
    "    'MADS box factors',\n",
    "    'Cold-shock domain factors',\n",
    "    'CXXC zinc finger factors',\n",
    "    'Leucine-rich repeat flightless-interacting proteins',\n",
    "    'STAT domain factors',\n",
    "    'Grainyhead domain factors'\n",
    "    ]\n",
    "\n",
    "if gs=='219':    \n",
    "    pre = ['Calcium channel regulatory subunits', 'Netrin-Unc5-Slit-Robo',\n",
    "          'Nrxn+Nlgn+Nphx+Dystroglycan+Cbln', 'neuronal_IgCAMs', 'Contactins',\n",
    "          'Metabotropic neurotransmitter receptors',      'Dock + Rho-GEF', 'Complexin-Syt']\n",
    "\n",
    "#pre = ['Syntaxin', 'RIM-proteins/Active zone', 'Dock-GEF','SNAP-complex' ]\n",
    "\n",
    "#pre = ['RIM-proteins/Active zone'] \n",
    "\n",
    "#pre = []\n",
    "\n",
    "\n",
    "Elfn = ['Elfn1', 'Elfn2']\n",
    "\n",
    "pregs = []\n",
    "for i in pre:\n",
    "    in_i = (df_Paul_gs.loc[:,'Gene Set']==i)\n",
    "    n_in_i = df_Paul_gs.loc[in_i,'Number of genes'].astype(int).values[0]\n",
    "    pri = df_Paul_gs.loc[in_i,df_Paul_gs.columns[4:4+n_in_i]]\n",
    "    pregs = pregs + pri.values.tolist()[0]\n",
    "    \n",
    "#pregs = pregs +  Elfn  \n",
    "if (gs=='1512')|(gs=='5188')|(gs=='15656'):\n",
    "    pregs = pregs + SV_cycle\n",
    "    \n",
    "pregs = pregs +Cas + AZ2 \n",
    "#pregs = pregs +Cas \n",
    "\n",
    "## best gs\n",
    "#pregs = best_pre\n",
    "\n",
    "if (gs=='1512')|(gs=='5188')|(gs=='15656'):\n",
    "    post = ['Netrin-Unc5-Slit-Robo',\n",
    "    'Nrxn+Nlgn+Nphx+Dystroglycan+Cbln',\n",
    "    'neuronal_IgCAMs',\n",
    "    'Contactins',\n",
    "    'Semaphorin+Plexin',\n",
    "    'LRR+Slitrk+Elfn+Lphn',\n",
    "    'Eph+EphR',\n",
    "    'Calcium channel regulatory subunits',\n",
    "    'Calcium channel poreforming subunits',\n",
    "    'Calcium channel gamma/TARP subunits',\n",
    "    'AMPA+TARP',\n",
    "    'Orphan GPCRs (based off HGNC)',\n",
    "    'Metabotropic neurotransmitter receptors',\n",
    "    'Neuropeptide receptors']\n",
    "\n",
    "\n",
    "#post = ['Netrin-Unc5-Slit-Robo',\n",
    "#'Nrxn+Nlgn+Nphx+Dystroglycan+Cbln',\n",
    "#'neuronal_IgCAMs',\n",
    "#'Contactins',\n",
    "#'Semaphorin+Plexin',\n",
    "#'LRR+Slitrk+Elfn+Lphn',\n",
    "#'Eph+EphR']\n",
    "\n",
    "if gs=='219':\n",
    "     post = ['Netrin-Unc5-Slit-Robo',\n",
    "            'Nrxn+Nlgn+Nphx+Dystroglycan+Cbln', 'neuronal_IgCAMs', 'Contactins']\n",
    "\n",
    "\n",
    "#post = [ 'Nrxn+Nlgn+Nphx+Dystroglycan+Cbln']\n",
    "\n",
    "#post=[]\n",
    "\n",
    "\n",
    "postgs = []\n",
    "for i in post:\n",
    "    in_i = (df_Paul_gs.loc[:,'Gene Set']==i)\n",
    "    n_in_i = df_Paul_gs.loc[in_i,'Number of genes'].astype(int).values[0]\n",
    "    posti = df_Paul_gs.loc[in_i,df_Paul_gs.columns[4:4+n_in_i]]\n",
    "    postgs = postgs + posti.values.tolist()[0]\n",
    "    \n",
    "postgs = postgs +  Elfn    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SET FLAG : do_select_best_genes_from_previous_RF_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### fi2 = results of previous search\n",
    "#fi2.to_excel('best_genes_1691_RF.xlsx')\n",
    "#fi2.to_excel('best_genes_280_RF_2.xlsx')\n",
    "#fi2.to_excel('best_genes_280_RF_hip.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_select_best_genes_from_previous_RF_run=0\n",
    "if do_select_best_genes_from_previous_RF_run==1:\n",
    "    # fi2 = results of previous search\n",
    "    fi2  = pd.read_excel('best_genes_1691_RF.xlsx')\n",
    "    fi2  = pd.read_excel('best_genes_280_RF.xlsx')\n",
    "    fi2  = pd.read_excel('best_genes_280_RF.xlsx')\n",
    "    fi2  = pd.read_excel('best_genes_280_RF_hip.xlsx')\n",
    "\n",
    "    fi2 = pd.read_excel('best_genes_hipp.xlsx')\n",
    "    fi3 = pd.read_excel('best_genes_visp.xlsx')\n",
    "    #\n",
    "    nbest = 10\n",
    "    bestn = fi2.iloc[0:nbest,:]\n",
    "    #best_pre = bestn.index[bestn.loc[:,'compartment']=='pre'].values.tolist()\n",
    "    #best_post = bestn.index[bestn.loc[:,'compartment']=='post'].values.tolist()\n",
    "    \n",
    "    best_pre = bestn.loc[bestn.loc[:,'compartment']=='pre','Unnamed: 0'].values.tolist()\n",
    "    best_post = bestn.loc[bestn.loc[:,'compartment']=='post','Unnamed: 0'].values.tolist()\n",
    "    \n",
    "    print('best presynaptic genes')\n",
    "    print(best_pre)\n",
    "    print('best postsynaptic genes')\n",
    "    print(best_post)\n",
    "    \n",
    "    ## best gs\n",
    "    ## best gs\n",
    "    pregs = best_pre\n",
    "    postgs = best_post\n",
    "\n",
    "print('postsynaptic')\n",
    "print(postgs)\n",
    "print(len(postgs))\n",
    "\n",
    "print('presynaptic')\n",
    "print(pregs)\n",
    "print(len(pregs))\n",
    "print(len(postgs))\n",
    "\n",
    "pregs =np.unique(pregs).tolist()\n",
    "postgs =np.unique(postgs).tolist()\n",
    "\n",
    "import pickle\n",
    "RF_file = open('RF_290_hipp_gs.pickle', mode='wb')\n",
    "pickle.dump([pregs, postgs],RF_file)\n",
    "\n",
    "do_visp_features=0\n",
    "if do_visp_features==1:\n",
    "    #pickle_in = open('RF_290_hipp.pickle',\"rb\")\n",
    "    RF_file = open('RF_290_visp_gs.pickle', mode='rb')\n",
    "    \n",
    "    gs_visp = pickle.load(RF_file)\n",
    "    pregs =gs_visp[0] \n",
    "    postgs=gs_visp[1]\n",
    "    \n",
    "    \n",
    "##from mutual_info import entropy\n",
    "##e = entropy(X, k=1)\n",
    "#np.finfo(float).eps\n",
    "#print(len(pregs))\n",
    "#print(np.unique(pregs).shape)\n",
    "#print(len(postgs))\n",
    "#print(np.unique(postgs).shape)\n",
    "#len(pregs)+len(postgs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfgs=pd.DataFrame(pregs+postgs)\n",
    "if (gs=='1512')|(gs=='5188')|(gs=='15656'):\n",
    "    dfgs.to_excel('gene_set_all_1512.xlsx')\n",
    "if gs=='219':\n",
    "    dfgs.to_excel('gene_set_all_219.xlsx')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load scVI latent factors data (cortex+hippocampus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ABA smart-seq?\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "do_laptop=1\n",
    "\n",
    "#d4='~/Documents/references/transcriptomes to STP/transcriptomes/ABA/mouse_VISp_gene_expression_matrices_2018-06-14/'\n",
    "if do_laptop:\n",
    "    d4 = '~/Documents/references/lists_of_proteins_2/Analysis_of_proteins/genetic_types/transcriptomic_interneurons/'\n",
    "else:\n",
    "    d4='~/Documents/references/transcriptomes to STP/transcriptomes/ABA/mouse_VISp_gene_expression_matrices_2018-06-14/'\n",
    "    #d4 = '/content/drive/My Drive/Colab Notebooks/scVI/'\n",
    "    \n",
    "\n",
    "fn='mouse_VISp_2018-06-14_samples-columns.csv'\n",
    "df_aba_vis_c=pd.read_csv(d4+fn)\n",
    "print(df_aba_vis_c.columns)\n",
    "print(df_aba_vis_c.head())\n",
    "\n",
    "fn = 'mouse_VISp_2018-06-14_genes-rows.csv'\n",
    "df_aba_g=pd.read_csv(d4+fn)\n",
    "print(df_aba_g.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_glut_l23 = (df_aba_vis_c.loc[:,'class']=='Glutamatergic')&(df_aba_vis_c.loc[:,'brain_subregion'].isin(['L2/3','L4','L5']))\n",
    "sum(in_glut_l23) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_aba_vis_c.index))\n",
    "\n",
    "in_glut_l23 = (df_aba_vis_c.loc[:,'class']=='Glutamatergic') #&(df_aba_vis_c.loc[:,'brain_subregion'].isin(['L2/3','L4','L5']))\n",
    "\n",
    "in_gaba = df_aba_vis_c.loc[:,'class']=='GABAergic'\n",
    "print(sum(in_gaba))\n",
    "df_aba_vis_in_c=df_aba_vis_c.loc[in_gaba,:]\n",
    "df_aba_vis_l23glu_c=df_aba_vis_c.loc[in_glut_l23,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aba_vis_in_c.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aba_vis_l23glu_c.columns==df_aba_vis_in_c.columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "#import loompy \n",
    "#import anndata\n",
    "if do_laptop:\n",
    "    os.chdir('/Users/stepaniu/scVI/tests/notebooks')\n",
    "\n",
    "print(os.getcwd())\n",
    "#os.listdir()\n",
    "\n",
    "if do_laptop:\n",
    "    from scvi.dataset import LoomDataset, CsvDataset, Dataset10X\n",
    "else:\n",
    "    from scvi.dataset import DownloadableAnnDataset\n",
    "\n",
    "    d4='/home/stepaniu/Documents/references/transcriptomes to STP/scVI_data'\n",
    "\n",
    "#df_aba_ad.write('mouse_VISp_2018-06-14_exon-matrix_GABAergic.h5ad')\n",
    "aba_vis_in = DownloadableAnnDataset(d4+\"/data/mouse_VISp_2018-06-14_exon-matrix_GABAergic.h5ad\") \n",
    "\n",
    "#aba_vis_l23glu = AnnDataset(\"mouse_VISp_2018-06-14_exon-matrix_L23_Glutamatergic_hda.h5ad\", new_n_genes = 45768,\n",
    "#                               save_path = save_path) \n",
    "#aba_vis_l23glu = AnnDataset(\"mouse_VISp_2018-06-14_exon-matrix_L2345_Glutamatergic_hda.h5ad\", new_n_genes = 45768,\n",
    "#                               save_path = save_path) \n",
    "\n",
    "aba_vis_l23glu = DownloadableAnnDataset(d4+\"/mouse_VISp_2018-06-14_exon-matrix_L23456_Glutamatergic_hda.h5ad\") \n",
    "\n",
    "\n",
    "#aba_vis_in.gene_names = df_aba_g.loc[:,'gene_symbol'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "\n",
    "#if do_laptop:\n",
    "#    os.chdir('/Users/stepaniu/scVI/tests/notebooks')\n",
    "\n",
    "#print(os.getcwd())\n",
    "##os.listdir()\n",
    "\n",
    "save_path = '' #'data/' #'' #'data/'\n",
    "\n",
    "#d4='~/Documents/references/transcriptomes to STP/transcriptomes/ABA/mouse_VISp_gene_expression_matrices_2018-06-14/'\n",
    "if do_laptop:\n",
    "    d4 = '~/Documents/references/lists_of_proteins_2/Analysis_of_proteins/genetic_types/transcriptomic_interneurons/'\n",
    "else:\n",
    "    d4='~/Documents/references/transcriptomes to STP/transcriptomes/ABA/mouse_VISp_gene_expression_matrices_2018-06-14/'\n",
    "\n",
    "fn='mouse_VISp_2018-06-14_samples-columns.csv'\n",
    "df_aba_vis_c=pd.read_csv(d4+fn)\n",
    "print(df_aba_vis_c.columns)\n",
    "print(df_aba_vis_c.head())\n",
    "\n",
    "fn = 'mouse_VISp_2018-06-14_genes-rows.csv'\n",
    "df_aba_g=pd.read_csv(d4+fn)\n",
    "print(df_aba_g.head())\n",
    "print(len(df_aba_vis_c.index))\n",
    "\n",
    "in_glut_l23 = (df_aba_vis_c.loc[:,'class']=='Glutamatergic') #&(df_aba_vis_c.loc[:,'brain_subregion'].isin(['L2/3','L4','L5']))\n",
    "\n",
    "in_gaba = df_aba_vis_c.loc[:,'class']=='GABAergic'\n",
    "print(sum(in_gaba))\n",
    "df_aba_vis_in_c=df_aba_vis_c.loc[in_gaba,:]\n",
    "df_aba_vis_l23glu_c=df_aba_vis_c.loc[in_glut_l23,:]\n",
    "\n",
    "###\n",
    "print('loading gene expression...')\n",
    "\n",
    "d14 = '/home/stepaniu/Documents/references/transcriptomes to STP/scVI_data/'\n",
    "if do_laptop:\n",
    "    d14 = '/Users/stepaniu/Documents/jan_2020/'\n",
    "    from scvi.dataset import LoomDataset, CsvDataset, Dataset10X, AnnDataset\n",
    "else:\n",
    "    from scvi.dataset import DownloadableAnnDataset\n",
    "\n",
    "if do_laptop:    \n",
    "##df_aba_ad.write('mouse_VISp_2018-06-14_exon-matrix_GABAergic.h5ad')\n",
    "    aba_vis_in = AnnDataset(\"mouse_VISp_2018-06-14_exon-matrix_GABAergic.h5ad\", new_n_genes = 45768,\n",
    "                                   save_path = save_path) \n",
    "else:\n",
    "    aba_vis_in = DownloadableAnnDataset(d14+\"data/mouse_VISp_2018-06-14_exon-matrix_GABAergic.h5ad\") \n",
    "\n",
    "if do_laptop:\n",
    "    #aba_vis_l23glu = AnnDataset(\"mouse_VISp_2018-06-14_exon-matrix_L23_Glutamatergic_hda.h5ad\", new_n_genes = 45768,\n",
    "    #                               save_path = save_path) \n",
    "    #aba_vis_l23glu = AnnDataset(\"mouse_VISp_2018-06-14_exon-matrix_L2345_Glutamatergic_hda.h5ad\", new_n_genes = 45768,\n",
    "    #                               save_path = save_path) \n",
    "\n",
    "    aba_vis_l23glu = AnnDataset(\"mouse_VISp_2018-06-14_exon-matrix_L23456_Glutamatergic_hda.h5ad\", new_n_genes = 45768,\n",
    "                                   save_path = '') \n",
    "else:\n",
    "    aba_vis_l23glu = DownloadableAnnDataset(d14+\"mouse_VISp_2018-06-14_exon-matrix_L23456_Glutamatergic_hda.h5ad\") \n",
    "\n",
    "diff_order = sum(aba_vis_in.gene_names[0:].astype(int) - np.arange(len(aba_vis_in.gene_names)))\n",
    "print(diff_order)\n",
    "\n",
    "aba_vis_in.gene_names = df_aba_g.loc[:,'gene_symbol'].values\n",
    "\n",
    "aba_vis_l23glu.gene_names = df_aba_g.loc[:,'gene_symbol'].values\n",
    "\n",
    "\n",
    "\n",
    "print(len(aba_vis_in.gene_names))\n",
    "print(len(aba_vis_l23glu.gene_names))\n",
    "len(list(set(aba_vis_l23glu.gene_names).difference(set(aba_vis_in.gene_names))))\n",
    "print(len(df_aba_vis_l23glu_c.index))\n",
    "print(len(df_aba_vis_l23glu_c.index))\n",
    "print(len(df_aba_vis_in_c.index))\n",
    "\n",
    "#from scvi.dataset.dataset import GeneExpressionDataset\n",
    "#if do_laptop:\n",
    "#    vis_dat = GeneExpressionDataset.concat_datasets(aba_vis_in,aba_vis_l23glu)    \n",
    "#else:    \n",
    "#    vis_dat  = GeneExpressionDataset()\n",
    "#    vis_dat.populate_from_datasets([aba_vis_in,aba_vis_l23glu]) \n",
    "   \n",
    "\n",
    "#print((vis_dat.batch_indices==0).sum())\n",
    "#(vis_dat.batch_indices==1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#if do_laptop:\n",
    "#    os.chdir('/Users/stepaniu/scVI/tests/notebooks')\n",
    "\n",
    "print(os.getcwd())\n",
    "#os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aba_vis_in.gene_names = df_aba_g.loc[:,'gene_symbol'].values\n",
    "#aba_vis_l23glu.gene_names = df_aba_g.loc[:,'gene_symbol'].values\n",
    "aba_vis_in._X.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#import os\n",
    "#print(os.getcwd())\n",
    "#os.listdir()\n",
    "\n",
    "#from scvi.dataset import LoomDataset, CsvDataset, Dataset10X, AnnDataset\n",
    "\n",
    "d7 = '/home/stepaniu/Documents/references/transcriptomes to STP/transcriptomes/ABA_2019/'\n",
    "#d7 = '/content/drive/My Drive/Colab Notebooks/ABA_2019/'\n",
    "\n",
    "#aba_vis_in = AnnDataset(\"ABA_2019_transcriptome.h5ad\", new_n_genes = 45768,\n",
    "#                               save_path = d7) \n",
    "aba_hipp_ent = DownloadableAnnDataset(d7+\"ABA_2019_transcriptome.h5ad\") \n",
    "\n",
    "\n",
    "obs = pd.read_excel(d7 +'ABA_2019_obs.xlsx') # samples\n",
    "var = pd.read_excel(d7 + 'ABA_2019_var.xlsx') # gene names\n",
    "#obs = aba_vis_in._obs # samples\n",
    "#var = aba_vis_in._var # gene names\n",
    "\n",
    "#df_aba_g = pd.read_csv(d7 + 'medians.csv')\n",
    "df_aba_ch = pd.read_csv(d7 + 'sample_annotations.csv')\n",
    "\n",
    "#in_gaba = df_aba_ch.loc[df_aba_ch.loc[:,'sample_name'].isin(obs.iloc[:,0]),['sample_name', 'class_label']]\n",
    "in_gaba = df_aba_ch.loc[df_aba_ch.loc[:,'sample_name'].isin(obs.loc[:,'samples']),['sample_name', 'class_label']]\n",
    "in_gaba = in_gaba.reset_index().set_index('sample_name',drop=True)\n",
    "in_gaba = in_gaba.loc[obs.loc[:,'samples'],:]\n",
    "in_gaba = in_gaba.reset_index().reset_index().set_index('sample_name',drop=True)\n",
    "in_neurons = in_gaba.loc[in_gaba.loc[:,'class_label'].isin(['GABAergic','Glutamatergic']),:].reset_index().set_index('level_0',drop=True)\n",
    "\n",
    "#obs = in_gaba.loc[:,'sample_name'].reset_index()\n",
    "obs = in_gaba.reset_index().loc[:,'sample_name']\n",
    "#var=var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "#os.listdir()\n",
    "\n",
    "if do_laptop:\n",
    "    from scvi.dataset import LoomDataset, CsvDataset, Dataset10X, AnnDataset\n",
    "else:\n",
    "    from scvi.dataset import DownloadableAnnDataset\n",
    "\n",
    "d_aba  ='/home/stepaniu/Documents/references/transcriptomes to STP/transcriptomes/ABA_2019/'\n",
    "if do_laptop:\n",
    "    d_aba  ='/Users/stepaniu/Documents/jan_2020/ABA_2019/'\n",
    "\n",
    "df_aba_g = pd.read_csv(d_aba + 'medians.csv')\n",
    "df_aba_c = pd.read_csv(d_aba + 'sample_annotations.csv')\n",
    "\n",
    "\n",
    "d7 = '/home/stepaniu/Documents/references/transcriptomes to STP/transcriptomes/ABA_2019/'\n",
    "if do_laptop:\n",
    "    d7  ='/Users/stepaniu/Documents/jan_2020/ABA_2019/'\n",
    "\n",
    "if do_laptop:    \n",
    "    aba_hipp_ent = AnnDataset(\"ABA_2019_transcriptome.h5ad\", new_n_genes = 45768,\n",
    "                             save_path = d7) \n",
    "else:     \n",
    "    aba_hipp_ent = DownloadableAnnDataset(d7 +\"ABA_2019_transcriptome.h5ad\") \n",
    "\n",
    "obs = pd.read_excel(d7 +'ABA_2019_obs.xlsx') # samples\n",
    "var = pd.read_excel(d7 + 'ABA_2019_var.xlsx') # gene names\n",
    "#obs = aba_vis_in._obs # samples\n",
    "#var = aba_vis_in._var # gene names\n",
    "\n",
    "#df_aba_g = pd.read_csv(d7 + 'medians.csv')\n",
    "df_aba_ch = pd.read_csv(d7 + 'sample_annotations.csv')\n",
    "\n",
    "#in_gaba = df_aba_ch.loc[df_aba_ch.loc[:,'sample_name'].isin(obs.iloc[:,0]),['sample_name', 'class_label']]\n",
    "in_gaba = df_aba_ch.loc[df_aba_ch.loc[:,'sample_name'].isin(obs.loc[:,'samples']),['sample_name', 'class_label']]\n",
    "in_gaba = in_gaba.reset_index().set_index('sample_name',drop=True)\n",
    "in_gaba = in_gaba.loc[obs.loc[:,'samples'],:]\n",
    "in_gaba = in_gaba.reset_index().reset_index().set_index('sample_name',drop=True)\n",
    "in_neurons = in_gaba.loc[in_gaba.loc[:,'class_label'].isin(['GABAergic','Glutamatergic']),:].reset_index().set_index('level_0',drop=True)\n",
    "\n",
    "#obs = in_gaba.loc[:,'sample_name'].reset_index()\n",
    "obs = in_gaba.reset_index().loc[:,'sample_name']\n",
    "#var=var\n",
    "\n",
    "\n",
    "#in_gaba = df_aba_c.loc[df_aba_c.loc[:,'sample_name'].isin(obs.loc[:,'samples']),['sample_name', 'class_label']]\n",
    "#in_gaba = in_gaba.reset_index().set_index('sample_name',drop=True)\n",
    "#in_gaba = in_gaba.loc[obs.loc[:,'samples'],:]\n",
    "#in_gaba = in_gaba.reset_index().reset_index().set_index('sample_name',drop=True)\n",
    "#in_gaba = in_gaba.loc[in_gaba.loc[:,'class_label']=='GABAergic',:].reset_index().set_index('level_0',drop=True)\n",
    "\n",
    "#aba_vis_in.gene_names = var.iloc[:,0].values\n",
    "aba_hipp_ent.gene_names = var.loc[:,'genes'].values\n",
    "#vis_dat = aba_vis_in\n",
    "#aba_vis_in = []\n",
    "\n",
    "#in_ds = df_aba_c.loc[df_aba_c.loc[:,'sample_name'].isin(obs.loc[:,'samples']),['sample_name', 'class_label']]\n",
    "#in_ds = in_ds.reset_index().set_index('sample_name',drop=True)\n",
    "#in_ds = in_ds.loc[obs.loc[:,'samples'],:]\n",
    "#in_ds = in_ds.reset_index().reset_index().set_index('sample_name',drop=True)\n",
    "##in_ds\n",
    "\n",
    "#df_aba_vis_in_c = df_aba_c.loc[in_ds.loc[:,'index'],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_g=df_aba_g.loc[:,'Unnamed: 0']==var.loc[:,'genes']\n",
    "same_g.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams[\"pdf.fonttype\"] = 42\n",
    "matplotlib.rcParams[\"ps.fonttype\"] = 42\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "#import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "import pandas as pd\n",
    "\n",
    "#do_laptop = False\n",
    "if do_laptop:\n",
    "    import umap\n",
    "    umap.__version__='0.3.7'\n",
    "    import scanpy as sc\n",
    "    import louvain\n",
    "    from umap.umap_ import UMAP\n",
    "#else:\n",
    "    #import umap\n",
    "    #umap.__version__='0.3.7'\n",
    "    #import scanpy as sc\n",
    "    #import louvain\n",
    "    #from umap.umap_ import UMAP\n",
    "    \n",
    "    \n",
    "use_cuda = True\n",
    "from scvi.dataset.dataset import GeneExpressionDataset\n",
    "from scvi.inference import UnsupervisedTrainer\n",
    "from scvi.models import SCANVI, VAE\n",
    "\n",
    "\n",
    "save_path2=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select genes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stype = ['pre']*len(pregs) + ['post']*len(postgs)\n",
    "stype = pd.DataFrame(stype,columns  = ['type'])\n",
    "dfgs = pd.concat([dfgs, stype],axis=1)\n",
    "dfgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "sc.settings.verbosity = 3\n",
    "sc.settings.set_figure_params(dpi=80, color_map='Greys')\n",
    "sc.logging.print_versions()\n",
    "results_file = 'X_scvi.h5ad'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy.api as sca\n",
    "sca.pp.filter(adt1,min_cells=3)\n",
    "#sc.pp.filter(adt1,min_genes=10)\n",
    "\n",
    "#all_data  =[aba_vis_in, aba_vis_l23glu, aba_hipp_ent]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gs=='1512':\n",
    "    dfgs.to_excel('gene_set_all_1512.xlsx')\n",
    "if gs=='219':\n",
    "    dfgs.to_excel('gene_set_all_219.xlsx')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pregs+postgs))\n",
    "print(len(list(set(pregs+postgs))))\n",
    "gene_set = list(set(pregs+postgs))\n",
    "# if (gs=='5188')|(gs=='15656'):\n",
    "#     igenes = pd.Series(aba_vis_in.gene_names).isin(gene_set).values\n",
    "#     gene_names2 = aba_vis_in.gene_names[igenes==False]\n",
    "    \n",
    "#     #gene_set = gene_set + gene_names2[np.arange(0,5188-igenes.sum())].tolist()\n",
    "#     gene_set = aba_vis_in.gene_names[igenes].tolist() + gene_names2[np.arange(0,5188-igenes.sum())].tolist()\n",
    "len(gene_set)    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "l1=len(gene_names2[np.arange(0,5188-igenes.sum())].tolist())\n",
    "l2=len(gene_set)\n",
    "print(l1)\n",
    "print(l2)\n",
    "print(l1+l2)\n",
    "print(5188-1525)\n",
    "print(aba_vis_in.gene_names.shape)\n",
    "print(np.unique(aba_vis_in.gene_names).shape)\n",
    "print(len(aba_vis_in.gene_names[igenes]))\n",
    "\n",
    "#len(gene_names2[np.arange(0,5188-igenes.sum())].tolist())+len(gene_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_filter_gene_set = False\n",
    "if do_filter_gene_set:\n",
    "    igenes = pd.Series(aba_vis_in.gene_names).isin(gene_set).values\n",
    "\n",
    "    aba_vis_in.gene_names = aba_vis_in.gene_names[igenes]\n",
    "    aba_vis_in.X = aba_vis_in.X[:,igenes]\n",
    "\n",
    "    #aba_vis_in.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_filter_gene_set:\n",
    "    igenes = pd.Series(aba_vis_l23glu.gene_names).isin(gene_set).values\n",
    "\n",
    "    aba_vis_l23glu.gene_names = aba_vis_l23glu.gene_names[igenes]\n",
    "    aba_vis_l23glu.X = aba_vis_l23glu.X[:,igenes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_filter_gene_set:\n",
    "    aba_hipp_ent.gene_names  = var.genes.values\n",
    "    igenes = pd.Series(aba_hipp_ent.gene_names).isin(gene_set).values\n",
    "\n",
    "    aba_hipp_ent.gene_names = aba_hipp_ent.gene_names[igenes]\n",
    "    aba_hipp_ent.X = aba_hipp_ent.X[:,igenes]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "all_dataset = GeneExpressionDataset()\n",
    "all_dataset.populate_from_datasets([aba_vis_in, aba_vis_l23glu, aba_hipp_ent])    \n",
    "\n",
    "print((all_dataset.batch_indices==0).sum())\n",
    "print((all_dataset.batch_indices==2).sum())\n",
    "(all_dataset.batch_indices==1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scvi.dataset.dataset import GeneExpressionDataset\n",
    "if do_laptop==1:\n",
    "    all_dataset = GeneExpressionDataset.concat_datasets(aba_vis_in,aba_vis_l23glu, aba_hipp_ent)    \n",
    "else:    \n",
    "    all_dataset = GeneExpressionDataset()\n",
    "    all_dataset.populate_from_datasets([aba_vis_in, aba_vis_l23glu, aba_hipp_ent])    \n",
    "   \n",
    "\n",
    "print((all_dataset.batch_indices==0).sum())\n",
    "print((all_dataset.batch_indices==2).sum())\n",
    "(all_dataset.batch_indices==1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if gs=='219':\n",
    "    n_epochs_all = 100\n",
    "    n_epochs=100 if n_epochs_all is None else n_epochs_all\n",
    "    lr=2e-3\n",
    "    use_batches=True\n",
    "    use_cuda=True\n",
    "    \n",
    "if (gs=='1512')|(gs=='5188'):\n",
    "    n_epochs_all = 2000\n",
    "    n_epochs=2000 if n_epochs_all is None else n_epochs_all\n",
    "    lr=2e-3\n",
    "    use_batches=True\n",
    "    use_cuda=True   \n",
    "    \n",
    "if  (gs=='15656'):\n",
    "    n_epochs_all = 2000\n",
    "    n_epochs=2000 if n_epochs_all is None else n_epochs_all\n",
    "    lr=2e-3\n",
    "    use_batches=True\n",
    "    use_cuda=True     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gs=='219':\n",
    "    vae = VAE(all_dataset.nb_genes, n_batch=all_dataset.n_batches * use_batches, dispersion='gene-batch', \n",
    "        n_latent = 10,\n",
    "        n_layers=3, n_hidden=256)\n",
    "\n",
    "     #vae = VAE(all_dataset.nb_genes, n_batch=all_dataset.n_batches * use_batches, dispersion='gene-batch', n_latent = 10,\n",
    "     #         n_layers=2, n_hidden=256)\n",
    "\n",
    "    trainer = UnsupervisedTrainer(vae,\n",
    "                              all_dataset,\n",
    "                              train_size=0.95,\n",
    "                              use_cuda=use_cuda,\n",
    "                              frequency=1)\n",
    "\n",
    "    dn=100\n",
    "    file_results = 'vae_219_'\n",
    "    i_load0=0\n",
    "    i_load = 10\n",
    "    \n",
    "if (gs=='1512'):\n",
    "    vae = VAE(all_dataset.nb_genes, n_batch=all_dataset.n_batches * use_batches, dispersion='gene-batch', \n",
    "        n_latent = 20,\n",
    "        n_layers=3, n_hidden=256)\n",
    "\n",
    "     #vae = VAE(all_dataset.nb_genes, n_batch=all_dataset.n_batches * use_batches, dispersion='gene-batch', n_latent = 10,\n",
    "     #         n_layers=2, n_hidden=256)\n",
    "\n",
    "    trainer = UnsupervisedTrainer(vae,\n",
    "                              all_dataset,\n",
    "                              train_size=0.95,\n",
    "                              use_cuda=use_cuda,\n",
    "                              frequency=1)\n",
    "\n",
    "    dn=200\n",
    "    if gs=='1512':\n",
    "        dir_results = '/Users/stepaniu/Documents/jan_2020/scvi_vae_gs_1690/drive-download-20201214T201804Z-001'\n",
    "\n",
    "        file_results = dir_results +'/'+'vae_1690_'\n",
    "        i_load0=1\n",
    "        i_load = 10\n",
    "if gs=='5188':\n",
    "    dn=200\n",
    "    dir_results = '/Users/stepaniu/Documents/jan_2020/scvi_vae_gs_5188/gs_5188_2_drive-download-20201214T163529Z-001'\n",
    "   \n",
    "    file_results = dir_results +'/'+'vae_5188_2_'\n",
    "    i_load0=1\n",
    "    i_load = 7  \n",
    "        \n",
    "    import pickle\n",
    "    trainer_history = pickle.load(open(file_results+'trainer_history.p','rb'))\n",
    "    trainer_data = pickle.load(open(file_results+'trainer_data.p','rb'))\n",
    "    \n",
    "    #trainer_data.gene_names = df_aba_g.loc[trainer_data.gene_names.astype(int),'gene_symbol'].values\n",
    "    trainer_data.gene_names = df_aba_g.loc[trainer_data.gene_names.astype(int),'Unnamed: 0'].values\n",
    "    trainer_data.dense = all_dataset.dense\n",
    "    trainer_data.x_coord = all_dataset.x_coord\n",
    "    trainer_data.y_coord = all_dataset.y_coord\n",
    "    trainer_data.batch_indices = all_dataset.batch_indices\n",
    "    trainer_data.labels = all_dataset.labels\n",
    "    #all_dataset = trainer_data\n",
    "    \n",
    "    \n",
    "    #igenes = trainer_data.gene_names.astype(int)\n",
    "    #gene_nms = df_aba_g.loc[igenes,'Unnamed: 0'].values\n",
    "    #all_dataset.gene_names = all_dataset.gene_names[igenes]\n",
    "    #all_dataset.X = all_dataset.X[:,igenes]\n",
    "    \n",
    "    \n",
    "    vae = VAE(trainer_data._X.shape[1], n_batch=trainer_data.n_batches * use_batches, dispersion='gene-batch', \n",
    "        n_latent = 20,\n",
    "        n_layers=3, n_hidden=256)\n",
    "    #vae = VAE(all_dataset.nb_genes, n_batch=all_dataset.n_batches * use_batches, dispersion='gene-batch', \n",
    "    #    n_latent = 20,\n",
    "    #    n_layers=3, n_hidden=256)\n",
    "    trainer = UnsupervisedTrainer(vae,\n",
    "                              trainer_data,\n",
    "                              train_size=0.95,\n",
    "                              use_cuda=use_cuda,\n",
    "                              frequency=1)\n",
    "#     trainer = UnsupervisedTrainer(vae,\n",
    "#                               all_dataset,\n",
    "#                               train_size=0.95,\n",
    "#                               use_cuda=use_cuda,\n",
    "#                               frequency=1)\n",
    "\n",
    "if (gs=='15656'):  \n",
    "    dn=200\n",
    "    dir_results = '/Users/stepaniu/Documents/jan_2020/scvi_vae_gs_15656'\n",
    "    file_results = dir_results +'/'+'vae_15656_' #+'dataset_'\n",
    "    i_load0=1\n",
    "    i_load = 10\n",
    "    \n",
    "    #trainer = torch.load(file_results+'9'+'.pkl',map_location='cpu')\n",
    "    import pickle\n",
    "    trainer_history = pickle.load(open(file_results+'trainer_history.p','rb'))\n",
    "    trainer_data = pickle.load(open(file_results+'trainer_data.p','rb'))\n",
    "    \n",
    "    #trainer_data.gene_names = df_aba_g.loc[trainer_data.gene_names.astype(int),'gene_symbol'].values\n",
    "    trainer_data.gene_names = df_aba_g.loc[trainer_data.gene_names.astype(int),'Unnamed: 0'].values\n",
    "        \n",
    "    trainer_data.dense = all_dataset.dense \n",
    "    trainer_data.x_coord = all_dataset.x_coord\n",
    "    trainer_data.y_coord = all_dataset.y_coord\n",
    "    trainer_data.batch_indices = all_dataset.batch_indices\n",
    "    trainer_data.labels = all_dataset.labels\n",
    "    #all_dataset = trainer_data\n",
    "    \n",
    "    vae = VAE(trainer_data._X.shape[1], n_batch=trainer_data.n_batches * use_batches, dispersion='gene-batch', \n",
    "        n_latent = 20,\n",
    "        n_layers=3, n_hidden=256)\n",
    "    trainer = UnsupervisedTrainer(vae,\n",
    "                              trainer_data,\n",
    "                              train_size=0.95,\n",
    "                              use_cuda=use_cuda,\n",
    "                              frequency=1)\n",
    "    \n",
    "        \n",
    "n2 = int(n_epochs/dn)\n",
    "\n",
    "load_trained_model=False\n",
    "train_model=False\n",
    "if load_trained_model: #os.path.isfile('%s/vae.pkl' % save_path ):\n",
    "    #i=n2\n",
    "    i=0\n",
    "    #trainer.model.load_state_dict(torch.load('%s/vae'+str(i)+'.pkl' % save_path2)) #vae3.pkl\n",
    "    \n",
    "    #trainer.model.load_state_dict(torch.load('vae'+str(i)+'.pkl',map_location='cpu')) # last 18_11_2019 alignment on colab gpu\n",
    "    ##trainer.model.load_state_dict(torch.load('%s/vae2.pkl' % cwd))\n",
    "    trainer.model.load_state_dict(torch.load(file_results+str(i)+'.pkl')) # 219 selected genes\n",
    "    \n",
    "    trainer.model.eval()\n",
    "\n",
    "if train_model :\n",
    "    for i in range(0,n2):\n",
    "        trainer.train(n_epochs=dn, lr=lr)\n",
    "        #torch.save(trainer.model.state_dict(), 'save_path2'+'vae'+str(i)+'.pkl')\n",
    "        torch.save(trainer.model.state_dict(), file_results+str(i)+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_data._X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataset._X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainer_history['elbo_train_set'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.history[\"elbo_train_set\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "fis =os.listdir(dir_results)\n",
    "fis = pd.DataFrame(fis)\n",
    "fis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fis==(file_results+str(1)+'.pkl').split('/')[-1]).any()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "elbo_train_set = []\n",
    "elbo_test_set = []\n",
    "for i in range(i_load0,i_load):\n",
    "    is_pkl =(fis==(file_results+str(i)+'.pkl').split('/')[-1]).any()[0]\n",
    "    if is_pkl:\n",
    "        trainer.model.load_state_dict(torch.load(file_results+str(i)+'.pkl', map_location='cpu')) # 219 selected genes\n",
    "    \n",
    "        trainer.model.eval()\n",
    "        elbo_train_set = elbo_train_set + trainer.history[\"elbo_train_set\"]\n",
    "        elbo_test_set = elbo_test_set + trainer.history[\"elbo_test_set\"] \n",
    "        print(i,' : is data')\n",
    "        \n",
    "    print(i)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "if (gs=='5188')|(gs=='15656'):\n",
    "    \n",
    "    #x = np.linspace(0, 100, (len(trainer_history['elbo_train_set'])))\n",
    "    plt.plot(trainer_history['elbo_train_set'])\n",
    "    plt.plot(trainer_history['elbo_test_set'])\n",
    "    plt.ylim(20000, 30000)\n",
    "else:\n",
    "    x = np.linspace(0, 100, (len(elbo_train_set)))\n",
    "    plt.plot(x, elbo_train_set)\n",
    "    plt.plot(x, elbo_test_set)\n",
    "#plt.ylim(500, 700)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full = trainer.create_posterior(trainer.model, all_dataset, indices=np.arange(len(all_dataset)))\n",
    "full = trainer.create_posterior(trainer.model, trainer_data, indices=np.arange(len(trainer_data)))\n",
    "latent, batch_indices, labels = full.sequential().get_latent()\n",
    "batch_indices = batch_indices.ravel()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "from umap import UMAP\n",
    "latent_u = UMAP(spread=2).fit_transform(latent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.gene_dataset.gene_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputation gene expression\n",
    "px_rates = full.imputation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.gene_dataset.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.sum(px_rates, axis=0)==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import magic\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#magic_operator = magic.MAGIC()\n",
    "#X = pd.read_csv(\"test_data.csv\")\n",
    "#X_magic = magic_operator.fit_transform(X, genes=['VIM', 'CDH1', 'ZEB1'])\n",
    "#plt.scatter(X_magic['VIM'], X_magic['CDH1'], c=X_magic['ZEB1'], s=1, cmap='inferno')\n",
    "#plt.show()\n",
    "#magic.plot.animate_magic(X, gene_x='VIM', gene_y='CDH1', gene_color='ZEB1', operator=magic_operator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "g1 = 'Elfn1'\n",
    "g2 = 'Cacna2d1'\n",
    "g3  = 'Cplx1'\n",
    "\n",
    "genes = full.gene_dataset.gene_names\n",
    "ii=np.nonzero(genes==g1)[0]\n",
    "ii2=np.nonzero(genes==g2)[0]\n",
    "print(genes[ii])\n",
    "\n",
    "X_scvi = pd.DataFrame(px_rates, columns = genes)\n",
    "\n",
    "print(ii2)\n",
    "Xii2=full.gene_dataset._X[:,ii2]\n",
    "\n",
    "# impute with magic\n",
    "magic_operator = magic.MAGIC()\n",
    "#X = pd.read_csv(\"test_data.csv\")\n",
    "X = pd.DataFrame(full.gene_dataset._X, columns = genes)\n",
    "X_magic = magic_operator.fit_transform(X, genes=[g1, g2, g3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ax, fig) = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "plt.gca().legend('')\n",
    "\n",
    "Xii=full.gene_dataset._X[:,ii]\n",
    "nc = Xii.shape[0]\n",
    "#plt.scatter(np.arange(nc),Xii.ravel(),marker='o',c=[1,0,0],s=10)\n",
    "#plt.scatter(np.arange(nc),px_rates[:, ii].ravel(),marker='x',c=[0,1,0],s=20,alpha=0.3)\n",
    "plt.title('scVI imputation vs data : '+g1)\n",
    "plt.scatter(px_rates[:, ii].ravel(), Xii.ravel(), marker='o', c=[1,0,0], s=10)\n",
    "\n",
    "(ax1, fig1) = plt.subplots(figsize=(15, 5))\n",
    "plt.title('data   : '+g1)\n",
    "plt.scatter(np.arange(nc),Xii.ravel(),marker='o',c=[1,0,0],s=10)\n",
    "\n",
    "(ax1p5, fig1p5) = plt.subplots(figsize=(15, 5))\n",
    "plt.title('scVI imputation  : '+g1)\n",
    "plt.scatter(np.arange(nc),px_rates[:, ii].ravel(),marker='x',c=[0,1,0],s=20,alpha=0.3)\n",
    "\n",
    "\n",
    "(ax4, fig4) = plt.subplots(figsize=(15, 5))\n",
    "plt.title('data   : '+g2)\n",
    "plt.scatter(np.arange(nc),Xii2.ravel(),marker='o',c=[1,0,0],s=10)\n",
    "\n",
    "(ax5, fig5) = plt.subplots(figsize=(15, 5))\n",
    "plt.title('scVI imputation  : '+g2)\n",
    "plt.scatter(np.arange(nc),px_rates[:, ii2].ravel(),marker='x',c=[0,1,0],s=20,alpha=0.3)\n",
    "\n",
    "\n",
    "\n",
    "h1d = np.histogram(Xii, 100)\n",
    "h1i = np.histogram(px_rates[:, ii], 100)\n",
    "h1im = np.histogram(X_magic[g1], 100)\n",
    "\n",
    "h2d = np.histogram(Xii2, 100)\n",
    "h2i = np.histogram(px_rates[:, ii2], 100)\n",
    "h2im = np.histogram(X_magic[g2], 100)\n",
    "\n",
    "(ax2, fig2) = plt.subplots(figsize=(15, 5))\n",
    "plt.title('histogram : initial data vs scVI and MAGIC imputation  : '+g1)\n",
    "plt.scatter(h1d[1][:-1],np.log(h1d[0]),marker='o',c=[1,0,0],s=10)\n",
    "plt.scatter(h1i[1][:-1],np.log(h1i[0]),marker='x',c=[0,1,0],s=10)\n",
    "plt.scatter(h1im[1][:-1],np.log(h1im[0]),marker='d',c=[0,0,1],s=10)\n",
    "\n",
    "(ax2, fig2) = plt.subplots(figsize=(15, 5))\n",
    "plt.title('histogram : initial data vs scVI and MAGIC imputation  : '+g2)\n",
    "plt.scatter(h2d[1][:-1],np.log(h2d[0]),marker='o',c=[1,0,0],s=10)\n",
    "plt.scatter(h2i[1][:-1],np.log(h2i[0]),marker='x',c=[0,1,0],s=10)\n",
    "plt.scatter(h2im[1][:-1],np.log(h2im[0]),marker='d',c=[0,0,1],s=10)\n",
    "\n",
    "(ax6, fig6) = plt.subplots(figsize=(15, 5))\n",
    "plt.title('magic imputation')\n",
    "plt.scatter(X_magic[g1], X_magic[g2], c=X_magic[g3], s=1, cmap='inferno')\n",
    "plt.show()\n",
    "magic.plot.animate_magic(X, gene_x=g1, gene_y=g2, gene_color=g3, operator=magic_operator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aba_vis_in_c\n",
    "df_aba_vis_l23glu_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aba_vis_in_c.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aba_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aba_vis_in, aba_vis_l23glu, aba_hipp_ent\n",
    "aba_hipp_ent.obs\n",
    "aba_vis_in.obs\n",
    "all_samples = [aba_vis_in.obs.index.values,\n",
    "               aba_vis_l23glu.obs.loc[:,'sample_name'].values,\n",
    "               aba_hipp_ent.obs.loc[:,'samples'].values]\n",
    "all_samples = np.concatenate(all_samples)\n",
    "all_samples = pd.DataFrame(all_samples,columns=['sample_name'])\n",
    "print(all_samples.shape)\n",
    "all_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scVI_latent = pd.DataFrame(latent)\n",
    "scVI_latent = pd.concat([all_samples, scVI_latent, pd.DataFrame(batch_indices,columns=['batch_indices'])],axis=1)\n",
    "scVI_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputed scVI genes \n",
    "X_scvi = pd.DataFrame(px_rates, columns = genes)\n",
    "do_add_magic=1\n",
    "if do_add_magic==1:\n",
    "    X_scvi.loc[:,X_magic.columns] = X_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scVI_all = pd.concat([scVI_latent, X_scvi], axis=1)\n",
    "scVI_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scVI_all\n",
    "X_real = pd.DataFrame(full.gene_dataset._X, columns = full.gene_dataset.gene_names)\n",
    "X_real = pd.concat([all_samples, X_real],axis=1)\n",
    "X_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = 'Elfn1'\n",
    "g2 = 'Bsn'\n",
    "g3  = 'Syt1'\n",
    "\n",
    "\n",
    "\n",
    "(ax, fig) = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "plt.gca().legend('')\n",
    "\n",
    "Xii=X_real.loc[:] #full.gene_dataset._X[:,ii]\n",
    "nc = Xii.shape[0]\n",
    "#plt.scatter(np.arange(nc),Xii.ravel(),marker='o',c=[1,0,0],s=10)\n",
    "#plt.scatter(np.arange(nc),px_rates[:, ii].ravel(),marker='x',c=[0,1,0],s=20,alpha=0.3)\n",
    "plt.title('scVI imputation vs data : '+g1)\n",
    "plt.scatter(px_rates[:, ii].ravel(), Xii.ravel(), marker='o', c=[1,0,0], s=10)\n",
    "\n",
    "(ax1, fig1) = plt.subplots(figsize=(15, 5))\n",
    "plt.title('data   : '+g1)\n",
    "plt.scatter(np.arange(nc),Xii.ravel(),marker='o',c=[1,0,0],s=10)\n",
    "\n",
    "(ax1p5, fig1p5) = plt.subplots(figsize=(15, 5))\n",
    "plt.title('scVI imputation  : '+g1)\n",
    "plt.scatter(np.arange(nc),px_rates[:, ii].ravel(),marker='x',c=[0,1,0],s=20,alpha=0.3)\n",
    "\n",
    "\n",
    "(ax4, fig4) = plt.subplots(figsize=(15, 5))\n",
    "plt.title('data   : '+g2)\n",
    "plt.scatter(np.arange(nc),Xii2.ravel(),marker='o',c=[1,0,0],s=10)\n",
    "\n",
    "(ax5, fig5) = plt.subplots(figsize=(15, 5))\n",
    "plt.title('scVI imputation  : '+g2)\n",
    "plt.scatter(np.arange(nc),px_rates[:, ii2].ravel(),marker='x',c=[0,1,0],s=20,alpha=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scVI_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gs_name = '5188' #'1512' #'219'\n",
    "gs_name = gs\n",
    "scVI_all.to_hdf('scVI_latent_factors_and_imputation_aba2019_gs'+gs_name+'.hdf',key='data') # this is 290 genes dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STP modeling functions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PV->AAC ???\n",
    "Dt=50\n",
    "tF=10\n",
    "tD=410\n",
    "p0=0.23\n",
    "p=p0\n",
    "n = (1-p)\n",
    "n=1-(1-n)*np.exp(-Dt/tD)\n",
    "print(n)\n",
    "p=p+(1-p)*p0\n",
    "p=p0 +(p -p0)*np.exp((-Dt/tF))\n",
    "print(p)\n",
    "n*p/p0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CCK->CCK ???\n",
    "Dt=50\n",
    "tF=1542\n",
    "tD=115\n",
    "p0=0.11\n",
    "p=p0\n",
    "n = (1-p)\n",
    "n=1-(1-n)*np.exp(-Dt/tD)\n",
    "print(n)\n",
    "p=p+(1-p)*p0\n",
    "p=p0 +(p -p0)*np.exp((-Dt/tF))\n",
    "print(p)\n",
    "n*p/p0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PV->AAC ???\n",
    "Dt=50\n",
    "tF=1.6\n",
    "tD=930\n",
    "p0=0.26\n",
    "p=p0\n",
    "n = (1-p)\n",
    "n=1-(1-n)*np.exp(-Dt/tD)\n",
    "print(n)\n",
    "p=p+(1-p)*p0\n",
    "p=p0 +(p -p0)*np.exp((-Dt/tF))\n",
    "print(p)\n",
    "n*p/p0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ivy->PC ???\n",
    "Dt=50\n",
    "tF=62\n",
    "tD=144\n",
    "p0=0.32\n",
    "p=p0\n",
    "n = (1-p)\n",
    "n=1-(1-n)*np.exp(-Dt/tD)\n",
    "print(n)\n",
    "p=p+(1-p)*p0\n",
    "p=p0 +(p -p0)*np.exp((-Dt/tF))\n",
    "print(p)\n",
    "n*p/p0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CCK_DTI->PC ???\n",
    "Dt=50\n",
    "tF=14\n",
    "tD=185\n",
    "p0=0.15\n",
    "p=p0\n",
    "n = (1-p)\n",
    "n=1-(1-n)*np.exp(-Dt/tD)\n",
    "print(n)\n",
    "p=p+(1-p)*p0\n",
    "p=p0 +(p -p0)*np.exp((-Dt/tF))\n",
    "print(p)\n",
    "n*p/p0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  STP_sim(ge_data, T, init_state=None ):\n",
    "    # transform labels from TM to An:A1\n",
    "    #f = 20 # Hz\n",
    "    #N = 3\n",
    "    #T = np.arange(N)*1000/f\n",
    "\n",
    "    N    = len(T)\n",
    "    nc   = len(ge_data.index)\n",
    "\n",
    "    x_lower = np.array([1,       0.01,    1,      0.1,     1])\n",
    "    x_upper = np.array([10000,   1,      10000,    10,     1])\n",
    "    stp_ns =          ['tF',    'p0',    'tD',   'dp/p0', 'A']\n",
    "    for jj in range(len(stp_ns)):\n",
    "        nsj = stp_ns[jj]\n",
    "        ge_data.loc[:,nsj] = np.maximum(ge_data.loc[:,nsj].values,x_lower[jj])\n",
    "        ge_data.loc[:,nsj] = np.minimum(ge_data.loc[:,nsj].values,x_upper[jj])\n",
    "        if nsj=='dp/p0':\n",
    "            p0   = ge_data.loc[:,'p0'].values\n",
    "            dpp0 = ge_data.loc[:,'dp/p0'].values\n",
    "            dp = p0*dpp0\n",
    "            dp = np.minimum(dp,1)\n",
    "            ge_data.loc[:,nsj] = dp/(p0 +np.finfo(float).eps)\n",
    "\n",
    "    dpp0 = ge_data.loc[:,'dp/p0'].values\n",
    "    p0   = ge_data.loc[:,'p0'].values\n",
    "    tF   = ge_data.loc[:,'tF'].values\n",
    "    tD   = ge_data.loc[:,'tD'].values\n",
    "    A    = 1 + 0*ge_data.loc[:,'A'].values # simplify A\n",
    "\n",
    "    As = np.zeros((nc,N))\n",
    "    n = np.zeros((nc,))\n",
    "    p = np.zeros((nc,))\n",
    "    ns2 = np.zeros((nc,N))\n",
    "    ps2 = np.zeros((nc,N))\n",
    "\n",
    "    i=0\n",
    "    \n",
    "    if init_state is None :\n",
    "        n[:] = 1\n",
    "        p[:] = p0\n",
    "    else:\n",
    "        n = init_state[0]\n",
    "        p = init_state[1]\n",
    "\n",
    "\n",
    "    As[:,i] = A*n*p\n",
    "    \n",
    "    n = n*(1-p)\n",
    "    p = p + dp*(1-p)\n",
    "    \n",
    "    ns2[:,i]=n\n",
    "    ps2[:,i]=p\n",
    "\n",
    "    for i in range(1,N):\n",
    "        Dt=T[i]-T[i-1]\n",
    "        #n = 1 - (1 - (n -p*n))*np.exp((-Dt/tD).astype(float))\n",
    "        #p=p0 -(p0-(p + dpp0*p0*(1-p)))*np.exp((-Dt/tF).astype(float))\n",
    "        #As[:,i]=A*n*p\n",
    "        #ns2[:,i]=n\n",
    "        #ps2[:,i]=p\n",
    "\n",
    "        \n",
    "        n = 1 - (1 - n )*np.exp((-Dt/tD ).astype(float))\n",
    "        p=p0 +(p -p0)*np.exp((-Dt/tF).astype(float))\n",
    "            \n",
    "\n",
    "        As[:,i]=A*n*p\n",
    "       \n",
    "        n = n*(1-p)\n",
    "        p = p + dp*(1-p)\n",
    "        \n",
    "        ns2[:,i]=n\n",
    "        ps2[:,i]=p\n",
    "\n",
    "    \n",
    "\n",
    "    #aa = [As, ns2, ps2]\n",
    "    \n",
    "    return As, ns2, ps2, dpp0, p0, tF, tD, A"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def  STP_sim(ge_data, T ):\n",
    "    # transform labels from TM to An:A1\n",
    "    #f = 20 # Hz\n",
    "    #N = 3\n",
    "    #T = np.arange(N)*1000/f\n",
    "\n",
    "    N    = len(T)\n",
    "    nc   = len(ge_data.index)\n",
    "\n",
    "    x_lower = np.array([1,       0.01,    1,      0.1,     1])\n",
    "    x_upper = np.array([10000,   1,      10000,    10,     1])\n",
    "    stp_ns =          ['tF',    'p0',    'tD',   'dp/p0', 'A']\n",
    "    for jj in range(len(stp_ns)):\n",
    "        nsj = stp_ns[jj]\n",
    "        ge_data.loc[:,nsj] = np.maximum(ge_data.loc[:,nsj].values,x_lower[jj])\n",
    "        ge_data.loc[:,nsj] = np.minimum(ge_data.loc[:,nsj].values,x_upper[jj])\n",
    "        if nsj=='dp/p0':\n",
    "            p0   = ge_data.loc[:,'p0'].values\n",
    "            dpp0 = ge_data.loc[:,'dp/p0'].values\n",
    "            dp = p0*dpp0\n",
    "            dp = np.minimum(dp,1)\n",
    "            ge_data.loc[:,nsj] = dp/(p0 +np.finfo(float).eps)\n",
    "\n",
    "    dpp0 = ge_data.loc[:,'dp/p0'].values\n",
    "    p0   = ge_data.loc[:,'p0'].values\n",
    "    tF   = ge_data.loc[:,'tF'].values\n",
    "    tD   = ge_data.loc[:,'tD'].values\n",
    "    A    = ge_data.loc[:,'A'].values\n",
    "\n",
    "    As = np.zeros((nc,N))\n",
    "    n = np.zeros((nc,))\n",
    "    p = np.zeros((nc,))\n",
    "    ns2 = np.zeros((nc,N))\n",
    "    ps2 = np.zeros((nc,N))\n",
    "\n",
    "    i=0\n",
    "    n[:] = 1\n",
    "    p[:] = p0\n",
    "    As[:,i] = A*n*p\n",
    "    ns2[:,i]=n\n",
    "    ps2[:,i]=p\n",
    "\n",
    "    for i in range(1,N):\n",
    "        Dt=T[i]-T[i-1]\n",
    "        n = 1 - (1 - (n -p*n))*np.exp((-Dt/tD).astype(float))\n",
    "        p=p0 -(p0-(p + dpp0*p0*(1-p)))*np.exp((-Dt/tF).astype(float))\n",
    "        As[:,i]=A*n*p\n",
    "        ns2[:,i]=n\n",
    "        ps2[:,i]=p\n",
    "\n",
    "    #aa = [As, ns2, ps2]\n",
    "    \n",
    "    return As, ns2, ps2, dpp0, p0, tF, tD, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  STP_sim2(x, T, init_state=None, model_type='tm5' ):\n",
    "\n",
    "    N    = len(T)\n",
    "    tF      = x[0] #.astype(float)\n",
    "    p00     = x[1]\n",
    "    tD      = x[2] #.astype(float)\n",
    "    dp      = x[3]\n",
    "    A       = 1 #x[4] # simplify A\n",
    "    \n",
    "    #breakpoint()\n",
    "    mod_fdr2=False\n",
    "    if model_type=='tm5_fdr2':  # should be :check freq. dependent recovery\n",
    "        tDmin     = x[5]\n",
    "        dd        = x[6]\n",
    "        t_FDR     = x[7]\n",
    "        mod_fdr2=True\n",
    "        tDmax  = tD\n",
    "        itDmin = 1/tDmin\n",
    "        itDmax = 1/tDmax\n",
    "        #breakpoint()\n",
    "        \n",
    "    mod_smr=False\n",
    "    if model_type=='tm5_smr':  # should be :check freq. dependent recovery\n",
    "        t_SMR   = x[8]\n",
    "        dp0     = x[9]\n",
    "        mod_smr=True\n",
    "        #p00  = p00\n",
    "\n",
    "    As = np.zeros((N,))\n",
    "    state = np.zeros((N,4))\n",
    "\n",
    "   \n",
    "    if init_state is None :\n",
    "        n = 1\n",
    "        p0=p00\n",
    "        p = p0\n",
    "        d = 0\n",
    "    else:\n",
    "        n = init_state[0]\n",
    "        p = init_state[1]\n",
    "        d = init_state[2]\n",
    "        p0= init_state[3]\n",
    "\n",
    "    \n",
    "    for i in range(0,N):\n",
    "        if i==0:\n",
    "            Dt = T[i]\n",
    "        else:\n",
    "            Dt = T[i]-T[i-1]\n",
    "        \n",
    "        if mod_fdr2:\n",
    "            d0=d\n",
    "            d = d*np.exp(-Dt/t_FDR) \n",
    "            n = 1 - (1 - n )*np.exp(-Dt*itDmax -(itDmin -itDmax)*t_FDR*(d0-d))\n",
    "        else:\n",
    "            n = 1 - (1 - n )*np.exp(-Dt/tD )\n",
    "            \n",
    "        if mod_smr:\n",
    "            p01=p0\n",
    "            p0=p00 + (p0 -p00)*np.exp(-Dt/t_SMR)\n",
    "            p=p0 +(p -p01)*np.exp(-Dt/tF)\n",
    "        else:\n",
    "            p=p0 +(p -p0)*np.exp(-Dt/tF)\n",
    "            \n",
    "\n",
    "        As[i]=A*n*p\n",
    "       \n",
    "        n = n*(1-p)\n",
    "        p = p + dp*(1-p)\n",
    "        if mod_fdr2:\n",
    "            d  = d + dd*(1-d) \n",
    "        if mod_smr:\n",
    "            p0  = p0 - dp0*p0    \n",
    " \n",
    "        state[i] = [n,p,d,p0]\n",
    "\n",
    "    #return As, ns2, ps2, dpp0, p0, tF, tD, A\n",
    "    return As, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def STP_sim_complex(ge_data,l_pre_post2,stp_aba_names):\n",
    "    # transform labels from TM to An:A1\n",
    "    #fs = [20, 50, 10] # Hz\n",
    "    fs = [20, 50, 10]\n",
    "    N = 5\n",
    "\n",
    "    #Trec = [250, 500, 1000]\n",
    "    Trec =[250, 1000]\n",
    "    DT0 = 25000\n",
    "    if l_pre_post2<ge_data.shape[0]:\n",
    "        xs  =ge_data.iloc[l_pre_post2:,:].loc[:,stp_aba_names].values\n",
    "        xs  =np.delete(xs, [5,6],axis=1)\n",
    "        As2=np.zeros((xs.shape[0],0))\n",
    "    \n",
    "    \n",
    "    As=np.zeros((l_pre_post2,0))\n",
    "    if 1:\n",
    "\n",
    "        for f in fs:\n",
    "            T = np.arange(N)*1000/f\n",
    "\n",
    "            if l_pre_post2>0:\n",
    "                Asf, ns, ps, dpp0, p0, tF, tD, A = STP_sim(ge_data.iloc[0:l_pre_post2,:],T)\n",
    "                As = np.concatenate([As,Asf],axis=1)\n",
    "\n",
    "                for ri in range(len(Trec)):\n",
    "                    Asr, nsr, psr, dpp0, p0, tF, tD, A = STP_sim(ge_data.iloc[0:l_pre_post2,:],[Trec[ri]],init_state=[ns[:,-1],ps[:,-1]])\n",
    "                    As = np.concatenate([As,Asr],axis=1)\n",
    "\n",
    "            if l_pre_post2<ge_data.shape[0]:\n",
    "                As2f = np.zeros((xs.shape[0],N+len(Trec)))\n",
    "                for i2 in range(xs.shape[0]):\n",
    "                    #as2, sts2 = STP_sim2(xs[i2,:],np.arange(8)*50,model_type = 'tm5') # preconditioning series???\n",
    "                    #as2, sts2 = STP_sim2(xs[i2,:], T+DT0, init_state=sts2[-1], model_type='tm5' ) \n",
    "\n",
    "                    as2, sts2 = STP_sim2(xs[i2,:],T,model_type = 'tm5_smr')\n",
    "\n",
    "                    As2f[i2,0:N] = as2\n",
    "                    for ri in range(len(Trec)):\n",
    "                        as2r, sts2r = STP_sim2(xs[i2,:], [Trec[ri]], init_state=sts2[-1], model_type='tm5_smr' ) \n",
    "                        As2f[i2,N+ri] = as2r\n",
    "\n",
    "                As2 = np.concatenate([As2,As2f],axis=1)\n",
    "\n",
    "        if l_pre_post2<ge_data.shape[0]:\n",
    "            As2 = As2/As2[:,0].reshape((-1,1))\n",
    "        else:\n",
    "            As2  = np.zeros((0,As.shape[1]))\n",
    "        \n",
    "        \n",
    "        if l_pre_post2>0:\n",
    "            As = As/As[:,0].reshape((-1,1))\n",
    "            As = np.concatenate([As,As2],axis=0)\n",
    "        else:\n",
    "            As =As2\n",
    "    else:\n",
    "        As, ns, ps, dpp0, p0, tF, tD, A = STP_sim(ge_data,T)\n",
    "\n",
    "\n",
    "    #import matplotlib.pyplot as plt    \n",
    "    #plt.plot(As2[400:410,:].transpose(),'o-')\n",
    "    \n",
    "    return As"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#df_stp.loc[:,'name'].str.contains('Jaing_2015_PC')\n",
    "df_stp=pd.read_excel(d5+fn5)\n",
    "#df_stp.loc[df_stp.loc[:,'stp_comment'].isna() ,'stp_comment']=''\n",
    "\n",
    "############   \n",
    "do_averagig_stp =0\n",
    "stp_TM_names=['tF', 'p0','tD','dp/p0','A']\n",
    "stp_TM_start = 187\n",
    "ncol = len(df_stp.columns)\n",
    "df_tm_name = pd.DataFrame(np.zeros(ncol).reshape((1,ncol)), columns =df_stp.columns )\n",
    "df_stp = pd.concat([df_tm_name, df_stp],axis=0).reset_index(drop=True)\n",
    "\n",
    "for i,nm in enumerate(stp_TM_names):\n",
    "    ii = stp_TM_start + np.arange(0,ncol-i-stp_TM_start,len(stp_TM_names))+i\n",
    "    df_stp.iloc[0,ii] = nm\n",
    "\n",
    "#df_stp.loc[df_stp.loc[:,'name'].str.contains('Jaing_2015_PC')!=True,:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "##\n",
    "## Hippocampus gene expression\n",
    "##\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "#os.listdir()\n",
    "\n",
    "from scvi.dataset import LoomDataset, CsvDataset, Dataset10X, AnnDataset\n",
    "\n",
    "d_aba  ='/home/stepaniu/Documents/references/transcriptomes to STP/transcriptomes/ABA_2019/'\n",
    "\n",
    "df_aba_g = pd.read_csv(d_aba + 'medians.csv')\n",
    "df_aba_c = pd.read_csv(d_aba + 'sample_annotations.csv')\n",
    "\n",
    "\n",
    "d7 = '/home/stepaniu/Documents/references/transcriptomes to STP/transcriptomes/ABA_2019/'\n",
    "\n",
    "aba_vis_in = AnnDataset(\"ABA_2019_transcriptome.h5ad\", new_n_genes = 45768,\n",
    "                               save_path = d7) \n",
    "\n",
    "obs = pd.read_excel(d7 +'ABA_2019_obs.xlsx') # samples\n",
    "var = pd.read_excel(d7 + 'ABA_2019_var.xlsx') # gene names\n",
    "#obs = aba_vis_in._obs # samples\n",
    "#var = aba_vis_in._var # gene names\n",
    "\n",
    "in_gaba = df_aba_c.loc[df_aba_c.loc[:,'sample_name'].isin(obs.loc[:,'samples']),['sample_name', 'class_label']]\n",
    "in_gaba = in_gaba.reset_index().set_index('sample_name',drop=True)\n",
    "in_gaba = in_gaba.loc[obs.loc[:,'samples'],:]\n",
    "in_gaba = in_gaba.reset_index().reset_index().set_index('sample_name',drop=True)\n",
    "in_gaba = in_gaba.loc[in_gaba.loc[:,'class_label']=='GABAergic',:].reset_index().set_index('level_0',drop=True)\n",
    "\n",
    "#aba_vis_in.gene_names = var.iloc[:,0].values\n",
    "aba_vis_in.gene_names = var.loc[:,'genes'].values\n",
    "vis_dat = aba_vis_in\n",
    "aba_vis_in = []\n",
    "\n",
    "in_ds = df_aba_c.loc[df_aba_c.loc[:,'sample_name'].isin(obs.loc[:,'samples']),['sample_name', 'class_label']]\n",
    "in_ds = in_ds.reset_index().set_index('sample_name',drop=True)\n",
    "in_ds = in_ds.loc[obs.loc[:,'samples'],:]\n",
    "in_ds = in_ds.reset_index().reset_index().set_index('sample_name',drop=True)\n",
    "#in_ds\n",
    "\n",
    "df_aba_vis_in_c = df_aba_c.loc[in_ds.loc[:,'index'],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "##\n",
    "##  HIPPOCAMPUS STP\n",
    "##\n",
    "##"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "do_laptop=0\n",
    "if do_laptop:\n",
    "    d5 = '~/Documents/references/lists_of_proteins_2/Analysis_of_proteins/genetic_types/transcriptomic_interneurons/additional cortical STP data/'\n",
    "else:\n",
    "    d5 = '/home/stepaniu/Documents/references/transcriptomes to STP/scVI_data/additional cortical STP data/'\n",
    "#fn5='STP_cortex_interneurons_Cre_labeled.xlsx'\n",
    "#fn5='STP_cortex_interneurons_Cre_labeled_UDF2.xlsx'\n",
    "#fn5='STP_cortex_interneurons_Cre_labeled_UDF3.xlsx'\n",
    "\n",
    "do_3pars_TM=0 # 3 or 4 parametric TM model?\n",
    "if do_3pars_TM:\n",
    "    fn5='STP_cortex_interneurons_Cre_labeled_UDF5_3pars.xlsx'\n",
    "else:\n",
    "    #fn5='STP_cortex_interneurons_Cre_labeled_UDF5_4pars.xlsx'\n",
    "    fn5='STP_hippocampus_all_UDF3.xlsx'\n",
    "\n",
    "df_stp=pd.read_excel(d5+fn5)\n",
    "\n",
    "ii=np.nonzero(df_stp.columns=='Var1_  1')[0][0]\n",
    "#tmp2 = df_stp.iloc[[9,13,14,15],ii:]\n",
    "tmp2 = df_stp.iloc[[13],ii:]\n",
    "for i in range(tmp2.shape[0]):\n",
    "    #tmp3 = tmp2.iloc[i,:].abs().values.reshape((-1,100)).T.ravel()\n",
    "    #tmp2.iloc[i,:]  = tmp3\n",
    "    tmp2.iloc[i,1::5]=0.24\n",
    "    \n",
    "#df_stp.iloc[[9,13,14,15],ii:]=tmp2   \n",
    "df_stp.iloc[[13],ii:]=tmp2   \n",
    "df_stp.to_excel(d5+fn5)\n",
    "df_stp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#  HIPPOCAMPUS STP\n",
    "#\n",
    "#\n",
    "import re\n",
    "import time\n",
    "t1 = time.time()\n",
    "\n",
    "\n",
    "# names of gene expression based synapses types\n",
    "\n",
    "glu_l23=[] #list(set(df_aba_vis_l23glu_c.loc[:,'cluster']))\n",
    "\n",
    "dsg = [['PC']+glu_l23,\n",
    "       ['Pvalb','Pvalb Tpbg','Pvalb Reln Itm2a','Pvalb Reln Tac1','Pvalb Sema3e Kank4'], \n",
    "       ['Sst','Sst Calb2 Pdlim5','Sst Calb2 Necab1', 'Sst Hpse Cbln4','Sst Hpse Sema3c','Sst Nr2f2 Necab1'],\n",
    "       ['Vip','Vip Lmo1 Myl1', 'Vip Rspo1 Itga4','Vip Ptprt Pkp2','Vip Rspo4 Rxfp1 Chat'],\n",
    "       ['Lamp5','Lamp5 Plch2 Dock5'],\n",
    "       ['MC','L23MC','Sst Calb2 Pdlim5'],\n",
    "        'L5MC']\n",
    "      \n",
    "\n",
    "#set(df_stp.loc[:,'synapse_type_2'])\n",
    "\n",
    "# names of electrophysiology based synapses types\n",
    "dse = [['PC', 'L23P', 'L23PC', 'L5P'],\n",
    "       ['Pvalb', 'PV', 'L23BC', 'L5BC'], \n",
    "       ['Sst', 'L23MC', 'L5MC', 'Sst', 'MC'], \n",
    "       ['Vip', 'VIP', 'BTC'],\n",
    "       ['NGC', 'L23NGC'],\n",
    "       ['L23MC' ],\n",
    "       ['L5MC']]\n",
    "\n",
    "\n",
    "# load stp data table\n",
    "do_laptop=0\n",
    "if do_laptop:\n",
    "    d5 = '~/Documents/references/lists_of_proteins_2/Analysis_of_proteins/genetic_types/transcriptomic_interneurons/additional cortical STP data/'\n",
    "else:\n",
    "    d5 = '/home/stepaniu/Documents/references/transcriptomes to STP/scVI_data/additional cortical STP data/'\n",
    "#fn5='STP_cortex_interneurons_Cre_labeled.xlsx'\n",
    "#fn5='STP_cortex_interneurons_Cre_labeled_UDF2.xlsx'\n",
    "#fn5='STP_cortex_interneurons_Cre_labeled_UDF3.xlsx'\n",
    "\n",
    "do_3pars_TM=0 # 3 or 4 parametric TM model?\n",
    "if do_3pars_TM:\n",
    "    fn5='STP_cortex_interneurons_Cre_labeled_UDF5_3pars.xlsx'\n",
    "else:\n",
    "    #fn5='STP_cortex_interneurons_Cre_labeled_UDF5_4pars.xlsx'\n",
    "    fn5='STP_hippocampus_all_UDF3.xlsx'\n",
    "\n",
    "df_stp=pd.read_excel(d5+fn5)\n",
    "#df_stp.loc[df_stp.loc[:,'stp_comment'].isna() ,'stp_comment']=''\n",
    "\n",
    "############   \n",
    "do_averagig_stp =0\n",
    "stp_TM_names=['tF', 'p0','tD','dp/p0','A']\n",
    "stp_TM_start = np.nonzero(df_stp.columns=='references')[0][0]+1 # 187\n",
    "ncol = len(df_stp.columns)\n",
    "df_tm_name = pd.DataFrame(np.zeros(ncol).reshape((1,ncol)), columns =df_stp.columns )\n",
    "df_stp = pd.concat([df_tm_name, df_stp],axis=0).reset_index(drop=True)\n",
    "\n",
    "for i,nm in enumerate(stp_TM_names):\n",
    "    ii = stp_TM_start + np.arange(0,ncol-i-stp_TM_start,len(stp_TM_names))+i\n",
    "    df_stp.iloc[0,ii] = nm\n",
    "\n",
    "\n",
    "#print(df_stp.head())\n",
    "#############\n",
    "##Remove pure PPR data (Jaing_2015_PC)\n",
    "#df_stp = df_stp.loc[df_stp.loc[:,'name'].str.contains('Jaing_2015_PC')!=True,:].reset_index(drop=True) \n",
    "\n",
    "############\n",
    "\n",
    "\n",
    "#stp_TM_names_2=['tF', 'p0','tD','dp','A','A1','A2','tDmin','dd','t_FDR','t_SMR','dp0'] # tm5+smr\n",
    "##################\n",
    "#stp_2_default = np.ones(7)\n",
    "#for i in range(1,df_stp.shape[0]):\n",
    "#    stp_par = df_stp.iloc[i,stp_TM_start:].values.reshape((len(stp_TM_names),-1))\n",
    "    \n",
    "\n",
    "##################\n",
    "\n",
    "df_stp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hippocampus: allign gene expression and stp data (for selected gene sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rules for cell types to aba clusters mapping:\n",
    "#1 dorsal : 'subregion_label'== 'anterior' , ventral: 'subregion_label'== posterior' ,  or by genes : see Cembrowski 2016\n",
    "#2 TA = ECIII (PCP4+) ->CA1 \n",
    "#3 pp = ECII (Reelin+) ->CA3, DG, CA2\n",
    "#4 TA = ECII (Wlf1+, Calb2+) ->CCK ppa in CA1 mostly\n",
    "#5 no cre lines annotations - use only old cck datatypes->clusters or corresponding cortical cre lines?\n",
    "#6 Ivy 1, 2 ?? - 2 clusters (Lhx6 1,2) and may be Lhx6 3 (no reelin), but also could be some Lamp5_5? - less reelin, more Nos1 \n",
    "#  use only only PPF Ivy cells? or try 2 subtypes = 2 clusters?? - dep=lhx6_4\n",
    "#7 NGF types 1,2? Price 2005 - correlate with age - select only facilitating (type 2)!\n",
    "#8 HIPP - Sst, pp-associated ~OLM??\n",
    "#9 HICAP - CCk - comissural assoc - ~ CCK Schffer comm : Sncg6 (Sncg/Ndnf HPF1)\n",
    "#10 Kohus: CCK DTI vs BC? - use Cbln4, Lgi2 ratio? (Favuzzi 2019) - Sncg2?, (Sncg6) - DTI vs Sncg/Ndnf HPF2,3,6, Serpinf1, Sncg7, Vip_14\n",
    "#11 CA1 PC: use subregions: anterior, mid-anterior  for dorsal\n",
    "\n",
    "\n",
    "\n",
    "# batches and gene sets\n",
    "pre_post = df_stp.loc[:,'synapse_type_2'].str.split(pat='->',expand=True)\n",
    "\n",
    "vbi = vis_dat.batch_indices.reshape((vis_dat.batch_indices.shape[0],))\n",
    "vbi=vbi>0\n",
    "\n",
    "df_gs  = pd.DataFrame( vis_dat.gene_names ) \n",
    "in_pregs = df_gs.loc[:,0].isin(pregs).values\n",
    "in_postgs = df_gs.loc[:,0].isin(postgs).values\n",
    "\n",
    "pregs2 = df_gs.loc[in_pregs,0].values\n",
    "postgs2 = df_gs.loc[in_postgs,0].values\n",
    "\n",
    "do_hippocampus = 1\n",
    "if do_hippocampus ==0:\n",
    "\n",
    "    do_2=1;\n",
    "    # select stp data for appropriate frequency and ages \n",
    "    stp_type = {'name':['Walker_2016_PV->MC L23',\n",
    "                   'Walker_2016_VIP->MC L23',\n",
    "                   'Yuste_2016_taus_PV->Sst L23 recalc',\n",
    "                   'Yuste_2016_taus_PV->VIP L23 recalc',\n",
    "                   'Yuste_2016_taus_Sst->VIP L23 recalc',\n",
    "                   'Yuste_2016_taus_VIP->Sst L23 recalc',\n",
    "                   'Yuste_2016_taus_VIP->VIP L23 recalc'],\n",
    "            'stp_freq': [20, 40, 50], 'stp_comment':  ['', ' P20-21', ' P20-22', ' P20-23', ' P25-31'],\n",
    "            'stp_data_type': df_stp.loc[0, :]!=0 }\n",
    "\n",
    "    if do_2==1:\n",
    "        stp_type = {'name':['Walker_2016_PV->MC L23',\n",
    "                    'Walker_2016_VIP->MC L23',\n",
    "                    'Yuste_2016_taus_PV->Sst L23 recalc',\n",
    "                    'Yuste_2016_taus_VIP->Sst L23 recalc'],\n",
    "            'stp_freq': [20, 40, 50], 'stp_comment':  ['', ' P20-21', ' P20-22', ' P20-23', ' P25-31'],\n",
    "            'stp_data_type': df_stp.loc[0, :]!=0 }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    in_stp_data = df_stp.loc[:,'stp_freq'].isin([20])\n",
    "    in_stp_data= in_stp_data|(df_stp.loc[:,'stp_freq'].isin([40,50])&df_stp.loc[:,'name'].isin(stp_type['name']))\n",
    "    in_stp_data = in_stp_data&df_stp.loc[:,'stp_comment'].isin(pd.Series(stp_type['stp_comment']).str.strip())\n",
    "\n",
    "else:\n",
    "    \n",
    "    in_stp_data = pd.Series(np.ones(df_stp.shape[0])==1)\n",
    "    # filter stp dataset \n",
    "    in_stp_data.loc[df_stp.loc[:,'name'].isin(['Price_2005EC->NGF type1',\n",
    "                                               'Qin_2017EC->Ivy cell type1',\n",
    "                                               'Qin_2017CA3->Ivy cell type1'])]=False\n",
    "    #rem = df_stp.loc[:,'name'].str.contains('HIPP').fillna(value=True)\n",
    "    #in_stp_data.loc[rem]=False\n",
    "    \n",
    "    # 17–20 Sprague-Dawley rats?? - too high depression? - 0.39\n",
    "    rem = df_stp.loc[:,'name'].str.contains('Fuentealba_2008CA1->Ivy cell').fillna(value=True)\n",
    "    in_stp_data.loc[rem]=False    \n",
    "    \n",
    "# show selected stp dataset\n",
    "ddd=df_stp.loc[in_stp_data,['name','synapse_type_2', 'stp_freq','area']]\n",
    "print(ddd)\n",
    "print(in_stp_data.sum())\n",
    "\n",
    "synij = np.repeat(False,in_stp_data.shape[0])\n",
    "\n",
    "cols2 = ['name','perc']\n",
    "cl2post =pd.DataFrame([],columns= cols2)\n",
    "cl2pre =pd.DataFrame([],columns= cols2)\n",
    "\n",
    "\n",
    "# strip cluster names\n",
    "#list(set(df_aba_vis_in_c.loc[df_aba_vis_in_c.loc[:,'subclass'].str.contains(clns6),'cluster']))\n",
    "df_aba_vis_in_c.loc[:,'cluster_label']=df_aba_vis_in_c.loc[:,'cluster_label'].str.strip()\n",
    "#df_aba_vis_l23glu_c.loc[:,'cluster']=df_aba_vis_l23glu_c.loc[:,'cluster'].str.strip()\n",
    "\n",
    "do_simplfy_clusters = 1 # do not take into account minor subclasses\n",
    "Sst_NonMC ='Sst, Sst Calb2 Pdlim5-, Sst Chrna2 Glra3-, Sst Chrna2 Ptgdr-, Sst Myh8 Etv1-, Sst Tac2 Myh4-, Sst Nr2f2 Necab1-, Sst Chodl-,  Sst Calb2 Necab1-, Sst Tac1 Htr1d-, Sst Myh8 Fibin-';\n",
    "\n",
    "pregs3 = ['pre__'+s for s in pregs2.tolist()]\n",
    "postgs3 = ['post_'+s for s in postgs2.tolist()]\n",
    "ge_columns = ['cell_type2_pre', 'cell_type2_post',\n",
    "                                             'layer_pre','layer_post',\n",
    "                                             'cre_line_pre', 'cre_line_post'] +pregs3 +postgs3\n",
    "\n",
    "stp_columns = stp_TM_names #list(set(df_stp.loc[0,:]).difference(set([0])))\n",
    "\n",
    "# output datasets\n",
    "stp_data = pd.DataFrame([], columns =['presynaptic', 'postsynaptic', 'stp_data'] )\n",
    "ge_data = pd.DataFrame([], columns =ge_columns+stp_columns+['index_ds'])\n",
    "\n",
    "\n",
    "for i in range(1,len(pre_post.index)): #range(1,len(pre_post.index)): # [1]: \n",
    "    \n",
    "    # syne3 - all available pre- and post- synaptic cells annotations : cre-lines, layers, morphologies\n",
    "    l = str.strip(df_stp.loc[i,'area'])\n",
    "    synel = re.split(';',l)\n",
    "    if len(synel)==1:\n",
    "        l1=l\n",
    "    else:\n",
    "        l  = str.strip(synel[1])\n",
    "        l1  = str.strip(synel[0])\n",
    "            \n",
    "    \n",
    "    if type(df_stp.loc[i,'synapse_type'])!=float:\n",
    "        #syne1 = re.split('\\;',df_stp.loc[i,'synapse_type']) no cre-line annotations for hippocampal data\n",
    "        syne1 = ''\n",
    "    else:\n",
    "        syne1 =''    \n",
    "    #syne2 = df_stp.loc[i,'synapse_type2'].str.split(pat='->',expand=True)\n",
    "    syne2 = re.split('->',df_stp.loc[i,'synapse_type_2'])\n",
    "    if len(syne2)==1:\n",
    "        pat ='→'\n",
    "        syne2 = re.split(pat,df_stp.loc[i,'synapse_type_2'])\n",
    "\n",
    "    syne3 ={'layer_pre':l1,  'cre_line_pre':'none', 'cell_type2_pre':str.strip(syne2[0]),'cluster_pre':[],\n",
    "            'layer_post':l, 'cre_line_post':'none','cell_type2_post':str.strip(syne2[1]),'cluster_post':[],  } \n",
    "\n",
    "    # add cre-lines when available\n",
    "    if len(syne1)>1:\n",
    "        syne3['cre_line_pre']  =str.strip(syne1[0])\n",
    "        syne3['cre_line_post'] =str.strip(syne1[1])\n",
    "    \n",
    "    # PV BC\n",
    "    if (syne3['cell_type2_post']=='PV')|(syne3['cell_type2_post']=='PVBC'):  \n",
    "        syne3['cluster_post']=[['Pvalb_3, Pvalb_10, Pvalb_4, Pvalb_6',100]] # modify!!! \n",
    "    if (syne3['cell_type2_pre']=='PV')|(syne3['cell_type2_pre']=='PVBC'):  \n",
    "        syne3['cluster_pre']=[['Pvalb_3, Pvalb_10, Pvalb_4, Pvalb_6',100]] # modify!!!   \n",
    "                     \n",
    "  \n",
    "    # HIPP -> HIPP Bartos\n",
    "    if syne3['cell_type2_post']=='HIPP':  \n",
    "        if syne3['layer_post'] == 'DG':\n",
    "            syne3['cluster_post']=[['Sst_15, Sst_24, Sst_13, Sst_6',100]] # modify!!!        \n",
    "    if syne3['cell_type2_pre']=='HIPP':  \n",
    "        if syne3['layer_pre'] == 'DG':\n",
    "            syne3['cluster_pre']=[['Sst_15, Sst_24, Sst_13, Sst_6',100]] # modify!!! \n",
    "            \n",
    "            \n",
    "    # HICAP -> HICAP Bartos\n",
    "    if syne3['cell_type2_post']=='HICAP':  \n",
    "        if syne3['layer_post'] == 'DG':\n",
    "            syne3['cluster_post']=[['Sncg6, Sncg/Ndnf HPF_1',100]] # modify!!!        \n",
    "    if syne3['cell_type2_pre']=='HICAP':  \n",
    "        if syne3['layer_pre'] == 'DG':\n",
    "            syne3['cluster_pre']=[['Sncg6, Sncg/Ndnf HPF_1',100]] # modify!!!      \n",
    "            \n",
    "            \n",
    "    # O-LM\n",
    "    if syne3['cell_type2_post']=='O-LM':  \n",
    "        if syne3['layer_post'] == 'CA1':\n",
    "            syne3['cluster_post']=[['Sst_15, Sst_24, Sst_13, Sst_6',100]] # modify!!!     \n",
    "            \n",
    "    # CCK BC\n",
    "    if syne3['cell_type2_post']=='CCKBC':  \n",
    "        syne3['cluster_post']=[['Sncg/Ndnf HPF_2, Sncg/Ndnf HPF_3, Sncg/Ndnf HPF_6, Serpinf1_1, Sncg_7, Vip_14',100]] # modify!!!    \n",
    "    if syne3['cell_type2_pre']=='CCKBC':  \n",
    "        syne3['cluster_pre']=[['Sncg/Ndnf HPF_2, Sncg/Ndnf HPF_3, Sncg/Ndnf HPF_6, Serpinf1_1, Sncg_7, Vip_14',100]] # modify!!!    \n",
    "\n",
    "    \n",
    "    # CCK DTI\n",
    "    if syne3['cell_type2_post']=='CCK_DTI':  \n",
    "        syne3['cluster_post']=[['Sncg/Ndnf HPF_1, Sncg6, Sncg2',100]] # modify!!!    \n",
    "    if syne3['cell_type2_pre']=='CCK_DTI':  \n",
    "        syne3['cluster_pre']=[['Sncg/Ndnf HPF_1, Sncg6, Sncg2',100]] # modify!!!    \n",
    "\n",
    "        \n",
    "        \n",
    "    # O-Bi\n",
    "    if syne3['cell_type2_post']=='O-Bi':  \n",
    "        if syne3['layer_post'] == 'CA1':\n",
    "            syne3['cluster_post']=[['Sst_27, Pvalb_1, Sst_25',100]] # modify!!!   \n",
    "     \n",
    "    # AAC\n",
    "    if syne3['cell_type2_pre']=='AAC':  \n",
    "        if syne3['layer_pre'] == 'CA3':\n",
    "            syne3['cluster_pre']=[['Pvalb_12, Pvalb_11',100]] # modify!!!   \n",
    "    if syne3['cell_type2_post']=='AAC':  \n",
    "        if syne3['layer_post'] == 'CA3':\n",
    "            syne3['cluster_post']=[['Pvalb_12, Pvalb_11',100]] # modify!!!         \n",
    "\n",
    "    # NGF    ? type2 vs type1? - remove type 1?  \n",
    "    if syne3['cell_type2_pre']=='NGF':  \n",
    "        syne3['cluster_pre']=[['Lamp5_2, Lamp5_3, Lamp5_4, Lamp5_5, Lamp5 Lhx6_2, Lamp5 Lhx6_3',100]] # modify!!! - only ~35 cells? - selected by anterior and mid-anterior subregions       \n",
    "    if (syne3['cell_type2_post']=='NGF')|(syne3['cell_type2_post']=='NGF type1')|(syne3['cell_type2_post']=='NGF type2'):  \n",
    "        syne3['cluster_post']=[['Lamp5_2, Lamp5_3, Lamp5_4, Lamp5_5, Lamp5 Lhx6_2, Lamp5 Lhx6_3',100]] # modify!!! - only ~35 cells? - selected by anterior and mid-anterior subregions   \n",
    "     \n",
    "    # Ivy cells   ? type2 vs type1? : CGE vs MGE? - no CGE is seen? or Lhs1 vs Lhs4? \n",
    "    if syne3['cell_type2_pre']=='Ivy cell':  \n",
    "        syne3['cluster_pre']=[['Lamp5 Lhx6_1, Lamp5 Lhx6_4',100]] # modify!!! - only ~35 cells? - selected by anterior and mid-anterior subregions       \n",
    "    if (syne3['cell_type2_post']=='Ivy cell')|(syne3['cell_type2_post']=='Ivy cell type1')|(syne3['cell_type2_post']=='Ivy cell type2'):  \n",
    "        syne3['cluster_post']=[['Lamp5 Lhx6_1, Lamp5 Lhx6_4',100]] # modify!!! - only ~35 cells? - selected by anterior and mid-anterior subregions   \n",
    "       \n",
    "    #IS3\n",
    "    if syne3['cell_type2_pre']=='IS3':  \n",
    "        syne3['cluster_pre']=[['Vip_6, Vip_12, Vip_15, Vip_17',100]] # modify!!! - only ~35 cells? - selected by anterior and mid-anterior subregions       \n",
    "    \n",
    "    ###### EXCitatory\n",
    "    # CA1 PC        \n",
    "    if (syne3['cell_type2_pre']=='CA1'): \n",
    "        syne3['cell_type2_pre']=='PC'\n",
    "        if syne3['layer_pre'] == 'CA1':\n",
    "            syne3['cluster_pre']=[['CA1sp_7, CA1sp_8',100]] # modify!!! - only ~35 cells? - selected by anterior and mid-anterior subregions       \n",
    "    if (syne3['cell_type2_post']=='CA1'):  \n",
    "        syne3['cell_type2_post']=='PC'\n",
    "        if syne3['layer_post'][0:3] == 'CA1':\n",
    "            syne3['cluster_post']=[['CA1sp_7, CA1sp_8',100]] # modify!!! - only ~35 cells? - selected by anterior and mid-anterior subregions   \n",
    " \n",
    "            \n",
    "    # CA3 PC        \n",
    "    if (syne3['cell_type2_pre']=='CA3'): \n",
    "        syne3['cell_type2_pre']=='PC'\n",
    "        if syne3['layer_pre'] == 'CA3':\n",
    "            syne3['cluster_pre']=[['CA3sp_6, CA3sp_5, CA3sp_7, CA3sp_1',100]] # modify!!! - only ~35 cells? - selected by anterior and mid-anterior subregions       \n",
    "    if (syne3['cell_type2_post']=='CA3'):  \n",
    "        syne3['cell_type2_post']=='PC'\n",
    "        if syne3['layer_post'] == 'CA3':\n",
    "            syne3['cluster_post']=[['CA3sp_6, CA3sp_5, CA3sp_7, CA3sp_1',100]] # modify!!! - only ~35 cells? - selected by anterior and mid-anterior subregions   \n",
    "    \n",
    "    # CA3 PC ventral\n",
    "    if (syne3['cell_type2_pre']=='CA3')|(syne3['cell_type2_pre']=='PC'): \n",
    "        syne3['cell_type2_pre']=='PC'\n",
    "        if (syne3['layer_pre'] == 'CA3 ventral')&(syne3['layer_post'] == 'CA1 ventral'):\n",
    "            syne3['cluster_pre']=[['CA3sp_2, CA3sp_4',100]] # modify!!! - only ~35 cells? - selected by anterior and mid-anterior subregions   \n",
    "\n",
    "    \n",
    "    \n",
    " \n",
    "    # ECII SS        \n",
    "    if (syne3['cell_type2_pre']=='EC'): \n",
    "        syne3['cell_type2_pre']=='PC'\n",
    "        if (syne3['layer_pre'] == 'EC')&((syne3['layer_post'] == 'DG')|(syne3['layer_post'] == 'CA3')):\n",
    "            syne3['cluster_pre']=[['L2/3 IT Ndst4 Endou_2',100]] # modify!!! - only ~35 cells? - selected by anterior and mid-anterior subregions       \n",
    "\n",
    "\n",
    "    # ECIII PC        \n",
    "    if (syne3['cell_type2_pre']=='EC'):\n",
    "        syne3['cell_type2_pre']=='PC'\n",
    "        if (syne3['layer_pre'] == 'EC')&((syne3['layer_post'][0:3] == 'CA1')):\n",
    "            syne3['cluster_pre']=[['L2/3 IT Plch1_1, L2/3 IT Plch1_2',100]] # modify!!! - only ~35 cells? - selected by anterior and mid-anterior subregions       \n",
    "\n",
    "            \n",
    "    # CA1 PC ventral       \n",
    "    if (syne3['cell_type2_pre']=='CA1 ventral'):  \n",
    "        syne3['cell_type2_pre']=='PC'\n",
    "        if syne3['layer_pre'] == 'CA1 ventral':\n",
    "            syne3['cluster_pre']=[['CA1sp_1, CA1sp_2, CA1sp_3',100]] # modify!!! - only ~35 cells? - selected by anterior and mid-anterior subregions       \n",
    "    if (syne3['cell_type2_post']=='CA1 ventral'):  \n",
    "        syne3['cell_type2_post']=='PC'\n",
    "        if syne3['layer_post'] == 'CA1 ventral':\n",
    "            syne3['cluster_post']=[['CA1sp_1, CA1sp_2, CA1sp_3',100]] # modify!!! - only ~35 cells? - selected by anterior and mid-anterior subregions   \n",
    "    \n",
    "    \n",
    "\n",
    "    # DG PC      \n",
    "    if (syne3['cell_type2_pre']=='DG'):\n",
    "        syne3['cell_type2_pre']=='PC'\n",
    "        #if syne3['layer_pre'] == 'DG':\n",
    "        syne3['cluster_pre']=[['DG_1, DG_2, DG_3',100]] # modify!!! - only ~35 cells? - selected by anterior and mid-anterior subregions       \n",
    "    if (syne3['cell_type2_post']=='DG'): \n",
    "        syne3['cell_type2_post']=='PC'\n",
    "        #if syne3['layer_post'] == 'DG':\n",
    "        syne3['cluster_post']=[['DG_1, DG_2, DG_3',100]] # modify!!! - only ~35 cells? - selected by anterior and mid-anterior subregions   \n",
    "   \n",
    "    \n",
    "    \n",
    "    ###########  Select data:\n",
    "    ##synij = (pre_post.loc[:,0].isin(pree))&(pre_post.loc[:,1].isin(poste))\n",
    "    \n",
    "\n",
    "    synij[:] = False\n",
    "    synij[i] = in_stp_data[i]\n",
    "    \n",
    "    #print(syne3)\n",
    "    \n",
    "    if sum(synij)>0:\n",
    "        print('\\n\\n')\n",
    "        print('i = ',str(i))\n",
    "        print(syne3)\n",
    "        #if sum(in_stp_data&synij)>0:\n",
    "        #    in_stp_data = in_stp_data&synij\n",
    "\n",
    "        #add ge data\n",
    "        #'layer_pre':l,  'cre_line_pre':'', 'cell_type2_pre':syne2[0].str.strip(),'cluster_pre':[]\n",
    "        \n",
    "        # PC or GABAergic dataset?\n",
    "        batch = [ syne3['cell_type2_pre']=='PC',syne3['cell_type2_post']=='PC']\n",
    "        \n",
    "        # add ge data depending on available stp cell types labels:\n",
    "        #  select presynaptic cells\n",
    "        if 0: #batch[0]:\n",
    "            #in_preg  = df_aba_vis_l23glu_c.loc[:,'brain_subregion'].isin([syne3['layer_pre']]) #modify!!!\n",
    "            in_preg  = df_aba_vis_l23glu_c.loc[:,'brain_subregion']==syne3['layer_pre'] #modify!!!\n",
    "        else:\n",
    "            if len(syne3['cluster_pre'])==0:\n",
    "                # if len(syne3['cre_line_pre'])>0:\n",
    "                in_preg = df_aba_vis_in_c.loc[:,'full_genotype_label'].str.contains(syne3['cre_line_pre'])\n",
    "                ##in_preg = in_preg&df_aba_vis_in_c.loc[:,'brain_subregion'].isin([syne3['layer_pre']])\n",
    "                #in_preg = in_preg&df_aba_vis_in_c.loc[:,'brain_subregion'].str.contains(syne3['layer_pre'])\n",
    "            else:                  \n",
    "                ##in_preg = df_aba_vis_in_c.loc[:,'brain_subregion'].isin([syne3['layer_pre']])\n",
    "                in_preg = df_aba_vis_in_c.loc[:,'region_label'].str.contains(syne3['layer_pre'])\n",
    "                in_preg[:] = True\n",
    "                \n",
    "                cl = syne3['cluster_pre']\n",
    "                #cols2 = ['name','perc']\n",
    "                #cl2pre =pd.DataFrame([],columns= cols2)\n",
    "                in_preg0 = in_preg.copy()\n",
    "                in_preg[:] = False \n",
    "                for icl in range(len(cl)):\n",
    "                    clp = cl[icl][1]\n",
    "                    clns = re.split(',',cl[icl][0])\n",
    "                    cls3 = []\n",
    "                    cls3m=[]\n",
    "                    for icls in range(len(clns)):\n",
    "                        clns4 = str.strip(clns[icls])\n",
    "                        clns5 = re.split('\\_',clns4)\n",
    "                        if len(clns5)<2:\n",
    "                            clns6=clns5[0]\n",
    "                            cls3=cls3 + list(set(df_aba_vis_in_c.loc[df_aba_vis_in_c.loc[:,'subclass_label'].str.contains(clns6),'cluster_label']))\n",
    "                        else:\n",
    "                            clns4=str.strip(clns4)\n",
    "                            clns7=re.split('\\-',clns4)\n",
    "                            if len(clns7)>1:\n",
    "                                clns4=clns7[0]\n",
    "                                cls3m=cls3m+[str.strip(clns4)]\n",
    "                            else:\n",
    "                                cls3=cls3+[str.strip(clns4)]\n",
    "                    cls3 = list(set(cls3).difference(set(cls3m)))\n",
    "                    cl22=pd.DataFrame([cls3,[clp]],index=cols2).T\n",
    "                    cl2pre = cl2pre.append(cl22)\n",
    "                    \n",
    "                    in_preg = in_preg|(in_preg0&df_aba_vis_in_c.loc[:,'cluster_label'].isin(cls3))\n",
    "                    \n",
    "                if (len(syne3['cre_line_pre'])>0)&(syne3['cre_line_pre']!='none'):\n",
    "                    #in_preg = in_preg&df_aba_vis_in_c.loc[:,'driver_lines'].str.contains(syne3['cre_line_pre'])\n",
    "                    in_line=df_aba_vis_in_c.loc[:,'full_genotype_label'].str.contains(syne3['cre_line_pre'])\n",
    "                    print('is pre cre line')\n",
    "                    if in_line.sum()>0:\n",
    "                        in_preg = in_preg&in_line                        \n",
    "        \n",
    "        #  select postsynaptic cells\n",
    "        if 0: #batch[1]:\n",
    "            #in_postg = df_aba_vis_l23glu_c.loc[:,'brain_subregion'].isin([syne3['layer_post']]) #modify!!!\n",
    "            in_postg = df_aba_vis_l23glu_c.loc[:,'brain_subregion']==syne3['layer_post'] #modify!!!\n",
    "        else:    \n",
    "            if len(syne3['cluster_post'])==0:\n",
    "                # if len(syne3['cre_line_post'])>0:\n",
    "                in_postg = df_aba_vis_in_c.loc[:,'full_genotype_label'].str.contains(syne3['cre_line_post'])\n",
    "                #in_postg = in_postg&df_aba_vis_in_c.loc[:,'brain_subregion'].isin([syne3['layer_post']])\n",
    "                #in_postg = in_postg&df_aba_vis_in_c.loc[:,'brain_subregion_label'].str.contains(syne3['layer_post'])\n",
    "            else:    \n",
    "                ## under construction ...\n",
    "                ##in_postg = df_aba_vis_in_c.loc[:,'brain_subregion'].isin([syne3['layer_post']])\n",
    "                in_postg = df_aba_vis_in_c.loc[:,'region_label'].str.contains(syne3['layer_post'])\n",
    "                in_postg[:] = True\n",
    "                \n",
    "                cl = syne3['cluster_post']\n",
    "                #cols2 = ['name','perc']\n",
    "                #cl2post =pd.DataFrame([],columns= cols2)\n",
    "                in_postg0 = in_postg.copy()\n",
    "                in_postg[:] = False                 \n",
    "                for icl in range(len(cl)):\n",
    "                    clp = cl[icl][1]\n",
    "                    clns = re.split(',',cl[icl][0])\n",
    "                    cls3 = []\n",
    "                    cls3m=[]\n",
    "                    for icls in range(len(clns)):\n",
    "                        clns4 = str.strip(clns[icls])\n",
    "                        clns5 = re.split('\\_',clns4)\n",
    "                        if len(clns5)<2:\n",
    "                            clns6=clns5[0]\n",
    "                            cls3=cls3 + list(set(df_aba_vis_in_c.loc[df_aba_vis_in_c.loc[:,'subclass_label'].str.contains(clns6),'cluster_label']))\n",
    "                        else:\n",
    "                            clns4=str.strip(clns4)\n",
    "                            clns7=re.split('\\-',clns4)\n",
    "                            if len(clns7)>1:\n",
    "                                clns4=clns7[0]\n",
    "                                cls3m=cls3m+[str.strip(clns4)]\n",
    "                            else:\n",
    "                                cls3=cls3+[str.strip(clns4)]\n",
    "                    cls3 = list(set(cls3).difference(set(cls3m)))\n",
    "                    cl22=pd.DataFrame([cls3,[clp]],index=cols2).T\n",
    "                    cl2post = cl2post.append(cl22)\n",
    "                    \n",
    "                    #in_postg = in_postg&df_aba_vis_in_c.loc[:,'cluster'].isin(cls3)\n",
    "                    in_postg = in_postg|(in_postg0&df_aba_vis_in_c.loc[:,'cluster_label'].isin(cls3))\n",
    "                    \n",
    "                if (len(syne3['cre_line_post'])>0)&(syne3['cre_line_post']!='none'):\n",
    "                    in_line=df_aba_vis_in_c.loc[:,'full_genotype_label'].str.contains(syne3['cre_line_post'])\n",
    "                    print('is post cre line')\n",
    "                    if in_line.sum()>0:\n",
    "                        in_postg = in_postg&in_line\n",
    "            \n",
    "           \n",
    "            \n",
    "        # make equal size pre- and post- datasets\n",
    "        in_preg = in_preg&(in_preg.isna()==False)\n",
    "        in_postg = in_postg&(in_postg.isna()==False)\n",
    "        npre = in_preg.sum()\n",
    "        npost = in_postg.sum()\n",
    "        nsample = max([npost, npre] )\n",
    "        #nsample = min([npost, npre] ) # number of cells\n",
    "        \n",
    "        if (nsample<60)&(npre>0)&(npost>0):\n",
    "            nsample=60\n",
    "            \n",
    "        if (nsample>180)&(npre>0)&(npost>0):  # restrict nsample to avoid overrepresentation?\n",
    "            nsample=180    \n",
    "            \n",
    "        print('npre = '+ str(npre)+' , npost = ' + str(npost) + ' , nsample = '+str(nsample))    \n",
    "        \n",
    "        \n",
    "        ## select cell indices for gene expression sampling\n",
    "        #do_bootstrap=1\n",
    "        #if  do_bootstrap==0:\n",
    "        #    idx2 =   np.mod(np.arange(nsample),npre) \n",
    "        #    in_preg = np.nonzero(in_preg)[0][idx2]\n",
    "        #    idx2 =   np.mod(np.arange(nsample),npost) \n",
    "        #    in_postg = np.nonzero(in_postg)[0][idx2]\n",
    "        #else :\n",
    "        #    #random.uniform(low=0.0, high=1.0, size=None)\n",
    "        #    idx2 =  np.floor( np.random.uniform(0,npre, nsample) ).astype('int')\n",
    "        #    in_preg = np.nonzero(in_preg)[0][idx2]\n",
    "        #    idx2 =  np.floor( np.random.uniform(0,npost, nsample) ).astype('int')\n",
    "        #    in_postg = np.nonzero(in_postg)[0][idx2]\n",
    "            \n",
    "            \n",
    "        # select cell indices for gene expression sampling\n",
    "        do_bootstrap=1\n",
    "        if  do_bootstrap==0:\n",
    "            \n",
    "            if (nsample<60)&(npre>0)&(npost>0):\n",
    "                nsample=60\n",
    "            \n",
    "            if (nsample>180)&(npre>0)&(npost>0):  # restrict nsample to avoid overrepresentation?\n",
    "                nsample=180 \n",
    "            \n",
    "            idx2 =   np.mod(np.arange(nsample),npre) \n",
    "            in_preg = np.nonzero(in_preg)[0][idx2]\n",
    "            idx2 =   np.mod(np.arange(nsample),npost) \n",
    "            in_postg = np.nonzero(in_postg)[0][idx2]\n",
    "        else :\n",
    "            if (npre>0)&(npost>0):\n",
    "                nsample=200\n",
    "            #random.uniform(low=0.0, high=1.0, size=None)\n",
    "            idx2pre =  np.floor( np.random.uniform(0,npre, (nsample,1)) ).astype('int')\n",
    "            in_preg_bs = np.nonzero(in_preg)[0][idx2pre]\n",
    "            idx2post =  np.floor( np.random.uniform(0,npost, (nsample,1)) ).astype('int')\n",
    "            in_postg_bs = np.nonzero(in_postg)[0][idx2post]\n",
    "            in_preg_ = in_preg_bs[:,0]\n",
    "            in_postg_ = in_postg_bs[:,0]    \n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        if batch[0]:\n",
    "            #in_preg_cl = list(set(df_aba_vis_l23glu_c.iloc[in_preg,:].loc[:,'cluster']))\n",
    "            in_preg_cl = df_aba_vis_in_c.iloc[in_preg_,:].loc[:,'cluster_label'].value_counts()\n",
    "        else:\n",
    "            #in_preg_cl = list(set(df_aba_vis_in_c.iloc[in_preg,:].loc[:,'cluster']))\n",
    "            in_preg_cl = df_aba_vis_in_c.iloc[in_preg_,:].loc[:,'cluster_label'].value_counts()\n",
    "        \n",
    "        if batch[1]:\n",
    "            #in_postg_cl = list(set(df_aba_vis_l23glu_c.iloc[in_postg,:].loc[:,'cluster']))\n",
    "            in_postg_cl = df_aba_vis_in_c.iloc[in_postg_,:].loc[:,'cluster_label'].value_counts()\n",
    "        else:        \n",
    "            #in_postg_cl = list(set(df_aba_vis_in_c.iloc[in_postg,:].loc[:,'cluster']))\n",
    "            in_postg_cl = df_aba_vis_in_c.iloc[in_postg_,:].loc[:,'cluster_label'].value_counts()\n",
    "        in_preg_cl = in_preg_cl/in_preg_cl.sum()*100    \n",
    "        in_postg_cl = in_postg_cl/in_postg_cl.sum()*100    \n",
    "        print('presynaptic clusters')\n",
    "        print(in_preg_cl)\n",
    "        print('postsynaptic clusters')\n",
    "        print(in_postg_cl)\n",
    "\n",
    "        #if npost>npre:\n",
    "        #    idx = np.mod(np.arange(npost),npre)\n",
    "        #    #prege = prege[idx,:]\n",
    "        #    #postge = postge\n",
    "        #    prege = prege\n",
    "        #    postge = postge[0:npre,:]            \n",
    "        #elif npost<npre :\n",
    "        #    idx = np.mod(np.arange(npre),npost)\n",
    "        #    #prege = prege\n",
    "        #    #postge = postge[idx,:]\n",
    "        #    prege = prege[0:npost,:]\n",
    "        #    postge = postge         \n",
    "        \n",
    "        # take ge data    \n",
    "        #prege = vis_dat.X[:, in_pregs]  # presynaptic gene set expression\n",
    "        ##prege = prege[(vbi==batch[0]), :]\n",
    "        #prege = prege[in_preg,:]\n",
    "\n",
    "        #postge = vis_dat.X[:,  in_postgs] # postsynaptic gene set expression\n",
    "        ##postge = postge[(vbi==batch[1]),  :]\n",
    "        #postge = postge[in_postg,:]\n",
    "        \n",
    "        \n",
    "        \n",
    "        prege = vis_dat.X[:, in_pregs]  # presynaptic gene set expression\n",
    "        prege = prege[(vbi==batch[0]), :]\n",
    "        \n",
    "        postge = vis_dat.X[:,  in_postgs] # postsynaptic gene set expression\n",
    "        postge = postge[(vbi==batch[1]),  :]\n",
    "        \n",
    "        do_median=1\n",
    "        if  (do_bootstrap==0)|(do_median==0):\n",
    "            prege = prege[in_preg,:]\n",
    "            postge = postge[in_postg,:]\n",
    "        else: \n",
    "            nsample = 100\n",
    "            Nbs = nsample\n",
    "            ##random.uniform(low=0.0, high=1.0, size=None)\n",
    "            #nmax=5\n",
    "            nmax = npre #np.min([1000,npre])\n",
    "            idx2pre =  np.floor( np.random.uniform(0,npre, (nmax,Nbs)) ).astype('int')\n",
    "            in_preg_ = np.nonzero(in_preg)[0][np.arange(npre)]\n",
    "            nmax = npost #np.min([1000,npost])\n",
    "            idx2post =  np.floor( np.random.uniform(0,npost, (nmax,Nbs)) ).astype('int')\n",
    "            in_postg_ = np.nonzero(in_postg)[0][np.arange(npost)]\n",
    "\n",
    "                        # \n",
    "            # ??? transform ge to median bootstraped log ???\n",
    "            prege_ = prege[in_preg_,:]\n",
    "            postge_ = postge[in_postg_,:]\n",
    "\n",
    "            y = np.ma.masked_where(prege_ == 0, prege_)\n",
    "            prege_=np.ma.log2(y).filled(0)\n",
    "            y = np.ma.masked_where(postge_ == 0, postge_)\n",
    "            postge_=np.ma.log2(y).filled(0)\n",
    "\n",
    "\n",
    "\n",
    "            npregs=prege_.shape[1]\n",
    "            ge_bs = np.zeros((Nbs,npregs+postge_.shape[1]))\n",
    "            for ibs in range(Nbs):\n",
    "                #ans=np.apply_along_axis(lambda v: np.median(v[v!=0]), 0, x)\n",
    "                #ans[np.isnan(ans)]=0.\n",
    "                xbs = prege_[idx2pre[:,ibs],:]\n",
    "                pregbs  = np.apply_along_axis(lambda v: np.median(v[v!=0]), 0, xbs)\n",
    "                pregbs[np.isnan(pregbs)]=0.\n",
    "                xbs = postge_[idx2post[:,ibs],:]\n",
    "                postgbs = np.apply_along_axis(lambda v: np.median(v[v!=0]), 0, xbs)\n",
    "                postgbs[np.isnan(postgbs)]=0.\n",
    "\n",
    "                ge_bs[ibs,0:npregs] =pregbs\n",
    "                ge_bs[ibs,npregs:]  =postgbs\n",
    "\n",
    "\n",
    "            #df_hr2 = np.log2(df_hr2d)\n",
    "            #df_hr2 = pd.concat([df_hr2,df_hr2s],axis=1) # log2 normalization\n",
    "            #df_hr2 = df_hr2.replace({0:np.nan})\n",
    "            #df_hr2 = np.log2(df_hr2)\n",
    "\n",
    "            #df_hr2 = pd.concat([df_hr.iloc[1:,0],df_hr2],axis=1)\n",
    "\n",
    "            #df_hr2.dtypes\n",
    "            #df_hr2.head()\n",
    "\n",
    "            #df_hr2 = df_hr2.replace({0:np.nan})\n",
    "            #df_hrmg=df_hr2.groupby(['Class']) # logarithm_2 of UMI counts\n",
    "            #df_hrmg=df_hrmg.median(numeric_only=True) # median of log2(UMI)\n",
    "\n",
    "            #df_hrmg0 = df_hrmg.iloc[:,0:-1]\n",
    "\n",
    "            #df_hrmsg = pd.concat([df_hrmg.iloc[:,-1]]*df_hrmg0.shape[1],axis=1)\n",
    "            #df_hrmsg.columns = df_hrmg.iloc[:,0:-1].columns\n",
    "\n",
    "            #df_hrmg = df_hrmg.iloc[:,0:-1] - df_hrmsg +6*np.log2(10)  # transform from log2( UMI counts) to log2(CPM)\n",
    "\n",
    "            # ??? transform ge to median bootstraped log ???\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "              \n",
    "\n",
    "        ## add data to the final dataset    \n",
    "        #gnij = np.repeat(np.array([syne3['cell_type2_pre'], syne3['cell_type2_post'],\n",
    "        #                           syne3['layer_pre'], syne3['layer_post'],\n",
    "        #                           syne3['cre_line_pre'], syne3['cre_line_post']]).reshape((1,6)),nsample,axis=0 )\n",
    "        #gnij = pd.DataFrame(gnij, columns = ['cell_type2_pre', 'cell_type2_post',\n",
    "        #                                     'layer_pre','layer_post',\n",
    "        #                                     'cre_line_pre', 'cre_line_post'] )\n",
    "        #prege  = pd.DataFrame(prege,columns  = pregs3 )\n",
    "        #postge = pd.DataFrame(postge,columns = postgs3 )\n",
    "\n",
    "\n",
    "        #ge_dataij = pd.concat([gnij, prege, postge ], axis = 1)\n",
    "        \n",
    "        \n",
    "        # add data to the final dataset    \n",
    "        gnij = np.repeat(np.array([syne3['cell_type2_pre'], syne3['cell_type2_post'],\n",
    "                                   syne3['layer_pre'], syne3['layer_post'],\n",
    "                                   syne3['cre_line_pre'], syne3['cre_line_post']]).reshape((1,6)),nsample,axis=0 )\n",
    "        gnij = pd.DataFrame(gnij, columns = ['cell_type2_pre', 'cell_type2_post',\n",
    "                                             'layer_pre','layer_post',\n",
    "                                             'cre_line_pre', 'cre_line_post'] )\n",
    "        \n",
    "        if  (do_bootstrap==0)|(do_median==0):\n",
    "            prege  = pd.DataFrame(prege,columns  = pregs3 )\n",
    "            postge = pd.DataFrame(postge,columns = postgs3 )\n",
    "\n",
    "\n",
    "            ge_dataij = pd.concat([gnij, prege, postge ], axis = 1)\n",
    "            \n",
    "        else:\n",
    "            ge_bs = pd.DataFrame(ge_bs,columns = pregs3 + postgs3 )\n",
    "\n",
    "\n",
    "            ge_dataij = pd.concat([gnij, ge_bs ], axis = 1)\n",
    "\n",
    "       \n",
    "        \n",
    "        \n",
    "        # add stp data\n",
    "        # stp_dataij =  df_stp.loc[[0,i], stp_type['stp_data_type']!=0]\n",
    "        stp_dataij =  df_stp.loc[[0,i], df_stp.loc[0, :]!=0]\n",
    "        \n",
    "\n",
    "        i2 = np.trunc(np.arange(len(stp_dataij.columns))/len(stp_columns)).astype(int)\n",
    "        stp_dataij  = stp_dataij.T.set_index(i2).pivot(columns=0,values=i).loc[:,stp_columns] #ss.set_index('par',append=True).unstack('par')\n",
    "        #stp_dataij =  stp_dataij.T.pivot(index=0, values=synij).T\n",
    "        \n",
    "        idx = np.mod(np.arange(nsample),len(stp_dataij.index))\n",
    "        stp_dataij=stp_dataij.iloc[idx,:].reset_index(drop=True)  \n",
    "        \n",
    "        i_data_set = pd.DataFrame(i+np.zeros(len(stp_dataij.index)), index = stp_dataij.index, columns = ['index_ds'])\n",
    "        stp_dataij=pd.concat([stp_dataij, i_data_set],axis=1)\n",
    "        \n",
    "        #if do_averagig_stp:\n",
    "        #    #stp_dataij.loc[:,:] = stp_dataij.mean(axis=0).values\n",
    "        #    stp_dataij.loc[:,:] = stp_dataij.median(axis=0).values\n",
    "\n",
    "        \n",
    "        #stp_dataij = pd.DataFrame({'presynaptic':pree[0], 'postsynaptic': poste[0], \n",
    "        #                           'stp_data':stp_dataij.iloc[0,:].values})\n",
    "        #stp_data  = stp_data.append(stp_dataij) \n",
    "        \n",
    "        # names of ge and stp columns\n",
    "\n",
    "        \n",
    "        ge_dataij  = pd.concat([ge_dataij,stp_dataij] , axis=1)\n",
    "        \n",
    "        # add to all ge and stp dataframe\n",
    "        ge_data  = pd.concat([ge_data, ge_dataij] , axis=0)\n",
    "        \n",
    "#stp_data =  stp_data.reset_index(drop=True)  \n",
    "\n",
    "\n",
    "\n",
    "#stp_data.loc[stp_data.loc[:,'postsynaptic']=='NGC','postsynaptic']='Lamp5'\n",
    "#stp_data2 = ge_data.iloc[:,0:3].copy(deep=True)                                 # modify!\n",
    "#stp_data2.columns=['presynaptic', 'postsynaptic', 'ppr']\n",
    "#print(stp_data2.head())\n",
    "#for i in stp_data.index: #stp_data.index:\n",
    "#    pre=stp_data.loc[i,'presynaptic']\n",
    "#    post=stp_data.loc[i,'postsynaptic']\n",
    "#    stp_data2.loc[stp_data2.loc[:,'presynaptic'].isin([pre])&stp_data2.loc[:,'postsynaptic'].isin([post]),'ppr']=stp_data.loc[i,'stp_data'] \n",
    "    \n",
    "ge_data = ge_data.reset_index()    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# transform labels from TM to An:A1\n",
    "#f = 20 # Hz\n",
    "#N = 5\n",
    "#T = np.arange(N)*1000/f\n",
    "\n",
    "\n",
    "#As, ns, ps, dpp0, p0, tF, tD, A = STP_sim(ge_data,T)\n",
    "As = STP_sim_complex(ge_data,ge_data.shape[0],[])\n",
    "N  = As.shape[1]\n",
    "\n",
    "#As =  As[:,1:]/np.tile(As[:,0],(N-1,1)).T\n",
    "As =  As[:,1:]/np.tile(As[:,0],(N-1,1)).T\n",
    "\n",
    "#As =  As[:,1:]/np.tile(As[:,0],(N-1,1)).T\n",
    "\n",
    "# add new stp parameters to ge_data dataset\n",
    "stp_columns2 = ['A'+str(s)+'/A1' for s in range(2,N+1)]\n",
    "stp_data2 = pd.DataFrame(As,columns = stp_columns2, index = ge_data.index)\n",
    "\n",
    "ge_data = pd.concat([ge_data, stp_data2 ],axis=1)\n",
    "\n",
    "if do_averagig_stp:\n",
    "     #stp_dataij.loc[:,:] = stp_dataij.mean(axis=0).values\n",
    "     #stp_dataij.loc[:,:] = stp_dataij.median(axis=0).values\n",
    "    all_i_ds =  ge_data.loc[:,'index_ds']                             \n",
    "    for i_ds in list(set(all_i_ds)):\n",
    "              ids2 =  ge_data.loc[:,'index_ds'].isin([i_ds])  \n",
    "              ge_data.loc[ids2,stp_columns2] = ge_data.loc[ids2,stp_columns2].median(axis=0).values                   \n",
    "                                   \n",
    "                                   \n",
    "## filter TM parameters\n",
    "do_filter_pars = 0\n",
    "\n",
    "if do_filter_pars!=0:\n",
    "    print((ge_data.loc[:,stp_columns ]<0).sum().sum())\n",
    "    print(len(ge_data.index))\n",
    "\n",
    "    nannot=6\n",
    "    annot_columns = ge_columns[0:nannot]\n",
    "    ge_data2 = ge_data.loc[:,annot_columns + stp_columns2].groupby(annot_columns).mean()\n",
    "    f,ax = plt.subplots(figsize=(5, 5))\n",
    "    ge_data2.T.plot(ax=ax)\n",
    "    plt.gca().legend('')\n",
    "    #plt.gca().title('An:A1 before filtering data')\n",
    "    ax.title.set_text('An:A1 before filtering data')\n",
    "    #ge_data2 = ge_data.loc[:,annot_columns + stp_columns2].groupby(annot_columns).mean()\n",
    "    print('mean of stp before filtering:')\n",
    "    print(ge_data2)\n",
    "\n",
    "    #i_filter_out = (ge_data.loc[:,'dp/p0']*ge_data.loc[:,'p0']+)|(ge_data.loc[:,'tD'])\n",
    "    #i_filter_out = i_filter_out&((ge_data.loc[:,'tF'])|(ge_data.loc[:,'tF']))\n",
    "    i_filter_out = np.sum(ps2>1,axis=1)|np.sum(ps2<0,axis=1)\n",
    "    i_filter_out = i_filter_out|(np.sum(ns2>1,axis=1)|np.sum(ns2<0,axis=1))\n",
    "    ge_data = ge_data.loc[i_filter_out==False ,:]\n",
    "\n",
    "    ge_data2 = ge_data.loc[:,annot_columns + stp_columns2].groupby(annot_columns).mean()\n",
    "    f1,ax1 = plt.subplots(figsize=(5, 5))\n",
    "    ge_data2.T.plot(ax=ax1)\n",
    "    plt.gca().legend('')\n",
    "    #plt.gca().title('An:A1 after filtering data')\n",
    "    ax1.title.set_text('An:A1 after filtering data')\n",
    "    #ge_data2 = ge_data.loc[:,annot_columns + stp_columns2].groupby(annot_columns).mean()\n",
    "\n",
    "    print('mean of stp after filtering:')\n",
    "    print(ge_data2)\n",
    "    \n",
    "#print('mean of stp after filtering:')\n",
    "#print(ge_data2)\n",
    "\n",
    "\n",
    "print(len(ge_data.index))\n",
    "#ge_data.loc[:,stp_columns ] =ge_data.loc[:,stp_columns ].abs()  \n",
    "\n",
    "\n",
    "#\n",
    "## normalize stp parameters - for TM parameters\n",
    "do_normalize_stp_pars = 0;\n",
    "if do_normalize_stp_pars!=0:\n",
    "    ge_data.loc[:,stp_columns  ] =np.log(ge_data.loc[:,stp_columns  ].values.astype(float))\n",
    "\n",
    "#  assign training and labels \n",
    "stp_columns1 = stp_columns \n",
    "stp_columns = stp_columns2 # select training labels names\n",
    "\n",
    "nannot = 7\n",
    "nstp =len(stp_columns)\n",
    "annot_columns = ge_columns[0:6]+['index_ds']\n",
    "\n",
    "## averaging of stp for each synapse type \n",
    "\n",
    "\n",
    "#X = ge_data.loc[:,ge_data.columns[0:]].values\n",
    "X = ge_data.loc[:,annot_columns + ge_columns[6:]].values\n",
    "y = ge_data.loc[:,annot_columns + stp_columns].values \n",
    "#y = stp_data2.loc[:,:].values\n",
    "\n",
    "## normalize stp parameters - for TM parameters\n",
    "#y[:,nannot: ] =np.log(y[:,nannot: ].astype(float))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# print elapsed time\n",
    "t2 = time.time()\n",
    "#print(regr.feature_importances_)\n",
    "print('Elapsed time: '+str(t2-t1))\n",
    "\n",
    "print(y.shape)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge_data_h = ge_data\n",
    "X_h = X\n",
    "y_h = y\n",
    "annot_columns_h = annot_columns\n",
    "ge_columns_h = ge_columns\n",
    "stp_columns_h = stp_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge_data_h.to_excel('ge_data_h.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "##\n",
    "##  CORTEX gene expression\n",
    "##\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'data/'\n",
    "\n",
    "#d4='~/Documents/references/transcriptomes to STP/transcriptomes/ABA/mouse_VISp_gene_expression_matrices_2018-06-14/'\n",
    "if do_laptop:\n",
    "    d4 = '~/Documents/references/lists_of_proteins_2/Analysis_of_proteins/genetic_types/transcriptomic_interneurons/'\n",
    "else:\n",
    "    d4='~/Documents/references/transcriptomes to STP/transcriptomes/ABA/mouse_VISp_gene_expression_matrices_2018-06-14/'\n",
    "\n",
    "fn='mouse_VISp_2018-06-14_samples-columns.csv'\n",
    "df_aba_vis_c=pd.read_csv(d4+fn)\n",
    "print(df_aba_vis_c.columns)\n",
    "print(df_aba_vis_c.head())\n",
    "\n",
    "fn = 'mouse_VISp_2018-06-14_genes-rows.csv'\n",
    "df_aba_g=pd.read_csv(d4+fn)\n",
    "print(df_aba_g.head())\n",
    "print(len(df_aba_vis_c.index))\n",
    "\n",
    "in_glut_l23 = (df_aba_vis_c.loc[:,'class']=='Glutamatergic') #&(df_aba_vis_c.loc[:,'brain_subregion'].isin(['L2/3','L4','L5']))\n",
    "\n",
    "in_gaba = df_aba_vis_c.loc[:,'class']=='GABAergic'\n",
    "print(sum(in_gaba))\n",
    "df_aba_vis_in_c=df_aba_vis_c.loc[in_gaba,:]\n",
    "df_aba_vis_l23glu_c=df_aba_vis_c.loc[in_glut_l23,:]\n",
    "\n",
    "###\n",
    "print('loading gene expression...')\n",
    "\n",
    "from scvi.dataset import LoomDataset, CsvDataset, Dataset10X, AnnDataset\n",
    "\n",
    "#df_aba_ad.write('mouse_VISp_2018-06-14_exon-matrix_GABAergic.h5ad')\n",
    "aba_vis_in = AnnDataset(\"mouse_VISp_2018-06-14_exon-matrix_GABAergic.h5ad\", new_n_genes = 45768,\n",
    "                               save_path = save_path) \n",
    "\n",
    "#aba_vis_l23glu = AnnDataset(\"mouse_VISp_2018-06-14_exon-matrix_L23_Glutamatergic_hda.h5ad\", new_n_genes = 45768,\n",
    "#                               save_path = save_path) \n",
    "#aba_vis_l23glu = AnnDataset(\"mouse_VISp_2018-06-14_exon-matrix_L2345_Glutamatergic_hda.h5ad\", new_n_genes = 45768,\n",
    "#                               save_path = save_path) \n",
    "\n",
    "aba_vis_l23glu = AnnDataset(\"mouse_VISp_2018-06-14_exon-matrix_L23456_Glutamatergic_hda.h5ad\", new_n_genes = 45768,\n",
    "                               save_path = '') \n",
    "\n",
    "diff_order = sum(aba_vis_in.gene_names[0:].astype(int) - np.arange(len(aba_vis_in.gene_names)))\n",
    "print(diff_order)\n",
    "\n",
    "aba_vis_in.gene_names = df_aba_g.loc[:,'gene_symbol'].values\n",
    "\n",
    "aba_vis_l23glu.gene_names = df_aba_g.loc[:,'gene_symbol'].values\n",
    "\n",
    "\n",
    "\n",
    "print(len(aba_vis_in.gene_names))\n",
    "print(len(aba_vis_l23glu.gene_names))\n",
    "len(list(set(aba_vis_l23glu.gene_names).difference(set(aba_vis_in.gene_names))))\n",
    "print(len(df_aba_vis_l23glu_c.index))\n",
    "print(len(df_aba_vis_l23glu_c.index))\n",
    "print(len(df_aba_vis_in_c.index))\n",
    "\n",
    "from scvi.dataset.dataset import GeneExpressionDataset\n",
    "\n",
    "vis_dat = GeneExpressionDataset.concat_datasets(aba_vis_in,aba_vis_l23glu)    \n",
    "    \n",
    "\n",
    "print((vis_dat.batch_indices==0).sum())\n",
    "(vis_dat.batch_indices==1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "##\n",
    "##  CORTEX STP\n",
    "##\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if do_laptop:\n",
    "    d5 = '~/Documents/references/lists_of_proteins_2/Analysis_of_proteins/genetic_types/transcriptomic_interneurons/additional cortical STP data/'\n",
    "else:\n",
    "    d5 = '/home/stepaniu/Documents/references/transcriptomes to STP/scVI_data/additional cortical STP data/'\n",
    "#fn5='STP_cortex_interneurons_Cre_labeled.xlsx'\n",
    "#fn5='STP_cortex_interneurons_Cre_labeled_UDF2.xlsx'\n",
    "fn5='STP_cortex_interneurons_Cre_labeled_UDF3.xlsx'\n",
    "\n",
    "df_stp=pd.read_excel(d5+fn5)\n",
    "df_stp.loc[df_stp.loc[:,'stp_comment'].isna() ,'stp_comment']=''\n",
    "\n",
    "############   \n",
    "do_averagig_stp =0\n",
    "stp_TM_names=['tF', 'p0','tD','dp/p0','A']\n",
    "stp_TM_start = 29\n",
    "ncol = len(df_stp.columns)\n",
    "df_tm_name = pd.DataFrame(np.zeros(ncol).reshape((1,ncol)), columns =df_stp.columns )\n",
    "df_stp = pd.concat([df_tm_name, df_stp],axis=0).reset_index(drop=True)\n",
    "\n",
    "for i,nm in enumerate(stp_TM_names):\n",
    "    ii = stp_TM_start + np.arange(0,ncol-i-stp_TM_start,len(stp_TM_names))+i\n",
    "    df_stp.iloc[0,ii] = nm\n",
    "\n",
    "#df_stp.loc[df_stp.loc[:,'name'].str.contains('Jaing_2015_PC')!=True,:].reset_index(drop=True)\n",
    "\n",
    "################# ADD ABA STP data\n",
    "do_add_aba_synphys=True\n",
    "\n",
    "#QTX.to_hdf('fit_aba_2019_A1_8_fited_pulses_syntypes_results','data')\n",
    "d_aba = '/home/stepaniu/Documents/references/transcriptomes to STP/'\n",
    "#STP_aba = pd.read_hdf(d_aba + 'fit_aba_2019_A1_8_fited_pulses_TM_8_syntypes_results') # tm5 + FDR\n",
    "\n",
    "#STP_aba = pd.read_hdf(d_aba + 'fit_aba_2019_A1_8_fited_pulses_syntypes_results') # tm5\n",
    "#QTX2 = STP_aba.iloc[0:43,0:14]\n",
    "#QTX3 = STP_aba.iloc[43:,14:]\n",
    "#QTX3.index = QTX2.index\n",
    "#STP_aba = pd.concat([QTX2, QTX3],axis=1)\n",
    "\n",
    "STP_aba = pd.read_hdf(d_aba + 'fit_aba_2019_mean_fited_pulses_TM_SMP0_100bs_syntypes_results') # tm5 + SMR\n",
    "#STP_aba = pd.read_hdf(d_aba + 'fit_aba_2019_mean_fited_pulses_TM_SMP0_50bs_syntypes_results') \n",
    "\n",
    "STP_aba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# transform labels from TM to An:A1\n",
    "fs = [20, 50, 10] # Hz\n",
    "N = 8\n",
    "\n",
    "\n",
    "\n",
    "#Trec = [125, 250, 500, 1000, 2000]\n",
    "Trec = [ 250, 500, 1000]\n",
    "DT0 = 25000\n",
    "#xs  =ge_data.iloc[l_pre_post2:,:].loc[:,stp_aba_names].values\n",
    "\n",
    "idx = 'vip L2/3; sst L2/3'\n",
    "xs  = STP_aba.loc[idx,'differential_evolution best x']\n",
    "\n",
    "xs  =np.delete(xs, [5,6],axis=1)\n",
    "As=np.zeros((l_pre_post2,0))\n",
    "As2=np.zeros((xs.shape[0],0))\n",
    "if do_add_aba_synphys:\n",
    "    \n",
    "    for f in fs:\n",
    "        T = np.arange(N)*1000/f\n",
    "        \n",
    "        Asf, ns, ps, dpp0, p0, tF, tD, A = STP_sim(ge_data.iloc[0:l_pre_post2,:],T)\n",
    "        As = np.concatenate([As,Asf],axis=1)\n",
    "        \n",
    "        for ri in range(len(Trec)):\n",
    "            Asr, nsr, psr, dpp0, p0, tF, tD, A = STP_sim(ge_data.iloc[0:l_pre_post2,:],[Trec[ri]],init_state=[ns[:,-1],ps[:,-1]])\n",
    "            As = np.concatenate([As,Asr],axis=1)\n",
    "        \n",
    "\n",
    "        As2f = np.zeros((xs.shape[0],N+len(Trec)))\n",
    "        for i2 in range(xs.shape[0]):\n",
    "            #as2, sts2 = STP_sim2(xs[i2,:],np.arange(8)*50,model_type = 'tm5') # preconditioning series???\n",
    "            #as2, sts2 = STP_sim2(xs[i2,:], T+DT0, init_state=sts2[-1], model_type='tm5' ) \n",
    "            \n",
    "            as2, sts2 = STP_sim2(xs[i2,:],T,model_type = 'tm5_smr')\n",
    "            \n",
    "            As2f[i2,0:N] = as2\n",
    "            for ri in range(len(Trec)):\n",
    "                as2r, sts2r = STP_sim2(xs[i2,:], [Trec[ri]], init_state=sts2[-1], model_type='tm5_smr' ) \n",
    "                As2f[i2,N+ri] = as2r\n",
    "                \n",
    "        As2 = np.concatenate([As2,As2f],axis=1)\n",
    "            \n",
    "    \n",
    "    As2 = As2/As2[:,0].reshape((-1,1))\n",
    "    As = As/As[:,0].reshape((-1,1))\n",
    "    As = np.concatenate([As,As2],axis=0)\n",
    "else:\n",
    "    As, ns, ps, dpp0, p0, tF, tD, A = STP_sim(ge_data,T)\n",
    "    \n",
    "\n",
    "import matplotlib.pyplot as plt    \n",
    "plt.plot(As2[:,:].transpose(),'o-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORTEX : allign gene expression and stp data (for selected gene sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# allign gene expression and stp data (for selected gene sets)\n",
    "import re\n",
    "import time\n",
    "t1 = time.time()\n",
    "\n",
    "\n",
    "\n",
    "# names of gene expression based synapses types\n",
    "\n",
    "glu_l23=list(set(df_aba_vis_l23glu_c.loc[:,'cluster']))\n",
    "\n",
    "dsg = [['PC']+glu_l23,\n",
    "       ['Pvalb','Pvalb Tpbg','Pvalb Reln Itm2a','Pvalb Reln Tac1','Pvalb Sema3e Kank4'], \n",
    "       ['Sst','Sst Calb2 Pdlim5','Sst Calb2 Necab1', 'Sst Hpse Cbln4','Sst Hpse Sema3c','Sst Nr2f2 Necab1'],\n",
    "       ['Vip','Vip Lmo1 Myl1', 'Vip Rspo1 Itga4','Vip Ptprt Pkp2','Vip Rspo4 Rxfp1 Chat'],\n",
    "       ['Lamp5','Lamp5 Plch2 Dock5'],\n",
    "       ['MC','L23MC','Sst Calb2 Pdlim5'],\n",
    "        'L5MC']\n",
    "      \n",
    "\n",
    "set(df_stp.loc[:,'synapse_type_2'])\n",
    "\n",
    "# names of electrophysiology based synapses types\n",
    "dse = [['PC', 'L23P', 'L23PC', 'L5P'],\n",
    "       ['Pvalb', 'PV', 'L23BC', 'L5BC'], \n",
    "       ['Sst', 'L23MC', 'L5MC', 'Sst', 'MC'], \n",
    "       ['Vip', 'VIP', 'BTC'],\n",
    "       ['NGC', 'L23NGC'],\n",
    "       ['L23MC' ],\n",
    "       ['L5MC']]\n",
    "\n",
    "\n",
    "\n",
    "# load stp data table\n",
    "if do_laptop:\n",
    "    d5 = '~/Documents/references/lists_of_proteins_2/Analysis_of_proteins/genetic_types/transcriptomic_interneurons/additional cortical STP data/'\n",
    "else:\n",
    "    d5 = '/home/stepaniu/Documents/references/transcriptomes to STP/scVI_data/additional cortical STP data/'\n",
    "#fn5='STP_cortex_interneurons_Cre_labeled.xlsx'\n",
    "#fn5='STP_cortex_interneurons_Cre_labeled_UDF2.xlsx'\n",
    "#fn5='STP_cortex_interneurons_Cre_labeled_UDF3.xlsx'\n",
    "\n",
    "do_3pars_TM=0 # 3 or 4 parametric TM model?\n",
    "if do_3pars_TM:\n",
    "    fn5='STP_cortex_interneurons_Cre_labeled_UDF5_3pars.xlsx'\n",
    "else:\n",
    "    fn5='STP_cortex_interneurons_Cre_labeled_UDF5_4pars.xlsx'\n",
    "\n",
    "df_stp=pd.read_excel(d5+fn5)\n",
    "df_stp.loc[df_stp.loc[:,'stp_comment'].isna() ,'stp_comment']=''\n",
    "\n",
    "# drop TC->\n",
    "TC_names = ['Miao_2016_fig1_TC->PC','Miao_2016_fig1_TC->PV']\n",
    "df_stp = df_stp.drop(index = df_stp.index[df_stp.loc[:,'name'].isin(TC_names)] )\n",
    "\n",
    "############   \n",
    "do_averagig_stp =0\n",
    "stp_TM_names=['tF', 'p0','tD','dp/p0','A']\n",
    "stp_TM_start = 29\n",
    "ncol = len(df_stp.columns)\n",
    "df_tm_name = pd.DataFrame(np.zeros(ncol).reshape((1,ncol)), columns =df_stp.columns )\n",
    "df_stp = pd.concat([df_tm_name, df_stp],axis=0).reset_index(drop=True)\n",
    "\n",
    "for i,nm in enumerate(stp_TM_names):\n",
    "    ii = stp_TM_start + np.arange(0,ncol-i-stp_TM_start,len(stp_TM_names))+i\n",
    "    df_stp.iloc[0,ii] = nm\n",
    "\n",
    "\n",
    "\n",
    "print(df_stp.head())\n",
    "#############\n",
    "##Remove pure PPR data (Jaing_2015_PC)\n",
    "#df_stp = df_stp.loc[df_stp.loc[:,'name'].str.contains('Jaing_2015_PC')!=True,:].reset_index(drop=True) \n",
    "\n",
    "############\n",
    "\n",
    "# batches and gene sets\n",
    "pre_post = df_stp.loc[:,'synapse_type_2'].str.split(pat='->',expand=True)\n",
    "\n",
    "if do_add_aba_synphys:\n",
    "    pre_post_aba = STP_aba.loc[:,'synapse_type_2'].str.split(pat=';',expand=True)\n",
    "\n",
    "vbi = vis_dat.batch_indices.reshape((vis_dat.batch_indices.shape[0],))\n",
    "vbi=vbi>0\n",
    "\n",
    "df_gs  = pd.DataFrame( vis_dat.gene_names ) \n",
    "in_pregs = df_gs.loc[:,0].isin(pregs).values\n",
    "in_postgs = df_gs.loc[:,0].isin(postgs).values\n",
    "\n",
    "pregs2 = df_gs.loc[in_pregs,0].values\n",
    "postgs2 = df_gs.loc[in_postgs,0].values\n",
    "\n",
    "\n",
    "do_2=1;\n",
    "# select stp data for appropriate frequency and ages \n",
    "stp_type = {'name':['Walker_2016_PV->MC L23',\n",
    "                    'Walker_2016_VIP->MC L23',\n",
    "                    'Yuste_2016_taus_PV->Sst L23 recalc',\n",
    "                    'Yuste_2016_taus_PV->VIP L23 recalc',\n",
    "                    'Yuste_2016_taus_Sst->VIP L23 recalc',\n",
    "                    'Yuste_2016_taus_VIP->Sst L23 recalc',\n",
    "                    'Yuste_2016_taus_VIP->VIP L23 recalc'],\n",
    "            'name_remove': [], \n",
    "            'stp_freq': [20, 40, 50], 'stp_comment':  ['', ' P20-21', ' P20-22', ' P20-23', ' P25-31'],\n",
    "            'stp_data_type': df_stp.loc[0, :]!=0 }\n",
    "\n",
    "if do_add_aba_synphys:\n",
    "    STP_aba_type = {'name':[],\n",
    "            'stp_freq': [], 'stp_comment':  [],\n",
    "            'stp_data_type': 'differential_evolution best x' }  # 'random_starts best x'\n",
    "    #stp_aba_names = ['tF', 'p0','tD','dp','A','A1','A2'] # tm5\n",
    "    stp_aba_names = ['tF', 'p0','tD','dp','A','A1','A2','tDmin','dd','t_FDR','t_SMR','dp0'] # tm5+smr\n",
    "    \n",
    "if do_2==1:\n",
    "    #stp_type = {'name':['Walker_2016_PV->MC L23',\n",
    "    #               'Walker_2016_VIP->MC L23',\n",
    "    #                'Yuste_2016_taus_PV->Sst L23 recalc',\n",
    "    #                'Yuste_2016_taus_VIP->Sst L23 recalc'],\n",
    "    #        'stp_freq': [20, 40, 50], 'stp_comment':  ['', ' P20-21', ' P20-22', ' P20-23', ' P25-31'],\n",
    "    #        'stp_data_type': df_stp.loc[0, :]!=0 }\n",
    "    \n",
    "    # without Yuste and\n",
    "    stp_type = {'name':['Walker_2016_PV->MC L23',\n",
    "                    'Walker_2016_VIP->MC L23'],\n",
    "             'name_remove': ['Jaing','Yuste'],  \n",
    "            'stp_freq': [20, 40, 50], 'stp_comment':  ['', ' P20-21', ' P20-22', ' P20-23', ' P25-31'],\n",
    "            'stp_data_type': df_stp.loc[0, :]!=0 }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "in_stp_data = df_stp.loc[:,'stp_freq'].isin([20])\n",
    "in_stp_data= in_stp_data|(df_stp.loc[:,'stp_freq'].isin([40,50])&df_stp.loc[:,'name'].isin(stp_type['name']))\n",
    "in_stp_data = in_stp_data&df_stp.loc[:,'stp_comment'].isin(pd.Series(stp_type['stp_comment']).str.strip())\n",
    "if len(stp_type['name_remove'])>0:\n",
    "        remove = df_stp.loc[:,'name'].str.contains(stp_type['name_remove'][0])\n",
    "        for rmn in stp_type['name_remove']:\n",
    "             remove= remove|df_stp.loc[:,'name'].str.contains(rmn)\n",
    "        in_stp_data = in_stp_data&( remove!=True)\n",
    "        \n",
    "#show selected stp dataset\n",
    "ddd=df_stp.loc[in_stp_data,['name','synapse_type_2', 'stp_freq','area']]\n",
    "print(ddd)\n",
    "print(in_stp_data.sum())\n",
    "\n",
    "synij = np.repeat(False,in_stp_data.shape[0])\n",
    "if do_add_aba_synphys:\n",
    "    synij = np.repeat(False,in_stp_data.shape[0]+STP_aba.shape[0])\n",
    "\n",
    "cols2 = ['name','perc']\n",
    "cl2post =pd.DataFrame([],columns= cols2)\n",
    "cl2pre =pd.DataFrame([],columns= cols2)\n",
    "\n",
    "\n",
    "# strip cluster names\n",
    "#list(set(df_aba_vis_in_c.loc[df_aba_vis_in_c.loc[:,'subclass'].str.contains(clns6),'cluster']))\n",
    "df_aba_vis_in_c.loc[:,'cluster']=df_aba_vis_in_c.loc[:,'cluster'].str.strip()\n",
    "df_aba_vis_l23glu_c.loc[:,'cluster']=df_aba_vis_l23glu_c.loc[:,'cluster'].str.strip()\n",
    "\n",
    "do_simplfy_clusters = 1 # do not take into account minor subclasses\n",
    "Sst_NonMC ='Sst, Sst Calb2 Pdlim5-, Sst Chrna2 Glra3-, Sst Chrna2 Ptgdr-, Sst Myh8 Etv1-, Sst Tac2 Myh4-, Sst Nr2f2 Necab1-, Sst Chodl-,  Sst Calb2 Necab1-, Sst Tac1 Htr1d-, Sst Myh8 Fibin-';\n",
    "\n",
    "pregs3 = ['pre__'+s for s in pregs2.tolist()]\n",
    "postgs3 = ['post_'+s for s in postgs2.tolist()]\n",
    "ge_columns = ['cell_type2_pre', 'cell_type2_post',\n",
    "                                             'layer_pre','layer_post',\n",
    "                                             'cre_line_pre', 'cre_line_post'] +pregs3 +postgs3\n",
    "\n",
    "stp_columns = stp_TM_names #list(set(df_stp.loc[0,:]).difference(set([0])))\n",
    "\n",
    "# output datasets\n",
    "stp_data = pd.DataFrame([], columns =['presynaptic', 'postsynaptic', 'stp_data'] )\n",
    "ge_data = pd.DataFrame([], columns =ge_columns+stp_columns+['index_ds'])\n",
    "\n",
    "if do_add_aba_synphys:\n",
    "    l_pre_post=len(pre_post.index)+len(pre_post_aba.index)\n",
    "else:\n",
    "    l_pre_post=len(pre_post.index)\n",
    "l_pre_post1 = len(pre_post.index)\n",
    "\n",
    "for i in range(1,l_pre_post): #range(1,len(pre_post.index)):# range(len(pre_post.index)): # [1]: #range(len(pre_post.index)):\n",
    "    if i==l_pre_post1:\n",
    "        l_pre_post2 = ge_data.shape[0]\n",
    "        \n",
    "    # syne3 - all available pre- and post- synaptic cells annotations : cre-lines, layers, morphologies\n",
    "    if i<l_pre_post1:\n",
    "        l = str.strip(df_stp.loc[i,'area'])\n",
    "        if type(df_stp.loc[i,'synapse_type'])!=float:\n",
    "            syne1 = re.split('\\;',df_stp.loc[i,'synapse_type'])\n",
    "        else:\n",
    "            syne1 =''    \n",
    "        #syne2 = df_stp.loc[i,'synapse_type2'].str.split(pat='->',expand=True)\n",
    "        syne2 = re.split('->',df_stp.loc[i,'synapse_type_2'])\n",
    "        if len(syne2)==1:\n",
    "            pat ='→'\n",
    "            syne2 = re.split(pat,df_stp.loc[i,'synapse_type_2'])\n",
    "\n",
    "        syne3 ={'layer_pre':l,  'cre_line_pre':'none', 'cell_type2_pre':str.strip(syne2[0]),'cluster_pre':[],\n",
    "                'layer_post':l, 'cre_line_post':'none','cell_type2_post':str.strip(syne2[1]),'cluster_post':[],  } \n",
    "\n",
    "        # add cre-lines when available\n",
    "        if len(syne1)>1:\n",
    "            syne3['cre_line_pre']  =str.strip(syne1[0])\n",
    "            syne3['cre_line_post'] =str.strip(syne1[1])\n",
    "\n",
    "        # introduce study-specific modifications to syne3 (using morphology+layer->clusters map)\n",
    "        if  df_stp.loc[i,'name'][0:5]=='Jaing':\n",
    "\n",
    "            # modify layer_post and reduce cell_type_post to mtype name\n",
    "            if syne3['cell_type2_post'][0:3]=='L23':\n",
    "                syne3['layer_post'] = 'L2/3'\n",
    "                syne3['cell_type2_post']=syne3['cell_type2_post'][3:]\n",
    "            elif syne3['cell_type2_post'][0:2]=='L5':\n",
    "                     syne3['layer_post'] = 'L5'\n",
    "                     syne3['cell_type2_post']=syne3['cell_type2_post'][2:]  \n",
    "\n",
    "            #  reduce cell_type_pre to mtype name\n",
    "            if syne3['cell_type2_pre'][0:3]=='L23':\n",
    "                syne3['layer_pre'] = 'L2/3'\n",
    "                syne3['cell_type2_pre']=syne3['cell_type2_pre'][3:]\n",
    "            elif syne3['cell_type2_pre'][0:2]=='L5':\n",
    "                     syne3['layer_pre'] = 'L5'\n",
    "                     syne3['cell_type2_pre']=syne3['cell_type2_pre'][2:]    \n",
    "\n",
    "            # for each mtype specify expected ABI gene custers and their expected % (based on Jiang 2015 cre-lines to morhology table)\n",
    "            if syne3['cell_type2_post']=='MC':\n",
    "                if syne3['layer_post'] == 'L2/3':\n",
    "                    syne3['cluster_post']=[['Sst Calb2 Pdlim5',100]]\n",
    "                if syne3['layer_post'] == 'L5':\n",
    "                    syne3['cre_line_post']='Chrna2'  \n",
    "                    syne3['cluster_post']=[['Sst',100]]\n",
    "\n",
    "            if syne3['cell_type2_post']=='BC':  \n",
    "                if syne3['layer_post'] == 'L2/3':\n",
    "                    if do_simplfy_clusters==0:\n",
    "                        syne3['cluster_post']=[['Pvalb, Pvalb Vipr2-',75],[Sst_NonMC,25]] # modify!!!    \n",
    "                    else:\n",
    "                        syne3['cluster_post']=[['Pvalb, Pvalb Vipr2-',100]] # modify!!!  \n",
    "\n",
    "                if syne3['layer_post'] == 'L5':\n",
    "                    if do_simplfy_clusters==0:\n",
    "                        syne3['cluster_post']=[['Pvalb, Pvalb Vipr2-',75],[Sst_NonMC,25]] # modify!!!    \n",
    "                    else:\n",
    "                        syne3['cluster_post']=[['Pvalb, Pvalb Vipr2-',100]] # modify!!!    \n",
    "            if syne3['cell_type2_post']=='BTC':\n",
    "                if do_simplfy_clusters==0:\n",
    "                    syne3['cluster_post']=[['Vip Lmo1 Myl1, Vip Ptprt Pkp2, Vip Rspo4 Rxfp1 Chat',90], # modify!!! \n",
    "                                         [Sst_NonMC,10]]\n",
    "                else:\n",
    "                    syne3['cluster_post']=[['Vip Lmo1 Myl1, Vip Ptprt Pkp2, Vip Rspo4 Rxfp1 Chat',100]]\n",
    "            if syne3['cell_type2_post']=='HEC':  # modify!!! \n",
    "                if do_simplfy_clusters==0:\n",
    "                    syne3['cluster_post']=[['Pvalb, Pvalb Vipr2-,Pvalb Tpbg-,Pvalb Reln Itm2a-,Pvalb Reln Tac1-',75],\n",
    "                                           [Sst_NonMC,25]]\n",
    "                else:\n",
    "                    syne3['cluster_post']=[['Pvalb, Pvalb Vipr2-,Pvalb Tpbg-,Pvalb Reln Itm2a-,Pvalb Reln Tac1-',100]]            \n",
    "            if syne3['cell_type2_post']=='NGC':            \n",
    "                  syne3['cluster_post']=[['Lamp5 Ntn1 Npy2r, Lamp5 Plch2 Dock5',100]] \n",
    "\n",
    "        if  df_stp.loc[i,'name'][0:5]=='Walke':    \n",
    "            if syne3['cell_type2_post']=='MC':\n",
    "                 syne3['cluster_post']=[['Sst Calb2 Pdlim5',100]]            \n",
    "\n",
    "        #if  df_stp.loc[i,'name'][0:5]=='Miao_':     \n",
    "\n",
    "        #if  df_stp.loc[i,'name'][0:5]=='Yuste':     \n",
    "    \n",
    "    # ABA synphys dataset\n",
    "    if i>=l_pre_post1:\n",
    "        #l = str.strip(STP_aba.loc[i1,'area'])\n",
    "\n",
    "            \n",
    "        i1 = STP_aba.index[i - l_pre_post1]\n",
    "          \n",
    "        syne0 = re.split('\\,',STP_aba.loc[i1,'synapse_type_2'])\n",
    "        syne1 = re.split('\\;',syne0[0])\n",
    "        \n",
    "        syne0 = re.split('\\,',STP_aba.loc[i1,'comments'])\n",
    "        syne1 = re.split('\\;',syne0[0])\n",
    "        syne12 = re.split('\\'',syne1[0])[1]\n",
    "        syne13 = re.split('\\'',syne1[1])[0]\n",
    "\n",
    "        syne_pre = re.split('\\s+',str.strip(syne12))\n",
    "        syne_post = re.split('\\s+',str.strip(syne13))\n",
    "        \n",
    "   \n",
    "        l1 = str.strip(syne_pre[1])\n",
    "        l2 = str.strip(syne_post[1])\n",
    "        \n",
    "        syne3 ={'layer_pre':[l1],  'cre_line_pre':'none', 'cell_type2_pre':'PC','cluster_pre':[],\n",
    "                'layer_post':[l2], 'cre_line_post':'none','cell_type2_post':'PC','cluster_post':[]  } \n",
    "\n",
    "        # add cre-lines when available\n",
    "        in_lines = pd.DataFrame(['vip','pvalb','sst'])\n",
    "        if str.strip(syne_pre[0])!='unknown':\n",
    "            syne3['cre_line_pre']   =str.strip(syne_pre[0])\n",
    "            if in_lines.isin([syne3['cre_line_pre']]).any()[0]:\n",
    "                syne3['cell_type2_pre'] = syne3['cre_line_pre'] \n",
    "        else:\n",
    "            syne3['cell_type2_pre']   = 'PC'\n",
    "            \n",
    "        if str.strip(syne_post[0])!='unknown':\n",
    "            syne3['cre_line_post']  =str.strip(syne_post[0])  \n",
    "            if in_lines.isin([syne3['cre_line_post']]).any()[0]:\n",
    "                syne3['cell_type2_post'] = syne3['cre_line_post'] \n",
    "        else:\n",
    "            syne3['cell_type2_post']   = 'PC'    \n",
    "            \n",
    "            \n",
    "        if syne3['cre_line_pre']=='tlx3':\n",
    "            syne3['layer_pre']=['L4-L6']\n",
    "                    \n",
    "        if syne3['cre_line_post']=='tlx3':\n",
    "            syne3['layer_post']=['L4-L6']\n",
    "        \n",
    "                    \n",
    "        if syne3['cre_line_pre']=='sim1':\n",
    "            syne3['layer_pre']=['L4-L5','L4-L6']\n",
    "        \n",
    "                    \n",
    "        if syne3['cre_line_post']=='sim1':\n",
    "            syne3['layer_post']=['L4-L5','L4-L6']\n",
    "            \n",
    "        if syne3['cre_line_post']=='vip':\n",
    "            if syne3['layer_post'][0]=='L2/3':\n",
    "                syne3['layer_post']=['L1-L2/3']\n",
    "        if syne3['cre_line_pre']=='vip':\n",
    "            if syne3['layer_pre'][0]=='L2/3':\n",
    "                syne3['layer_pre']=['L1-L2/3']\n",
    "                \n",
    "        if syne3['cre_line_post']=='sst':\n",
    "            if syne3['layer_post'][0]=='L2/3':\n",
    "                syne3['layer_post']=['L1-L2/3']\n",
    "        if syne3['cre_line_pre']=='sst':\n",
    "            if syne3['layer_pre'][0]=='L2/3':\n",
    "                syne3['layer_pre']=['L1-L2/3']        \n",
    " \n",
    "    \n",
    "    # # find sets of gene clusters corresponding to each pre- and post- neuron types pair from stp-table\n",
    "    # for ii in range(len(dse)): \n",
    "    #    if sum(np.array(dse[ii])==pre_post.loc[i,0])>0:\n",
    "    #        i1=ii\n",
    "    #    if sum(np.array(dse[ii])==pre_post.loc[i,1])>0:\n",
    "    #        j1=ii    \n",
    "    #\n",
    "    ## select names of stp-types\n",
    "    #pree = dse[i1]\n",
    "    #poste = dse[j1]\n",
    "    \n",
    "    ## select names of genes-types (ABA cluster)\n",
    "    #preg = dsg[i1]\n",
    "    #postg = dsg[j1]\n",
    "\n",
    "    ##synij = (pre_post.loc[:,0].isin(pree))&(pre_post.loc[:,1].isin(poste))\n",
    "    \n",
    "\n",
    "    synij[:] = False\n",
    "    \n",
    "    if i<l_pre_post1:\n",
    "        synij[i] = in_stp_data[i]\n",
    "    else:\n",
    "        synij[i] = True\n",
    "    \n",
    "    #print(syne3)\n",
    "    \n",
    "    if sum(synij)>0:\n",
    "        print('\\n\\n')\n",
    "        print('i = ',str(i))\n",
    "        print(syne3)\n",
    "        #if sum(in_stp_data&synij)>0:\n",
    "        #    in_stp_data = in_stp_data&synij\n",
    "\n",
    "        #add ge data\n",
    "        #'layer_pre':l,  'cre_line_pre':'', 'cell_type2_pre':syne2[0].str.strip(),'cluster_pre':[]\n",
    "        \n",
    "        # PC or GABAergic dataset?\n",
    "        batch = [ syne3['cell_type2_pre']=='PC',syne3['cell_type2_post']=='PC']\n",
    "        \n",
    "        # add ge data depending on available stp cell types labels:\n",
    "        #  select presynaptic cells\n",
    "        if batch[0]:\n",
    "\n",
    "            if i>=l_pre_post1:\n",
    "                in_preg  = df_aba_vis_l23glu_c.loc[:,'brain_subregion'].isin(syne3['layer_pre']) #modify!!!\n",
    "                #in_preg  = in_preg&(df_aba_vis_l23glu_c.loc[:,'driver_lines']==syne3['cre_line_pre']) #modify!!!\n",
    "                if syne3['cre_line_pre']!='none':\n",
    "                    in_preg  = in_preg&(df_aba_vis_l23glu_c.loc[:,'driver_lines'].str.lower().str.contains(syne3['cre_line_pre'])) #modify!!!\n",
    "                #set(df_aba_vis_l23glu_c.loc[:,'driver_lines'].str.lower().str.contains('nr5a1'))\n",
    "            else:\n",
    "                #in_preg  = df_aba_vis_l23glu_c.loc[:,'brain_subregion'].isin([syne3['layer_pre']]) #modify!!!\n",
    "                in_preg  = df_aba_vis_l23glu_c.loc[:,'brain_subregion'].isin([syne3['layer_pre']]) #modify!!!\n",
    "        else:\n",
    "            if len(syne3['cluster_pre'])==0:\n",
    "                # if len(syne3['cre_line_pre'])>0:\n",
    "                in_preg = df_aba_vis_in_c.loc[:,'driver_lines'].str.lower().str.contains(str.lower(syne3['cre_line_pre']))\n",
    "                #in_preg = in_preg&df_aba_vis_in_c.loc[:,'brain_subregion'].isin([syne3['layer_pre']])\n",
    "                if i>=l_pre_post1:\n",
    "                    in_preg = in_preg&df_aba_vis_in_c.loc[:,'brain_subregion'].isin(syne3['layer_pre'])\n",
    "                else:\n",
    "                    in_preg = in_preg&(df_aba_vis_in_c.loc[:,'brain_subregion'].str.contains(syne3['layer_pre']))\n",
    "                    \n",
    "                #if i==72:\n",
    "                #    breakpoint()\n",
    "                    \n",
    "                    \n",
    "            else:                  \n",
    "                #in_preg = df_aba_vis_in_c.loc[:,'brain_subregion'].isin([syne3['layer_pre']])\n",
    "                in_preg = df_aba_vis_in_c.loc[:,'brain_subregion'].str.contains(syne3['layer_pre'])\n",
    "                cl = syne3['cluster_pre']\n",
    "                #cols2 = ['name','perc']\n",
    "                #cl2pre =pd.DataFrame([],columns= cols2)\n",
    "                in_preg0 = in_preg.copy()\n",
    "                in_preg[:] = False \n",
    "                for icl in range(len(cl)):\n",
    "                    clp = cl[icl][1]\n",
    "                    clns = re.split(',',cl[icl][0])\n",
    "                    cls3 = []\n",
    "                    cls3m=[]\n",
    "                    for icls in range(len(clns)):\n",
    "                        clns4 = clns[icls]\n",
    "                        clns5 = re.split('\\s+',clns4)\n",
    "                        if len(clns5)<3:\n",
    "                            clns6=clns5[0]\n",
    "                            cls3=cls3 + list(set(df_aba_vis_in_c.loc[df_aba_vis_in_c.loc[:,'subclass'].str.contains(clns6),'cluster']))\n",
    "                        else:\n",
    "                            clns4=str.strip(clns4)\n",
    "                            clns7=re.split('\\-',clns4)\n",
    "                            if len(clns7)>1:\n",
    "                                clns4=clns7[0]\n",
    "                                cls3m=cls3m+[str.strip(clns4)]\n",
    "                            else:\n",
    "                                cls3=cls3+[str.strip(clns4)]\n",
    "                    cls3 = list(set(cls3).difference(set(cls3m)))\n",
    "                    cl22=pd.DataFrame([cls3,[clp]],index=cols2).T\n",
    "                    cl2pre = cl2pre.append(cl22)\n",
    "                    \n",
    "                    in_preg = in_preg|(in_preg0&df_aba_vis_in_c.loc[:,'cluster'].isin(cls3))\n",
    "                    \n",
    "                if len(syne3['cre_line_pre'])>0:\n",
    "                    #in_preg = in_preg&df_aba_vis_in_c.loc[:,'driver_lines'].str.contains(syne3['cre_line_pre'])\n",
    "                    in_line=df_aba_vis_in_c.loc[:,'driver_lines'].str.contains(syne3['cre_line_pre'])\n",
    "                    if in_line.sum()>0:\n",
    "                        in_preg = in_preg&in_line                        \n",
    "        \n",
    "        #  select postsynaptic cells\n",
    "        if batch[1]:\n",
    "\n",
    "            if i>=l_pre_post1:\n",
    "                in_postg  = df_aba_vis_l23glu_c.loc[:,'brain_subregion'].isin(syne3['layer_post']) #modify!!!\n",
    "                if syne3['cre_line_post']!='none':\n",
    "                    in_postg  = in_postg&(df_aba_vis_l23glu_c.loc[:,'driver_lines'].str.lower().str.contains(syne3['cre_line_post'])) #modify!!!\n",
    "            else:\n",
    "                #in_postg = df_aba_vis_l23glu_c.loc[:,'brain_subregion'].isin([syne3['layer_post']]) #modify!!!\n",
    "                in_postg = df_aba_vis_l23glu_c.loc[:,'brain_subregion']==syne3['layer_post'] #modify!!!\n",
    "        else:    \n",
    "            if len(syne3['cluster_post'])==0:\n",
    "                # if len(syne3['cre_line_post'])>0:\n",
    "                in_postg = df_aba_vis_in_c.loc[:,'driver_lines'].str.lower().str.contains(str.lower(syne3['cre_line_post']))\n",
    "                #in_postg = in_postg&df_aba_vis_in_c.loc[:,'brain_subregion'].isin([syne3['layer_post']])\n",
    "                if i>=l_pre_post1:\n",
    "                    in_postg = in_postg&df_aba_vis_in_c.loc[:,'brain_subregion'].isin(syne3['layer_post'])\n",
    "                else:\n",
    "                    in_postg = in_postg&df_aba_vis_in_c.loc[:,'brain_subregion'].str.contains(syne3['layer_post'])\n",
    "            else:    \n",
    "                ## under construction ...\n",
    "                #in_postg = df_aba_vis_in_c.loc[:,'brain_subregion'].isin([syne3['layer_post']])\n",
    "                in_postg = df_aba_vis_in_c.loc[:,'brain_subregion'].str.contains(syne3['layer_post'])\n",
    "                cl = syne3['cluster_post']\n",
    "                #cols2 = ['name','perc']\n",
    "                #cl2post =pd.DataFrame([],columns= cols2)\n",
    "                in_postg0 = in_postg.copy()\n",
    "                in_postg[:] = False                 \n",
    "                for icl in range(len(cl)):\n",
    "                    clp = cl[icl][1]\n",
    "                    clns = re.split(',',cl[icl][0])\n",
    "                    cls3 = []\n",
    "                    cls3m=[]\n",
    "                    for icls in range(len(clns)):\n",
    "                        clns4 = clns[icls]\n",
    "                        clns5 = re.split('\\s+',clns4)\n",
    "                        if len(clns5)<3:\n",
    "                            clns6=clns5[0]\n",
    "                            cls3=cls3 + list(set(df_aba_vis_in_c.loc[df_aba_vis_in_c.loc[:,'subclass'].str.contains(clns6),'cluster']))\n",
    "                        else:\n",
    "                            clns4=str.strip(clns4)\n",
    "                            clns7=re.split('\\-',clns4)\n",
    "                            if len(clns7)>1:\n",
    "                                clns4=clns7[0]\n",
    "                                cls3m=cls3m+[str.strip(clns4)]\n",
    "                            else:\n",
    "                                cls3=cls3+[str.strip(clns4)]\n",
    "                    cls3 = list(set(cls3).difference(set(cls3m)))\n",
    "                    cl22=pd.DataFrame([cls3,[clp]],index=cols2).T\n",
    "                    cl2post = cl2post.append(cl22)\n",
    "                    \n",
    "                    #in_postg = in_postg&df_aba_vis_in_c.loc[:,'cluster'].isin(cls3)\n",
    "                    in_postg = in_postg|(in_postg0&df_aba_vis_in_c.loc[:,'cluster'].isin(cls3))\n",
    "                    \n",
    "                if len(syne3['cre_line_post'])>0:\n",
    "                    in_line=df_aba_vis_in_c.loc[:,'driver_lines'].str.contains(syne3['cre_line_post'])\n",
    "                    if in_line.sum()>0:\n",
    "                        in_postg = in_postg&in_line\n",
    "            \n",
    "            \n",
    "        ## make equal size pre- and post- datasets\n",
    "        #in_preg = in_preg&(in_preg.isna()==False)\n",
    "        #in_postg = in_postg&(in_postg.isna()==False)\n",
    "        #npre = in_preg.sum()\n",
    "        #npost = in_postg.sum()\n",
    "        ##nsample = max([npost, npre] )\n",
    "        #nsample = min([npost, npre] ) # number of cells\n",
    "        #print('npre = '+ str(npre)+' , npost = ',str(npost))\n",
    "        #if (nsample<150)&(npre>0)&(npost>0):\n",
    "        #    nsample=150\n",
    "        \n",
    "        #idx2 =  np.mod(np.arange(nsample),npre)\n",
    "        #in_preg = np.nonzero(in_preg)[0][idx2]\n",
    "        #idx2 =  np.mod(np.arange(nsample),npost)\n",
    "        #in_postg = np.nonzero(in_postg)[0][idx2]\n",
    "        \n",
    "        \n",
    "        # make equal size pre- and post- datasets\n",
    "        in_preg = in_preg&(in_preg.isna()==False)\n",
    "        in_postg = in_postg&(in_postg.isna()==False)\n",
    "        npre = in_preg.sum()\n",
    "        npost = in_postg.sum()\n",
    "        nsample = max([npost, npre] )\n",
    "        #nsample = min([npost, npre] ) # number of cells\n",
    "        \n",
    "\n",
    "        \n",
    "   \n",
    "            \n",
    "        print('npre = '+ str(npre)+' , npost = ' + str(npost) + ' , nsample = '+str(nsample))    \n",
    "        \n",
    "        \n",
    "        # select cell indices for gene expression sampling\n",
    "        do_bootstrap=1\n",
    "        if  do_bootstrap==0:\n",
    "            \n",
    "            if (nsample<60)&(npre>0)&(npost>0):\n",
    "                nsample=60\n",
    "            \n",
    "            if (nsample>180)&(npre>0)&(npost>0):  # restrict nsample to avoid overrepresentation?\n",
    "                nsample=180 \n",
    "            \n",
    "            idx2 =   np.mod(np.arange(nsample),npre) \n",
    "            in_preg = np.nonzero(in_preg)[0][idx2]\n",
    "            idx2 =   np.mod(np.arange(nsample),npost) \n",
    "            in_postg = np.nonzero(in_postg)[0][idx2]\n",
    "        else :\n",
    "            if (npre>0)&(npost>0):\n",
    "                nsample=200\n",
    "            #random.uniform(low=0.0, high=1.0, size=None)\n",
    "            idx2pre =  np.floor( np.random.uniform(0,npre, (nsample,1)) ).astype('int')\n",
    "            in_preg_bs = np.nonzero(in_preg)[0][idx2pre]\n",
    "            idx2post =  np.floor( np.random.uniform(0,npost, (nsample,1)) ).astype('int')\n",
    "            in_postg_bs = np.nonzero(in_postg)[0][idx2post]\n",
    "            in_preg_ = in_preg_bs[:,0]\n",
    "            in_postg_ = in_postg_bs[:,0]\n",
    "        \n",
    "        \n",
    "\n",
    "        if batch[0]:\n",
    "            #in_preg_cl = list(set(df_aba_vis_l23glu_c.iloc[in_preg,:].loc[:,'cluster']))\n",
    "            in_preg_cl = df_aba_vis_l23glu_c.iloc[in_preg_,:].loc[:,'cluster'].value_counts()\n",
    "        else:\n",
    "            #in_preg_cl = list(set(df_aba_vis_in_c.iloc[in_preg,:].loc[:,'cluster']))\n",
    "            in_preg_cl = df_aba_vis_in_c.iloc[in_preg_,:].loc[:,'cluster'].value_counts()\n",
    "        \n",
    "        if batch[1]:\n",
    "            #in_postg_cl = list(set(df_aba_vis_l23glu_c.iloc[in_postg,:].loc[:,'cluster']))\n",
    "            in_postg_cl = df_aba_vis_l23glu_c.iloc[in_postg_,:].loc[:,'cluster'].value_counts()\n",
    "        else:        \n",
    "            #in_postg_cl = list(set(df_aba_vis_in_c.iloc[in_postg,:].loc[:,'cluster']))\n",
    "            in_postg_cl = df_aba_vis_in_c.iloc[in_postg_,:].loc[:,'cluster'].value_counts()\n",
    "        in_preg_cl = in_preg_cl/in_preg_cl.sum()*100    \n",
    "        in_postg_cl = in_postg_cl/in_postg_cl.sum()*100    \n",
    "        print('presynaptic clusters')\n",
    "        print(in_preg_cl)\n",
    "        print('postsynaptic clusters')\n",
    "        print(in_postg_cl)\n",
    "        \n",
    "        \n",
    "\n",
    "        #if npost>npre:\n",
    "        #    idx = np.mod(np.arange(npost),npre)\n",
    "        #    #prege = prege[idx,:]\n",
    "        #    #postge = postge\n",
    "        #    prege = prege\n",
    "        #    postge = postge[0:npre,:]            \n",
    "        #elif npost<npre :\n",
    "        #    idx = np.mod(np.arange(npre),npost)\n",
    "        #    #prege = prege\n",
    "        #    #postge = postge[idx,:]\n",
    "        #    prege = prege[0:npost,:]\n",
    "        #    postge = postge         \n",
    "        \n",
    "        # take ge data  \n",
    "        \n",
    "        prege = vis_dat.X[:, in_pregs]  # presynaptic gene set expression\n",
    "        prege = prege[(vbi==batch[0]), :]\n",
    "        \n",
    "        postge = vis_dat.X[:,  in_postgs] # postsynaptic gene set expression\n",
    "        postge = postge[(vbi==batch[1]),  :]\n",
    "        \n",
    "        do_median=1\n",
    "        if  (do_bootstrap==0)|(do_median==0):\n",
    "            prege = prege[in_preg,:]\n",
    "            postge = postge[in_postg,:]\n",
    "        else: \n",
    "            nsample = 100\n",
    "            Nbs = nsample\n",
    "            ##random.uniform(low=0.0, high=1.0, size=None)\n",
    "            #nmax=5\n",
    "            nmax = npre #np.min([1000,npre])\n",
    "            idx2pre =  np.floor( np.random.uniform(0,npre, (nmax,Nbs)) ).astype('int')\n",
    "            in_preg_ = np.nonzero(in_preg)[0][np.arange(npre)]\n",
    "            nmax = npost #np.min([1000,npost])\n",
    "            idx2post =  np.floor( np.random.uniform(0,npost, (nmax,Nbs)) ).astype('int')\n",
    "            in_postg_ = np.nonzero(in_postg)[0][np.arange(npost)]\n",
    "\n",
    "                        # \n",
    "            # ??? transform ge to median bootstraped log ???\n",
    "            prege_ = prege[in_preg_,:]\n",
    "            postge_ = postge[in_postg_,:]\n",
    "\n",
    "            y = np.ma.masked_where(prege_ == 0, prege_)\n",
    "            prege_=np.ma.log2(y).filled(0)\n",
    "            y = np.ma.masked_where(postge_ == 0, postge_)\n",
    "            postge_=np.ma.log2(y).filled(0)\n",
    "\n",
    "\n",
    "\n",
    "            npregs=prege_.shape[1]\n",
    "            ge_bs = np.zeros((Nbs,npregs+postge_.shape[1]))\n",
    "            for ibs in range(Nbs):\n",
    "                #ans=np.apply_along_axis(lambda v: np.median(v[v!=0]), 0, x)\n",
    "                #ans[np.isnan(ans)]=0.\n",
    "                xbs = prege_[idx2pre[:,ibs],:]\n",
    "                pregbs  = np.apply_along_axis(lambda v: np.median(v[v!=0]), 0, xbs)\n",
    "                pregbs[np.isnan(pregbs)]=0.\n",
    "                xbs = postge_[idx2post[:,ibs],:]\n",
    "                postgbs = np.apply_along_axis(lambda v: np.median(v[v!=0]), 0, xbs)\n",
    "                postgbs[np.isnan(postgbs)]=0.\n",
    "\n",
    "                ge_bs[ibs,0:npregs] =pregbs\n",
    "                ge_bs[ibs,npregs:]  =postgbs\n",
    "\n",
    "\n",
    "            #df_hr2 = np.log2(df_hr2d)\n",
    "            #df_hr2 = pd.concat([df_hr2,df_hr2s],axis=1) # log2 normalization\n",
    "            #df_hr2 = df_hr2.replace({0:np.nan})\n",
    "            #df_hr2 = np.log2(df_hr2)\n",
    "\n",
    "            #df_hr2 = pd.concat([df_hr.iloc[1:,0],df_hr2],axis=1)\n",
    "\n",
    "            #df_hr2.dtypes\n",
    "            #df_hr2.head()\n",
    "\n",
    "            #df_hr2 = df_hr2.replace({0:np.nan})\n",
    "            #df_hrmg=df_hr2.groupby(['Class']) # logarithm_2 of UMI counts\n",
    "            #df_hrmg=df_hrmg.median(numeric_only=True) # median of log2(UMI)\n",
    "\n",
    "            #df_hrmg0 = df_hrmg.iloc[:,0:-1]\n",
    "\n",
    "            #df_hrmsg = pd.concat([df_hrmg.iloc[:,-1]]*df_hrmg0.shape[1],axis=1)\n",
    "            #df_hrmsg.columns = df_hrmg.iloc[:,0:-1].columns\n",
    "\n",
    "            #df_hrmg = df_hrmg.iloc[:,0:-1] - df_hrmsg +6*np.log2(10)  # transform from log2( UMI counts) to log2(CPM)\n",
    "\n",
    "            # ??? transform ge to median bootstraped log ???\n",
    "\n",
    "\n",
    "              \n",
    "\n",
    "        # add data to the final dataset    \n",
    "        gnij = np.repeat(np.array([syne3['cell_type2_pre'], syne3['cell_type2_post'],\n",
    "                                   syne3['layer_pre'], syne3['layer_post'],\n",
    "                                   syne3['cre_line_pre'], syne3['cre_line_post']]).reshape((1,6)),nsample,axis=0 )\n",
    "        gnij = pd.DataFrame(gnij, columns = ['cell_type2_pre', 'cell_type2_post',\n",
    "                                             'layer_pre','layer_post',\n",
    "                                             'cre_line_pre', 'cre_line_post'] )\n",
    "        \n",
    "        if  (do_bootstrap==0)|(do_median==0):\n",
    "            prege  = pd.DataFrame(prege,columns  = pregs3 )\n",
    "            postge = pd.DataFrame(postge,columns = postgs3 )\n",
    "\n",
    "\n",
    "            ge_dataij = pd.concat([gnij, prege, postge ], axis = 1)\n",
    "            \n",
    "        else:\n",
    "            ge_bs = pd.DataFrame(ge_bs,columns = pregs3 + postgs3 )\n",
    "\n",
    "\n",
    "            ge_dataij = pd.concat([gnij, ge_bs ], axis = 1)\n",
    "       \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # add stp data\n",
    "        if i<l_pre_post1:\n",
    "            stp_dataij =  df_stp.loc[[0,i], stp_type['stp_data_type']].T\n",
    "            stp_columns = stp_TM_names\n",
    "        else:\n",
    "            #stp_dataij  = STP_aba.loc[i1, STP_aba_type['stp_data_type']]\n",
    "            #stp_columns = np.array(stp_aba_names)\n",
    "            #nbs = int(len(stp_dataij)/len(stp_columns))\n",
    "            #stp_dataij  = pd.DataFrame(stp_dataij, index = np.repeat(stp_columns,nbs)).T\n",
    "            ##i2 = np.trunc(np.arange(len(stp_dataij.columns))/len(stp_columns)).astype(int)\n",
    "            #stp_dataij = stp_dataij.T.reset_index(drop=False)\n",
    "            #stp_dataij.columns =[0,i]\n",
    "            \n",
    "            stp_dataij  = STP_aba.loc[i1, STP_aba_type['stp_data_type']]\n",
    "            stp_columns = np.array(stp_aba_names)\n",
    "            #nbs = int(len(stp_dataij)/len(stp_columns))\n",
    "            nbs = int(stp_dataij.shape[0]) #/len(stp_columns))\n",
    "            stp_dataij  = pd.DataFrame(stp_dataij.reshape((-1,)), index = np.tile(np.array(stp_aba_names),(nbs,))).T\n",
    "            #i2 = np.trunc(np.arange(len(stp_dataij.columns))/len(stp_columns)).astype(int)\n",
    "            stp_dataij = stp_dataij.T.reset_index(drop=False)\n",
    "            stp_dataij.columns =[0,i]\n",
    "            \n",
    "        \n",
    "\n",
    "        i2 = np.trunc(np.arange(len(stp_dataij.index))/len(stp_columns)).astype(int)\n",
    "        stp_dataij  = stp_dataij.set_index(i2).pivot(columns=0,values=i).loc[:,stp_columns] #ss.set_index('par',append=True).unstack('par')\n",
    "        #stp_dataij =  stp_dataij.T.pivot(index=0, values=synij).T\n",
    "        \n",
    "        idx = np.mod(np.arange(nsample),len(stp_dataij.index))\n",
    "        stp_dataij=stp_dataij.iloc[idx,:].reset_index(drop=True)  \n",
    "        \n",
    "        i_data_set = pd.DataFrame(i+np.zeros(len(stp_dataij.index)), index = stp_dataij.index, columns = ['index_ds'])\n",
    "        stp_dataij=pd.concat([stp_dataij, i_data_set],axis=1)\n",
    "        \n",
    "        #if do_averagig_stp:\n",
    "        #    #stp_dataij.loc[:,:] = stp_dataij.mean(axis=0).values\n",
    "        #    stp_dataij.loc[:,:] = stp_dataij.median(axis=0).values\n",
    "\n",
    "        \n",
    "        #stp_dataij = pd.DataFrame({'presynaptic':pree[0], 'postsynaptic': poste[0], \n",
    "        #                           'stp_data':stp_dataij.iloc[0,:].values})\n",
    "        #stp_data  = stp_data.append(stp_dataij) \n",
    "        \n",
    "        # names of ge and stp columns\n",
    "\n",
    "        \n",
    "        ge_dataij  = pd.concat([ge_dataij,stp_dataij] , axis=1)\n",
    "        \n",
    "        # add to all ge and stp dataframe\n",
    "        ge_data  = pd.concat([ge_data, ge_dataij] , axis=0)\n",
    "        \n",
    "\n",
    "        \n",
    "#stp_data =  stp_data.reset_index(drop=True)  \n",
    "\n",
    "\n",
    "\n",
    "#stp_data.loc[stp_data.loc[:,'postsynaptic']=='NGC','postsynaptic']='Lamp5'\n",
    "#stp_data2 = ge_data.iloc[:,0:3].copy(deep=True)                                 # modify!\n",
    "#stp_data2.columns=['presynaptic', 'postsynaptic', 'ppr']\n",
    "#print(stp_data2.head())\n",
    "#for i in stp_data.index: #stp_data.index:\n",
    "#    pre=stp_data.loc[i,'presynaptic']\n",
    "#    post=stp_data.loc[i,'postsynaptic']\n",
    "#    stp_data2.loc[stp_data2.loc[:,'presynaptic'].isin([pre])&stp_data2.loc[:,'postsynaptic'].isin([post]),'ppr']=stp_data.loc[i,'stp_data'] \n",
    "    \n",
    "ge_data = ge_data.reset_index()    \n",
    "\n",
    " \n",
    "## transform labels from TM to An:A1\n",
    "As = STP_sim_complex(ge_data,l_pre_post2,stp_aba_names)\n",
    "\n",
    "N = As.shape[1]\n",
    "\n",
    "As =  As[:,1:]/As[:,0].reshape((-1,1))\n",
    "\n",
    "#As =  As[:,1:]/np.tile(As[:,0],(N-1,1)).T\n",
    "\n",
    "## add new stp parameters to ge_data dataset\n",
    "#stp_columns2 = ['A'+str(s)+'/A1' for s in range(1,N+1)]\n",
    "stp_columns2 = ['A'+str(s)+'/A1' for s in range(2,N+1)]\n",
    "stp_data2 = pd.DataFrame(As,columns = stp_columns2, index = ge_data.index)\n",
    "\n",
    "ge_data = pd.concat([ge_data, stp_data2 ],axis=1)\n",
    "\n",
    "if do_averagig_stp:\n",
    "     #stp_dataij.loc[:,:] = stp_dataij.mean(axis=0).values\n",
    "     #stp_dataij.loc[:,:] = stp_dataij.median(axis=0).values\n",
    "    all_i_ds =  ge_data.loc[:,'index_ds']                             \n",
    "    for i_ds in list(set(all_i_ds)):\n",
    "              ids2 =  ge_data.loc[:,'index_ds'].isin([i_ds])  \n",
    "              ge_data.loc[ids2,stp_columns2] = ge_data.loc[ids2,stp_columns2].median(axis=0).values                   \n",
    "                                   \n",
    "                                   \n",
    "## filter TM parameters\n",
    "do_filter_pars = 0\n",
    "\n",
    "if do_filter_pars!=0:\n",
    "    print((ge_data.loc[:,stp_columns ]<0).sum().sum())\n",
    "    print(len(ge_data.index))\n",
    "\n",
    "    nannot=6\n",
    "    annot_columns = ge_columns[0:nannot]\n",
    "    ge_data2 = ge_data.loc[:,annot_columns + stp_columns2].groupby(annot_columns).mean()\n",
    "    f,ax = plt.subplots(figsize=(5, 5))\n",
    "    ge_data2.T.plot(ax=ax)\n",
    "    plt.gca().legend('')\n",
    "    #plt.gca().title('An:A1 before filtering data')\n",
    "    ax.title.set_text('An:A1 before filtering data')\n",
    "    #ge_data2 = ge_data.loc[:,annot_columns + stp_columns2].groupby(annot_columns).mean()\n",
    "    print('mean of stp before filtering:')\n",
    "    print(ge_data2)\n",
    "\n",
    "    #i_filter_out = (ge_data.loc[:,'dp/p0']*ge_data.loc[:,'p0']+)|(ge_data.loc[:,'tD'])\n",
    "    #i_filter_out = i_filter_out&((ge_data.loc[:,'tF'])|(ge_data.loc[:,'tF']))\n",
    "    i_filter_out = np.sum(ps2>1,axis=1)|np.sum(ps2<0,axis=1)\n",
    "    i_filter_out = i_filter_out|(np.sum(ns2>1,axis=1)|np.sum(ns2<0,axis=1))\n",
    "    ge_data = ge_data.loc[i_filter_out==False ,:]\n",
    "\n",
    "    ge_data2 = ge_data.loc[:,annot_columns + stp_columns2].groupby(annot_columns).mean()\n",
    "    f1,ax1 = plt.subplots(figsize=(5, 5))\n",
    "    ge_data2.T.plot(ax=ax1)\n",
    "    plt.gca().legend('')\n",
    "    #plt.gca().title('An:A1 after filtering data')\n",
    "    ax1.title.set_text('An:A1 after filtering data')\n",
    "    #ge_data2 = ge_data.loc[:,annot_columns + stp_columns2].groupby(annot_columns).mean()\n",
    "\n",
    "    print('mean of stp after filtering:')\n",
    "    print(ge_data2)\n",
    "    \n",
    "#print('mean of stp after filtering:')\n",
    "#print(ge_data2)\n",
    "\n",
    "\n",
    "print(len(ge_data.index))\n",
    "#ge_data.loc[:,stp_columns ] =ge_data.loc[:,stp_columns ].abs()  \n",
    "\n",
    "\n",
    "#\n",
    "## normalize stp parameters - for TM parameters\n",
    "do_normalize_stp_pars = 0;\n",
    "if do_normalize_stp_pars!=0:\n",
    "    ge_data.loc[:,stp_columns  ] =np.log(ge_data.loc[:,stp_columns  ].values.astype(float))\n",
    "\n",
    "#  assign training and labels \n",
    "stp_columns1 = stp_columns \n",
    "stp_columns = stp_columns2 # select training labels names\n",
    "\n",
    "nannot = 7\n",
    "nstp =len(stp_columns)\n",
    "annot_columns = ge_columns[0:6]+['index_ds']\n",
    "\n",
    "## averaging of stp for each synapse type \n",
    "\n",
    "\n",
    "#X = ge_data.loc[:,ge_data.columns[0:]].values\n",
    "X_c = ge_data.loc[:,annot_columns + ge_columns[6:]].values\n",
    "y_c = ge_data.loc[:,annot_columns + stp_columns].values \n",
    "#y = stp_data2.loc[:,:].values\n",
    "\n",
    "## normalize stp parameters - for TM parameters\n",
    "#y[:,nannot: ] =np.log(y[:,nannot: ].astype(float))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# print elapsed time\n",
    "t2 = time.time()\n",
    "#print(regr.feature_importances_)\n",
    "print('Elapsed time: '+str(t2-t1))\n",
    "\n",
    "print(y_c.shape)\n",
    "X_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# transform labels from TM to An:A1\n",
    "fs = [20 , 10, 40] # Hz\n",
    "N = 8\n",
    "\n",
    "Trec = [250, 1000]\n",
    "Trec=[]\n",
    "DT0 = 25000\n",
    "xs  =ge_data.iloc[l_pre_post2:,:].loc[:,stp_aba_names].values\n",
    "xs  =np.delete(xs, [5,6],axis=1)\n",
    "As=np.zeros((l_pre_post2,0))\n",
    "As2=np.zeros((xs.shape[0],0))\n",
    "if do_add_aba_synphys:\n",
    "    \n",
    "    for f in fs:\n",
    "        T = np.arange(N)*1000/f\n",
    "        \n",
    "        Asf, ns, ps, dpp0, p0, tF, tD, A = STP_sim(ge_data.iloc[0:l_pre_post2,:],T)\n",
    "        As = np.concatenate([As,Asf],axis=1)\n",
    "        \n",
    "        for ri in range(len(Trec)):\n",
    "            Asr, nsr, psr, dpp0, p0, tF, tD, A = STP_sim(ge_data.iloc[0:l_pre_post2,:],[Trec[ri]],init_state=[ns[:,-1],ps[:,-1]])\n",
    "            As = np.concatenate([As,Asr],axis=1)\n",
    "        \n",
    "\n",
    "        As2f = np.zeros((xs.shape[0],N+len(Trec)))\n",
    "        for i2 in range(xs.shape[0]):\n",
    "            #as2, sts2 = STP_sim2(xs[i2,:],np.arange(8)*50,model_type = 'tm5') # preconditioning series???\n",
    "            #as2, sts2 = STP_sim2(xs[i2,:], T+DT0, init_state=sts2[-1], model_type='tm5' ) \n",
    "            \n",
    "            as2, sts2 = STP_sim2(xs[i2,:],T,model_type = 'tm5_smr')\n",
    "            \n",
    "            As2f[i2,0:N] = as2\n",
    "            for ri in range(len(Trec)):\n",
    "                as2r, sts2r = STP_sim2(xs[i2,:], [Trec[ri]], init_state=sts2[-1], model_type='tm5_smr' ) \n",
    "                As2f[i2,N+ri] = as2r\n",
    "                \n",
    "        As2 = np.concatenate([As2,As2f],axis=1)\n",
    "            \n",
    "    \n",
    "    As2 = As2/As2[:,0].reshape((-1,1))\n",
    "    As = As/As[:,0].reshape((-1,1))\n",
    "    As = np.concatenate([As,As2],axis=0)\n",
    "else:\n",
    "    As, ns, ps, dpp0, p0, tF, tD, A = STP_sim(ge_data,T)\n",
    "    \n",
    "\n",
    "import matplotlib.pyplot as plt    \n",
    "plt.plot(As2[0:100,:].transpose(),'o-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "##\n",
    "##  combine cortex and hippocampus datasets \n",
    "##\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ge_data_h = ge_data\n",
    "# X_h = X\n",
    "# y_h = y\n",
    "# annot_columns_h = annot_columns\n",
    "# ge_columns_h = ge_columns\n",
    "# stp_columns_h = stp_colum\n",
    "\n",
    "X = np.concatenate([X_c, X_h],axis=0)\n",
    "y = np.concatenate([y_c, y_h],axis=0)\n",
    "\n",
    "print(X_h.shape)\n",
    "print(X_c.shape)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "all_types = []\n",
    "for i in ge_data_h.index:\n",
    "    all_type=''\n",
    "    for j in range(6):\n",
    "        all_type = all_type + str(ge_data_h.loc[i,ge_data_h.columns[j+1]])+' '\n",
    "    all_types = all_types  +[all_type]    \n",
    "\n",
    "all_types =  pd.DataFrame(all_types)\n",
    "subclasses_hipp1 = pd.DataFrame(pd.unique(all_types.loc[:,0].values))\n",
    "#subclasses =  pd.DataFrame(subclasses,columns=['pre', 'post'] )\n",
    "#subclasses_hipp = pd.concat([subclasses_hipp1, subclasses], axis=1)\n",
    "subclasses2_hipp = ['edg','eca','ee','Pvalb','Sst','Vip','Lamp5','Cck']\n",
    "subclasses3_hipp = [['DG'],\n",
    "                    ['CA1','CA2','CA3'],\n",
    "                    ['EC'],\n",
    "                    ['PV','Pvalb','PVBC','AAC'],\n",
    "                    ['Sst','HIPP','O-LM','O-Bi'],\n",
    "                    ['Vip','IS3'],\n",
    "                    ['Lamp5','Ivy cell','NGF'],\n",
    "                    ['Cck','CCKBC','HICAP','CCK_DTI']]\n",
    "subclasses_hipp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "subclasses2_hipp = ['e_dg','e_ca','e_e','Pvalb_h','Sst_h','Vip','Lamp5_h','Cck_h']\n",
    "subclasses3_hipp = [['DG','CA3','CA3 ventral'],\n",
    "                    ['CA1','CA1 ventral'],\n",
    "                    ['EC'],\n",
    "                    ['PV','Pvalb','PVBC','AAC'],\n",
    "                    ['Sst','HIPP','O-LM','O-Bi'],\n",
    "                    ['Vip','IS3'],\n",
    "                    ['Lamp5','Ivy cell','NGF'],\n",
    "                    ['Cck','CCKBC','HICAP','CCK_DTI']]\n",
    "\n",
    "\n",
    "ge_data_h_subcl = pd.DataFrame(np.zeros((ge_data_h.shape[0],4)),columns = ['name_long_pre','name_long_post',\n",
    "                                                                           'subclass_pre','subclass_post'])\n",
    "ge_data_h_subcl.index = ge_data_h.index\n",
    "precol = ge_data_h.columns[[1,5]]\n",
    "postcol = ge_data_h.columns[[2,6]]\n",
    "for i in ge_data_h.index:\n",
    "    all_type_1=''\n",
    "    all_type_2=''\n",
    "    for j in range(3):\n",
    "        all_type_1 = all_type_1+ str(ge_data_h.loc[i,ge_data_h.columns[2*j+1]])+' '\n",
    "    for j in range(3):\n",
    "        all_type_2 = all_type_2+ str(ge_data_h.loc[i,ge_data_h.columns[2*j+2]])+' '  \n",
    "    ge_data_h_subcl.loc[i,'name_long_pre'] = all_type_1\n",
    "    ge_data_h_subcl.loc[i,'name_long_post'] = all_type_2\n",
    "    \n",
    "    ge_data_hi = ge_data_h.loc[i,:]\n",
    "    for j in range(len(subclasses3_hipp)): \n",
    "        is_in_pre = ge_data_hi.loc[precol].isin(subclasses3_hipp[j]).sum()>0\n",
    "        is_in_post = ge_data_hi.loc[postcol].isin(subclasses3_hipp[j]).sum()>0\n",
    "        if is_in_pre:\n",
    "            ge_data_h_subcl.loc[i,'subclass_pre'] =subclasses2_hipp[j]\n",
    "        if is_in_post:\n",
    "            ge_data_h_subcl.loc[i,'subclass_post'] =subclasses2_hipp[j]    \n",
    "        \n",
    "    #all_types = all_types  +[all_type]    \n",
    "\n",
    "    \n",
    "    \n",
    "# all_types =  pd.DataFrame(all_types)   \n",
    "# subclasses = [['Pvalb', 'eh'],\n",
    "#               ['Pvalb', 'Pvalb'],\n",
    "#               ['eh','Cck'],\n",
    "#               ['ee','Lamp5'],\n",
    "#               ['eh','eh'],\n",
    "#               ['ee','Lamp5'],\n",
    "#               ['Cck','eh'],\n",
    "#               ['Pvalb','eh'],\n",
    "#               ['Lamp5', 'Lamp5'],\n",
    "#               ['eh','Pvalb'],\n",
    "#               ['eh','eh'],\n",
    "#               ['eh','eh'],\n",
    "#               ['Lamp5','eh'],\n",
    "#               ['Cck','eh'],\n",
    "#               ['eh','Pvalb'],\n",
    "#               ['Lamp5','eh'],\n",
    "#               ['Pvalb','Pvalb'],\n",
    "#               ['ee','Lamp5'],\n",
    "#               ['ee','Lamp5'],\n",
    "#               ['eh','eh'],\n",
    "#               ['eh','Cck'],\n",
    "#               ['ee','Cck'],\n",
    "#               ['ee','eh'],\n",
    "#               ['eh','Sst'],\n",
    "#               ['Vip','Sst'],\n",
    "#               ['ee','eh'],\n",
    "#               ['ee','Pvalb'],\n",
    "#               ['eh','Lamp5'],\n",
    "#               ['eh','Lamp5'],\n",
    "#               ['Cck','Cck'],\n",
    "#               ['Pvalb','eh'],\n",
    "#               ['eh', 'Sst']]\n",
    "\n",
    "# #subclasses_hipp1 = pd.DataFrame(list(set(all_types)) )\n",
    "# subclasses_hipp1 = pd.DataFrame(pd.unique(all_types.loc[:,0].values))\n",
    "# subclasses =  pd.DataFrame(subclasses,columns=['pre', 'post'] )\n",
    "# subclasses_hipp = pd.concat([subclasses_hipp1, subclasses], axis=1)\n",
    "\n",
    "\n",
    "# ge_data_h_subcl = pd.DataFrame(np.tile(all_types,(1,4)),columns=['name_long_pre','name_long_post',\n",
    "#                                                                  'subclass_pre','subclass_post'])\n",
    "# for i in range(subclasses_hipp.shape[0]):\n",
    "\n",
    "#     ge_data_h_subcl.loc[ge_data_h_subcl.loc[:,'name_long'].isin([subclasses_hipp.loc[i,0]]),'subclass_pre'] =subclasses_hipp.loc[i,'pre']\n",
    "#     ge_data_h_subcl.loc[ge_data_h_subcl.loc[:,'name_long'].isin([subclasses_hipp.loc[i,0]]),'subclass_post'] =subclasses_hipp.loc[i,'post']\n",
    "t2=time.time()\n",
    "print('elapsed time : '+str(t2-t1))\n",
    "ge_data_h_subcl    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_types = []\n",
    "all_types2 = []\n",
    "columns = ['cell_type2_pre','cell_type2_post','layer_pre','layer_post','cre_line_pre','cre_line_post']\n",
    "for i in ge_data.index:\n",
    "    all_type=''\n",
    "    all_type2=''\n",
    "    for j in range(3):\n",
    "        sj = str(ge_data.loc[i,columns[2*j]])\n",
    "        if sj=='MC':\n",
    "            sj='Sst'    \n",
    "        all_type = all_type  +' '+sj\n",
    "        \n",
    "    for j in range(3):\n",
    "        sj = str(ge_data.loc[i,columns[2*j+1]])\n",
    "        if sj=='MC':\n",
    "            sj='Sst'    \n",
    "        all_type2 = all_type2 +' '+sj    \n",
    "    all_types = all_types  +[str.lower(all_type)]  \n",
    "    all_types2 = all_types2  +[str.lower(all_type2)]   \n",
    "\n",
    "all_types =  pd.DataFrame(all_types, columns=['pre'])\n",
    "all_types2 =  pd.DataFrame(all_types2, columns=['post'])\n",
    "all_types = pd.concat([all_types,all_types2], axis=1)\n",
    "\n",
    "subclasses = pd.Series(['Sst','Pvalb','Vip','ev_l23','ev_l4','ev_l56','ev_l56'])\n",
    "subclasses2 = pd.Series(['sst','pvalb','vip','l2',   'l4',   'l5',    'l6'])\n",
    "subclasses2_visp =subclasses2   \n",
    "\n",
    "subclasses_visp_1 = pd.DataFrame(pd.unique(all_types.loc[:,'pre'].values))\n",
    "subclasses_visp_2 = pd.DataFrame(pd.unique(all_types.loc[:,'post'].values))\n",
    "# subclasses =  pd.DataFrame(subclasses,columns=['pre', 'post'] )\n",
    "# subclasses_visp = pd.concat([subclasses_visp1, subclasses], axis=1)\n",
    "\n",
    "ge_data_c_subcl = pd.DataFrame(np.tile(all_types,(1,2)))\n",
    "ge_data_c_subcl.columns=['name_long_pre','name_long_post','subclass_pre','subclass_post']\n",
    "for i in range(subclasses_visp_1.shape[0]):\n",
    "    type_name = str.lower(subclasses_visp_1.loc[i,0])\n",
    "#     is_in=[]\n",
    "#     for si in subclasses2:\n",
    "#         is_in = is_in +[ss.contains(si)]\n",
    "#     is_in=np.nonzero(np.array(is_in))[0][0] \n",
    "#     subcl_name = subclasses[is_in]\n",
    "    \n",
    "    for ii,si in enumerate(subclasses2):\n",
    "        if si in type_name:\n",
    "            subcl_name = subclasses[ii]\n",
    "            break\n",
    "    ge_data_c_subcl.loc[ge_data_c_subcl.loc[:,'name_long_pre']==type_name,'subclass_pre'] =subcl_name\n",
    "    \n",
    "for i in range(subclasses_visp_2.shape[0]):\n",
    "    type_name = str.lower(subclasses_visp_2.loc[i,0])\n",
    "    for ii,si in enumerate(subclasses2):\n",
    "        if si in type_name:\n",
    "            subcl_name = subclasses[ii]\n",
    "            break\n",
    "    ge_data_c_subcl.loc[ge_data_c_subcl.loc[:,'name_long_post']==type_name,'subclass_post'] =subcl_name    \n",
    "\n",
    "#     ge_data_c_subcl.loc[ge_data_c_subcl.loc[:,'name_long'].isin([subclasses_visp.loc[i,0]]),'subclass_pre'] =subclasses_visp.loc[i,'pre']\n",
    "#     ge_data_c_subcl.loc[ge_data_c_subcl.loc[:,'name_long'].isin([subclasses_visp.loc[i,0]]),'subclass_post'] =subclasses_visp.loc[i,'post']\n",
    "\n",
    "#ge_data_c_subcl    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge_data_subcl = pd.concat([ge_data_c_subcl, ge_data_h_subcl],sort=False)\n",
    "subclasses2 =pd.concat([subclasses, pd.Series(subclasses2_hipp)])\n",
    "ge_data_subcl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# TRAIN MODEL\n",
    "\n",
    "##\n",
    "##\n",
    "##  train RF \n",
    "##\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subclasses2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['all','ex', 'inh', 'ex_ctx', 'ex_hipp','ex_ec','mge','cge','pvalb','vip','sst','cck','lamp5','ex_he',\n",
    "          'ex_l23','ex_l6','ex_l4','ca','dg','pvalb_h','pvalb_c','sst_h','sst_c']\n",
    "y_features = pd.DataFrame([['A2/A1_20Hz', 'A5/A1_20Hz', 'A2/A1_50Hz','A5/A1_50Hz','A250/A1','A1000/A1'],[0,4,8,11,5,6]],index=['names','index_y'])\n",
    "print(\"y features : \\n\", y_features,\"\\n\\n\")\n",
    "subclasses2 = pd.unique(subclasses2 )\n",
    "print(\"subclasses : \\n\",subclasses2,\"\\n\\n\") \n",
    "\n",
    "cl_in_subcl = np.array([[0,1,2,3,4,5,6,7,8,9,10,11,12],[3,4,5,6,7,8],\n",
    "                        [0,1,2,9,10,11,12],[3,4,5],[6,7],[8],[1,9,0,10],\n",
    "                        [2,11,12],[1,9],[2],[0,10],[12],[11],[6,7,8],[3],[4],[5],[7],[6],[9],[1],[10],[0]])\n",
    "classes = pd.DataFrame([classes, cl_in_subcl],index=['classes','subclasses'])\n",
    "classes = classes.T.set_index('classes').T\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(X,y,data_subclasses,subclasses):\n",
    "    #X_train = np.array([]).reshape((0,X.shape[1]))\n",
    "    #y_train = np.array([]).reshape((0,y.shape[1]))\n",
    "\n",
    "    in_subcl = (data_subclasses.loc[:,'subclass_post']!=False)\n",
    "    for i0 in range(len(subclasses[0])):\n",
    "        in_subcl = in_subcl|(data_subclasses.loc[:,'subclass_pre']==subclasses[0][i0])\n",
    "        \n",
    "    for i0 in range(len(subclasses[1])):        \n",
    "        #in_subcl = in_subcl|(ge_data_subcl.loc[:,'subclass_post']==subclasses2[sbi[i0]])\n",
    "        in_subcl = in_subcl|(data_subclasses.loc[:,'subclass_post']==subclasses[1][i0])\n",
    "        \n",
    "    in_subcl = np.nonzero(in_subcl.values)[0]\n",
    "    #X_train = np.concatenate([X_train,X[in_subcl]])   #np.delete(X, in_subcl,axis=0)\n",
    "    #y_train = np.concatenate([y_train,y[in_subcl]])   #np.delete(y, in_subcl,axis=0)\n",
    "    X_train = np.copy(X[in_subcl])   #np.delete(X, in_subcl,axis=0)\n",
    "    y_train = np.copy(y[in_subcl])   #np.delete(y, in_subcl,axis=0)\n",
    "    return X_train, y_train    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modf = pd.read_excel('synapses_types_tree.xlsx')\n",
    "modf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stp_columns3 = []\n",
    "stp_columns0 = ['A1',\n",
    "                'A2',\n",
    "                'A3',\n",
    "                'A4',\n",
    "                'A5']\n",
    "stpn_fr=[s+'_20Hz' for s in stp_columns0[1:]]\n",
    "stp_columns3 = stp_columns3 +stpn_fr+['A250_20Hz', 'A1000_20Hz']\n",
    "\n",
    "stpn_fr=[s+'_50Hz' for s in stp_columns0[0:]]\n",
    "stp_columns3 = stp_columns3 +stpn_fr+['A250_50Hz', 'A1000_50Hz']\n",
    "\n",
    "stpn_fr=[s+'_10Hz' for s in stp_columns0[0:]]\n",
    "stp_columns3 = stp_columns3 +stpn_fr+['A250_10Hz', 'A1000_10Hz']\n",
    "stp_columns3\n",
    "\n",
    "X_train = pd.DataFrame(np.copy(X),columns = annot_columns+ge_columns[6:])   #np.delete(X, in_subcl,axis=0)\n",
    "y_train = pd.DataFrame(np.copy(y),columns = annot_columns+stp_columns3)     #np.delete(X, in_subcl,axis=0)\n",
    "\n",
    "X_train = pd.concat([X_train,y_train.loc[:,stp_columns3]],axis=1)\n",
    "\n",
    "#X_train \n",
    "\n",
    "#classes \tall \tex \tinh \tex_ctx \tex_hipp \tex_ec \tmge \tcge \tpvalb \tvip \tsst \tcck \tlamp5\n",
    "\n",
    "modf = pd.read_excel('synapses_types_tree.xlsx')\n",
    "modf2 = pd.read_excel('synapses_types_tree.xlsx',sheet_name='Sheet2')\n",
    "mod =[ {'name': 'all',    'classes':['all'],                                   'classes':'pre'},\n",
    "       {'name': 'ex_inh', 'classes':['ex','inh'],                              'side':'pre'},\n",
    "       {'name': 'ex_inh_post', 'classes':['ex','inh'],                         'side':'post'},\n",
    "       {'name': 'ex2_inh', 'classes':['ex_ctx','ex_ec','ex_hipp','inh'],       'side':'pre'},\n",
    "       {'name': 'ex_inh2', 'classes':['ex','mge','cge'],                       'side':'pre'},\n",
    "       {'name': 'ex_inh', 'classes':['ex','inh'],                              'side':'pre'},\n",
    "       {'name': 'ex2_inh', 'classes':['ex_ctx','ex_ec','ex_hipp','inh'],       'side':'pre'},\n",
    "       {'name': 'ex_inh3', 'classes':['ex','pvalb','sst','cge'],               'side':'pre'},\n",
    "       {'name': 'ex_inh4', 'classes':['ex','pvalb','sst','vip','cck','lamp5'], 'side':'pre'},\n",
    "     ]\n",
    "\n",
    "subcl_pre=ge_data_subcl.loc[:,'subclass_pre'].copy()\n",
    "subcl_pre.index=X_train.index\n",
    "\n",
    "subcl_post=ge_data_subcl.loc[:,'subclass_post'].copy()\n",
    "subcl_post.index=X_train.index\n",
    "\n",
    "# classes_columns = []\n",
    "# for md in mod:\n",
    "#     in_subcl = subcl_pre.copy()\n",
    "#     for icl, cl in enumerate(md['classes']):\n",
    "#         #in_subcl = in_subcl|(ge_data_subcl.loc[:,'subclass_pre'].isin(classes.loc['subclasses']))\n",
    "#         subcl = subclasses2[classes.loc['subclasses',cl]]\n",
    "#         in_subcl.loc[subcl_pre.isin(subcl)] = icl #icl\n",
    "#     in_subcl.name =  md['name']  \n",
    "#     classes_columns = classes_columns + [in_subcl.name]\n",
    "#     X_train = pd.concat([X_train,in_subcl],axis=1)    \n",
    "    \n",
    "classes_columns = []\n",
    "for i in modf.index:\n",
    "    md = modf.loc[i,:]\n",
    "    in_subcl = subcl_pre.copy()\n",
    "    mdcl = np.char.strip(np.array(str.split(md['classes'],',')))\n",
    "    for icl, cl in enumerate(mdcl):\n",
    "        #in_subcl = in_subcl|(ge_data_subcl.loc[:,'subclass_pre'].isin(classes.loc['subclasses']))\n",
    "        subcl = subclasses2[classes.loc['subclasses',cl]]\n",
    "        in_subcl.loc[subcl_pre.isin(subcl)] = icl #icl\n",
    "    in_subcl.name =  md['name']+'__pre'  \n",
    "    classes_columns = classes_columns + [in_subcl.name]\n",
    "    X_train = pd.concat([X_train,in_subcl],axis=1)  \n",
    "    \n",
    "    in_subcl = subcl_post.copy()\n",
    "    mdcl = np.char.strip(np.array(str.split(md['classes_post'],',')))\n",
    "    for icl, cl in enumerate(mdcl):\n",
    "        #in_subcl = in_subcl|(ge_data_subcl.loc[:,'subclass_pre'].isin(classes.loc['subclasses']))\n",
    "        subcl = subclasses2[classes.loc['subclasses',cl]]\n",
    "        in_subcl.loc[subcl_post.isin(subcl)] = icl #icl\n",
    "    in_subcl.name =  md['name']+'__post'  \n",
    "    classes_columns = classes_columns + [in_subcl.name]\n",
    "    X_train = pd.concat([X_train,in_subcl],axis=1)\n",
    "        \n",
    "         \n",
    "#for i in\n",
    "#mod1_ = mod1_ +  [ sm.OLS( y_train, X_train) ]\n",
    "#res = mod.fit()\n",
    "#print(res.summary())\n",
    "ge_columns_train      = ge_columns[6:]\n",
    "annot_columns_train   = annot_columns\n",
    "stp_columns_train     = stp_columns3\n",
    "classes_columns_train = classes_columns\n",
    "print(X_train.columns)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns  =[\n",
    "[ 'ge_columns_train',\n",
    "  'annot_columns_train',\n",
    "  'stp_columns_train',\n",
    "  'classes_columns_train',],\n",
    "[ ge_columns_train,\n",
    "  annot_columns_train,\n",
    "  stp_columns_train,\n",
    "  classes_columns_train,\n",
    "]]\n",
    "columns = pd.DataFrame(columns)\n",
    "columns=columns.T.set_index([0]).T\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns.to_hdf('stp_to_ge_all_columns.hdf',key='data')\n",
    "X_train.to_hdf('stp_to_ge_all_data.hdf',key='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "columns  = pd.read_hdf('stp_to_ge_all_columns.hdf')\n",
    "ge_columns_train      = columns.loc[:,'ge_columns_train'].values[0]\n",
    "annot_columns_train   = columns.loc[:,'annot_columns_train'].values[0]\n",
    "stp_columns_train     = columns.loc[:,'stp_columns_train'].values[0]\n",
    "classes_columns_train = columns.loc[:,'classes_columns_train'].values[0]\n",
    "#print(X_train.columns)\n",
    "X_train0 = pd.read_hdf('stp_to_ge_all_data.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit cell types hierarhy tree with linear models in leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_classes_tree(X2,y2,X2_cl,cli,model_type='lasso',nmin = 20, alpha=1, l1_ratio=0.5):\n",
    "    model = []\n",
    "    support =[]\n",
    "    ncl1 = np.max(X2_cl[:,cli[0]])+1 \n",
    "    ncl2 = np.max(X2_cl[:,cli[1]])+1 \n",
    "    for i1 in range(ncl1):\n",
    "        model1 = []\n",
    "        support1 = []\n",
    "        for i2 in range(ncl2):\n",
    "\n",
    "            is_in_cl = (X2_cl[:,cli[0]]==i1)&(X2_cl[:,cli[1]]==i2)\n",
    "            #           support2 = [np.sum(is_in_cl),\n",
    "            #           np.nonzero((X2_cl[:,cli[0]]==i1))[0],\n",
    "            #           np.nonzero((X2_cl[:,cli[1]]==i2))[0]]\n",
    "            support2 = np.sum(is_in_cl)\n",
    "            \n",
    "            support1 = support1 + [support2]\n",
    "            regr = 0\n",
    "            if support2>nmin:\n",
    "                y_train = y2[is_in_cl,:]\n",
    "                X_train = X2[is_in_cl,:]\n",
    "                if model_type=='ridge':\n",
    "                    regr = sk.linear_model.Ridge(alpha=0.50, fit_intercept=True, normalize=False,\n",
    "                                                 copy_X=True, max_iter=None, tol=0.001, solver='auto',\n",
    "                                                 random_state=None)\n",
    "                \n",
    "                elif model_type=='lasso':\n",
    "                    regr = sk.linear_model.Lasso(alpha=alpha, fit_intercept=True, normalize=False,\n",
    "                                                 precompute=False, copy_X=True,\n",
    "                                                 max_iter=1000, tol=0.0001, warm_start=False, positive=False, \n",
    "                                                 random_state=None, selection='cyclic')\n",
    "                elif model_type=='elastic_net':   \n",
    "                    regr = sk.linear_model.ElasticNet(alpha=alpha, l1_ratio=l1_ratio, fit_intercept=True,\n",
    "                                    normalize=False,\n",
    "                                    max_iter=1000, copy_X=True, tol=0.0001, warm_start=False,\n",
    "                                    random_state=None, selection='cyclic')\n",
    "                else:\n",
    "                    print(\" model types supported: ridge, lasso, elastic_net\")\n",
    "                \n",
    "                regr.fit(X_train, y_train)\n",
    "                #y_pred = regr.predict(X_test[:,nannot:]) \n",
    "\n",
    "\n",
    "            model1 = model1 + [regr]\n",
    "            \n",
    "        support = support + [np.array(support1)]\n",
    "        model = model + [np.array(model1)]    \n",
    "    return np.array(model), np.array(support)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_classes_tree(model,X2,X2_cl,cli,nout=1,nmin = 20):\n",
    "    \n",
    "    ncl1 = np.max(X2_cl[:,cli[0]])+1 \n",
    "    ncl2 = np.max(X2_cl[:,cli[1]])+1 \n",
    "    y_pred = np.zeros((X2.shape[0],nout))\n",
    "    for i1 in range(ncl1):\n",
    "        #print(i1,\" f1\")\n",
    "        if len(model)>i1:\n",
    "            #print(i1,\" f2\")\n",
    "            model1 = model[i1]\n",
    "            for i2 in range(ncl2):\n",
    "\n",
    "                is_in_cl = (X2_cl[:,cli[0]]==i1)&(X2_cl[:,cli[1]]==i2)\n",
    "\n",
    "                support2 = np.sum(is_in_cl)\n",
    "                \n",
    "                #print(i1,i2,\" f3 \",support2)\n",
    "                #regr = 0\n",
    "                if support2>nmin:\n",
    "                    #print(i1,i2,\" f3 \",support2)\n",
    "                    if len(model1)>i2:\n",
    "                        #print(i1,i2,\" f4\")\n",
    "                        regr = model1[i2]\n",
    "                        #regr = sk.linear_model.MultiTaskElasticNet(alpha=alpha, l1_ratio=l1_ratio, fit_intercept=True,\n",
    "                        #                    normalize=False,\n",
    "                        #                    max_iter=1000, copy_X=True, tol=0.0001, warm_start=False,\n",
    "                        #                    random_state=None, selection='cyclic')\n",
    "                        #regr.fit(X_train, y_train)\n",
    "                        if regr!=0:\n",
    "                            X_test = X2[is_in_cl,:]\n",
    "                            y_pred[is_in_cl] = regr.predict(X_test).reshape((-1,1)) \n",
    "    \n",
    "    return y_pred       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Draw some interesting plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load important genes from iRF search\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "imp  = np.load('importance.npy')\n",
    "imp = pd.DataFrame(imp)\n",
    "imp = pd.concat([imp,pd.DataFrame(ge_columns[6:])],axis=1)\n",
    "imp.columns = ['importance', 'genes']\n",
    "imp = imp.set_index('genes')\n",
    "\n",
    "imp0  = np.load('importance0.npy')\n",
    "imp0 = pd.DataFrame(imp0)\n",
    "imp0 = pd.concat([imp0,pd.DataFrame(ge_columns[6:])],axis=1)\n",
    "imp0.columns = ['importance0', 'genes']\n",
    "imp0 = imp0.set_index('genes')\n",
    "\n",
    "\n",
    "# List of important genes : iRF\n",
    "imp = pd.concat([imp,imp0],axis=1)\n",
    "\n",
    "imp50 = imp.sort_values('importance',ascending=False).iloc[0:49,:]\n",
    "plt.plot(imp.sort_values('importance',ascending=False).loc[:,['importance','importance0']].values)\n",
    "imp50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "t1=time.time()\n",
    "\n",
    "Dn = 2\n",
    "cla_n = classes_columns_train #['ex_inh']\n",
    "ge_n = imp50.index[0:50].tolist() #ge_columns_train #imp50.index[0:25].tolist()\n",
    "X2 = X_train0.loc[:,ge_n ]\n",
    "X2_cl = X_train0.loc[:,cla_n ]\n",
    "#i_cl = np.nonzero(X2.columns.isin(cla_n))[0]\n",
    "X2=X2.iloc[0:X_train0.shape[0]:Dn,:].values\n",
    "X2_cl=X2_cl.iloc[0:X_train0.shape[0]:Dn,:].values\n",
    "stp_n = ['A2_20Hz']\n",
    "#  'A2_20Hz',\n",
    "#  'A5_20Hz',\n",
    "#  'A250_20Hz',\n",
    "#  'A1000_20Hz',\n",
    "#  'A2_50Hz',\n",
    "#  'A2_10Hz',\n",
    "#  'A5_50Hz',\n",
    "#  'A5_10Hz',\n",
    "y2 =  X_train0.loc[:,stp_n].iloc[0:X_train0.shape[0]:Dn,:].values\n",
    "##header = ge_n + cla_n + stp_n \n",
    "#header  = ge_n  + stp_n \n",
    "\n",
    "iy0=0\n",
    "mod_index = modf.index[[13]] #[3,9,13,16] [3] # modf.index #[0, 3, 9, 11, 13, 15]\n",
    "\n",
    "for ig,g in enumerate(ge_n):\n",
    "    #f, ax =plt.subplots(figsize=(16, 10))\n",
    "    ##f, ax = plt.figure()\n",
    "    ##ax = f.add_axes()\n",
    "    \n",
    "    #plt.title(stp_columns[iy]+\", model : \"+mdn+\", cv : \"+str(icv))\n",
    "    #plt.title(stp_columns[iy]+\", model : \"+mdn)\n",
    "    \n",
    "    #ax.set_title(stp_columns[i]+' '+', subclass out of bag: '+str(i0))\n",
    "    #yy1.loc[:,[stpn_test[i],stpn_pred[i]]].plot(ax=ax)\n",
    "    #plt.plot(y2.ravel(),'ob')\n",
    "    #plt.plot(y_pred.ravel(),'xr')\n",
    "    for i in mod_index:\n",
    "        md = modf.loc[i,:]\n",
    "\n",
    "\n",
    "        mdn = md['name']\n",
    "        #print(mdn)\n",
    "\n",
    "        cla_n = np.array(classes_columns_train)\n",
    "\n",
    "        cli = np.nonzero(cla_n==mdn+'__pre')[0] + np.arange(2)\n",
    "\n",
    "        mdcl2 = np.char.strip(np.array(str.split(md['classes_post'],',')))\n",
    "        mdcl1 = np.char.strip(np.array(str.split(md['classes'],','))) \n",
    "        #print(mdcl1)\n",
    "        #print(mdcl2)\n",
    "        \n",
    "        f, ax =plt.subplots(figsize=(16, 10))\n",
    "        ##f, ax = plt.figure()\n",
    "        ##ax = f.add_axes()\n",
    "    \n",
    "        #plt.title(stp_columns[iy]+\", model : \"+mdn+\", cv : \"+str(icv))\n",
    "        plt.title(g+\", model : \"+mdn+\" classes pre : \"+str(mdcl1)+\" classes post : \"+str(mdcl2))\n",
    "        \n",
    "        \n",
    "        plt.scatter(X2[:,ig],y2.ravel(),marker='o',c=X2_cl[:,cli[0]],s=10)\n",
    "        plt.scatter(X2[:,ig],y2.ravel(),marker='o',c=X2_cl[:,cli[1]],s=70,alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit set of linear models for each level of cell types hierarhy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "t1=time.time()\n",
    "\n",
    "Dn = 10\n",
    "cla_n = classes_columns_train #['ex_inh']\n",
    "ge_n = ge_columns_train #imp50.index[0:25].tolist()\n",
    "X2 = X_train0.loc[:,ge_n ]\n",
    "X2_cl = X_train0.loc[:,cla_n ]\n",
    "#i_cl = np.nonzero(X2.columns.isin(cla_n))[0]\n",
    "X2=X2.iloc[0:X_train0.shape[0]:Dn,:].values\n",
    "X2_cl=X2_cl.iloc[0:X_train0.shape[0]:Dn,:].values\n",
    "stp_n = ['A2_20Hz']\n",
    "#  'A2_20Hz',\n",
    "#  'A5_20Hz',\n",
    "#  'A250_20Hz',\n",
    "#  'A1000_20Hz',\n",
    "#  'A2_50Hz',\n",
    "#  'A2_10Hz',\n",
    "#  'A5_50Hz',\n",
    "#  'A5_10Hz',\n",
    "y2 =  X_train0.loc[:,stp_n].iloc[0:X_train0.shape[0]:Dn,:].values\n",
    "##header = ge_n + cla_n + stp_n \n",
    "#header  = ge_n  + stp_n \n",
    "\n",
    "iy0=0\n",
    "mod_index = modf.index #[3] # modf.index #[0, 3, 9, 11, 13, 15]\n",
    "Y_pred = []\n",
    "for i in mod_index:\n",
    "    md = modf.loc[i,:]\n",
    "\n",
    "\n",
    "    mdn = md['name']\n",
    "    print(mdn)\n",
    "\n",
    "    cla_n = np.array(classes_columns_train)\n",
    "\n",
    "    cli = np.nonzero(cla_n==mdn+'__pre')[0] + np.arange(2)\n",
    "\n",
    "    mdcl2 = np.char.strip(np.array(str.split(md['classes_post'],',')))\n",
    "    mdcl1 = np.char.strip(np.array(str.split(md['classes'],','))) \n",
    "    print(mdcl1)\n",
    "    print(mdcl2)\n",
    "\n",
    "    y_pred = np.zeros((0,y2.shape[1]))\n",
    "    \n",
    "    ncv = 10\n",
    "    n_samp_cv = np.rint(X2.shape[0]/ncv)\n",
    "    samples_all = np.arange(X2.shape[0])\n",
    "    r2cv = np.zeros(ncv)\n",
    "    r4cv = np.zeros(ncv)\n",
    "    for icv in range(ncv): #range(ncv):\n",
    "        samples_test = (np.arange(n_samp_cv) + icv*n_samp_cv).astype(int)\n",
    "        samples_train = np.delete(np.copy(samples_all),samples_test)\n",
    "        X2train, y2train, X2train_cl = X2[samples_train,:], y2[samples_train,:], X2_cl[samples_train,:]\n",
    "        X2test, y2test, X2test_cl = X2[samples_test,:], y2[samples_test,:], X2_cl[samples_test,:]\n",
    "        \n",
    "        model, support = fit_classes_tree(X2train,y2train,X2train_cl,cli,model_type='elastic_net',\n",
    "                                          nmin = 20, alpha=0.2, l1_ratio=0.05)\n",
    "        #model, support = fit_classes_tree(X2,y2,X2_cl,cli,model_type='ridge',nmin = 20, alpha=1, l1_ratio=0.5)\n",
    "        y_pred_i = predict_classes_tree(model,X2test,X2test_cl,cli,nout=y2.shape[1],nmin = 2)\n",
    "        y_pred = np.concatenate([y_pred, y_pred_i],axis=0)\n",
    "        \n",
    "        iy=iy0\n",
    "        if np.var(y2[:,iy])!=0:\n",
    "            nonz = y_pred_i[:,iy]!=0\n",
    "            if np.sum(nonz)>0:\n",
    "                r2cv[icv]=1 - np.mean((y2test[nonz,iy] - y_pred_i[nonz,iy])**2)/np.var(y2[:,iy])\n",
    "                r4cv[icv]=1 - np.mean((y2test[nonz,iy] - y_pred_i[nonz,iy])**2)/np.var(y2test[nonz,iy])\n",
    "    \n",
    "    Y_pred = Y_pred + [y_pred]\n",
    "    \n",
    "    f, ax =plt.subplots(figsize=(16, 10))\n",
    "    ##f, ax = plt.figure()\n",
    "    ##ax = f.add_axes()\n",
    "    \n",
    "    #plt.title(stp_columns[iy]+\", model : \"+mdn+\", cv : \"+str(icv))\n",
    "    plt.title(stp_columns[iy]+\", model : \"+mdn)\n",
    "    \n",
    "    #ax.set_title(stp_columns[i]+' '+', subclass out of bag: '+str(i0))\n",
    "    #yy1.loc[:,[stpn_test[i],stpn_pred[i]]].plot(ax=ax)\n",
    "    plt.plot(y2.ravel(),'ob')\n",
    "    plt.plot(y_pred.ravel(),'xr')\n",
    "    #plt.xticks(np.arange(len(yy1.index)), yy1.index, rotation=90)\n",
    "    iy=iy0\n",
    "    r2=0\n",
    "    r4=0\n",
    "    if np.var(y2[:,iy])!=0:\n",
    "        r2=1 - np.mean((y2[:,iy] - y_pred[:,iy])**2)/np.var(y2[:,iy])\n",
    "        r4=1 - np.var(y_pred[:,iy])/np.var(y2[:,iy])\n",
    "        \n",
    "    print(\"R**2 = \",r2,\"  Part of Var = \",r4,\"\\n\",\n",
    "          \"  R**2 cv mean = \", np.mean(r2cv),\"\\n  R**2 cv unnormed mean = \", np.mean(r4cv),\n",
    "          \"\\n  R**2 cv = \", r2cv,\"\\n  R**2 cv unnormed = \", r4cv,\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "t2 =time.time()    \n",
    "print(\"Elapsed time \",t2-t1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "predict_classes_tree(model,X2test,X2test_cl,cli,nout=y2.shape[1],nmin = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_pred_i,'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F-test of regression models with and without subclass splitting \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.graphics.api import interaction_plot, abline_plot\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "formula = \"A2_20Hz ~ C(ex_in)  +  pre__Cplx1\"\n",
    "lm = ols(formula, data=X_train0[0:X_train0.shape[0]:10]).fit()\n",
    "#print(lm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii=np.nonzero(np.array(ge_columns)=='pre__Cplx1')[0]\n",
    "ge_columns[ii[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(ge_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lm.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load important genes from iRF search\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "imp  = np.load('importance.npy')\n",
    "imp = pd.DataFrame(imp)\n",
    "imp = pd.concat([imp,pd.DataFrame(ge_columns[6:])],axis=1)\n",
    "imp.columns = ['importance', 'genes']\n",
    "imp = imp.set_index('genes')\n",
    "\n",
    "imp0  = np.load('importance0.npy')\n",
    "imp0 = pd.DataFrame(imp0)\n",
    "imp0 = pd.concat([imp0,pd.DataFrame(ge_columns[6:])],axis=1)\n",
    "imp0.columns = ['importance0', 'genes']\n",
    "imp0 = imp0.set_index('genes')\n",
    "\n",
    "\n",
    "# List of important genes : iRF\n",
    "imp = pd.concat([imp,imp0],axis=1)\n",
    "\n",
    "imp50 = imp.sort_values('importance',ascending=False).iloc[0:49,:]\n",
    "plt.plot(imp.sort_values('importance',ascending=False).loc[:,['importance','importance0']].values)\n",
    "imp50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp50.index[0:15].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pixiedust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "#%%pixie_debugger\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "d_mtree = \"/home/stepaniu/LearningX/advanced_ML/model_tree\"\n",
    "\n",
    "import sys\n",
    "# insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, d_mtree)\n",
    "\n",
    "#from models.linear_regr import linear_regr\n",
    "import models.linear_regr as lir\n",
    "\n",
    "from models.svm_regr import svm_regr\n",
    "from models.DT_sklearn_regr import DT_sklearn_regr\n",
    "\n",
    "from models.modal_clf import modal_clf\n",
    "from models.DT_sklearn_clf import DT_sklearn_clf\n",
    "\n",
    "import os, pickle, csv\n",
    "\n",
    "import src.ModelTree as mtree\n",
    "from src.utils import load_csv_data, cross_validate\n",
    "\n",
    "\n",
    "from importlib import reload\n",
    "mtree = reload(mtree)\n",
    "lir=reload(lir)\n",
    "\n",
    "\n",
    "\n",
    "# ====================\n",
    "# Settings\n",
    "# ====================\n",
    "mode = \"regr\"  # \"clf\" / \"regr\"\n",
    "save_model_tree = True  # save model tree?\n",
    "save_model_tree_predictions = True  # save model tree predictions/explanations?\n",
    "cross_validation = True  # cross-validate model tree?\n",
    "\n",
    "# ====================\n",
    "# Load data\n",
    "# ====================\n",
    "#data_csv_data_filename = os.path.join(d_mtree,\"data\", \"data_clf.csv\")\n",
    "#X, y, header = load_csv_data(data_csv_data_filename, mode=mode, verbose=True)\n",
    "\n",
    "#X2 = np.copy(X[range(0,X.shape[0],10)])\n",
    "#y2 = np.copy(y[range(0,X.shape[0],10)])\n",
    "#nsel = np.arange(1)\n",
    "#y2=y2[:,0:nannot+nsel.shape[0]]\n",
    "\n",
    "\n",
    "# add unspliting class\n",
    "#cl1 = np.zeros((X.shape[0],1))\n",
    "#cl1[0:200]=1\n",
    "#cl1[1000:1200]=2\n",
    "\n",
    "\n",
    "#X2 = np.concatenate([X2,cl1],axis=1)\n",
    "#i_cl = [X2.shape[1]-1]\n",
    "\n",
    "# ['all', 'ex_inh']\n",
    "Dn = 10\n",
    "cla_n = ['ex_inh']\n",
    "ge_n = imp50.index[0:25].tolist()\n",
    "X2 = X_train0.loc[:,ge_n + cla_n]\n",
    "i_cl = np.nonzero(X2.columns.isin(cla_n))[0]\n",
    "X2=X2.iloc[0:X_train0.shape[0]:Dn,:].values\n",
    "stp_n = ['A2_20Hz']\n",
    "# 'A2_20Hz',\n",
    "#  'A3_20Hz',\n",
    "#  'A4_20Hz',\n",
    "#  'A5_20Hz',\n",
    "#  'A250_20Hz',\n",
    "#  'A1000_20Hz',\n",
    "#  'A1_50Hz',\n",
    "#  'A2_50Hz',\n",
    "#  'A3_50Hz',\n",
    "#  'A4_50Hz',\n",
    "#  'A5_50Hz',\n",
    "#  'A250_50Hz',\n",
    "#  'A1000_50Hz',\n",
    "#  'A1_10Hz',\n",
    "#  'A2_10Hz',\n",
    "#  'A3_10Hz',\n",
    "#  'A4_10Hz',\n",
    "#  'A5_10Hz',\n",
    "#  'A250_10Hz',\n",
    "#  'A1000_10Hz']\n",
    "y2 =  X_train0.loc[:,stp_n].iloc[0:X_train0.shape[0]:Dn,:].values\n",
    "#header = ge_n + cla_n + stp_n \n",
    "header = ge_n  + stp_n \n",
    "\n",
    "t1=time.time()\n",
    "\n",
    "# Choose model\n",
    "model = lir.linear_regr()\n",
    "model.lambda_classes  = 0.1\n",
    "model.class_index = i_cl\n",
    "\n",
    "# Build model tree\n",
    "model_tree = mtree.ModelTree(model, max_depth=4, min_samples_leaf=10,\n",
    "                       search_type=\"greedy\", n_search_grid=100)\n",
    "\n",
    "# ====================\n",
    "# Train model tree\n",
    "# ====================\n",
    "print(\"Training model tree with '{}'...\".format(model.__class__.__name__))\n",
    "model_tree.fit(X2, y2, verbose=True)\n",
    "\n",
    "t2=time.time() \n",
    "\n",
    "y2_pred = model_tree.predict(X2)\n",
    "\n",
    "t3=time.time() \n",
    "\n",
    "explanations = model_tree.explain(X2, header)\n",
    "loss = model_tree.loss(X2, y2, y2_pred)\n",
    "print(\" -> loss_train={:.6f}\\n\".format(loss))\n",
    "model_tree.export_graphviz(os.path.join(\"output\", \"model_tree\"), header,\n",
    "                           export_png=True, export_pdf=False)\n",
    "\n",
    "print(\"elapsed time : \",[t2-t1,t3-t2])\n",
    "\n",
    "# ====================\n",
    "# Save model tree results\n",
    "# ====================\n",
    "if save_model_tree:\n",
    "    model_tree_filename = os.path.join(\"output\", \"model_tree.p\")\n",
    "    print(\"Saving model tree to '{}'...\".format(model_tree_filename))\n",
    "    pickle.dump(model, open(model_tree_filename, 'wb'))\n",
    "\n",
    "if save_model_tree_predictions:\n",
    "    predictions_csv_filename = os.path.join(\"output\", \"model_tree_pred.csv\")\n",
    "    print(\"Saving mode tree predictions to '{}'\".format(predictions_csv_filename))\n",
    "    with open(predictions_csv_filename, \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        field_names = [\"x2\", \"y2\", \"y2_pred\", \"explanation\"]\n",
    "        writer.writerow(field_names)\n",
    "        for (x_i, y_i, y_pred_i, exp_i) in zip(X2, y2, y2_pred, explanations):\n",
    "            field_values = [x_i, y_i, y_pred_i, exp_i]\n",
    "            writer.writerow(field_values)\n",
    "\n",
    "# ====================\n",
    "# Cross-validate model tree\n",
    "# ====================\n",
    "cross_validation = 0\n",
    "if cross_validation:\n",
    "    t5 = time.time()\n",
    "    cross_validate(model_tree, X2, y2, kfold=5, seed=1)\n",
    "    t6 = time.time()\n",
    "    print(\"time for cv : \", t6-t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation = 0\n",
    "if cross_validation:\n",
    "    t5 = time.time()\n",
    "    cross_validate(model_tree, X2, y2, kfold=5, seed=1)\n",
    "    t6 = time.time()\n",
    "    print(\"time for cv : \", t6-t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.histogram([1,1,2,2,2,3,3,4,9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossvalidation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "columns  = pd.read_hdf('stp_to_ge_all_columns.hdf')\n",
    "ge_columns_train      = columns.loc[:,'ge_columns_train'].values[0]\n",
    "annot_columns_train   = columns.loc[:,'annot_columns_train'].values[0]\n",
    "stp_columns_train     = columns.loc[:,'stp_columns_train'].values[0]\n",
    "classes_columns_train = columns.loc[:,'classes_columns_train'].values[0]\n",
    "#print(X_train.columns)\n",
    "X_train0 = pd.read_hdf('stp_to_ge_all_data.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sklearn as sk\n",
    "\n",
    "import time\n",
    "t1 = time.time()\n",
    "\n",
    "do_oob_subclasses=0\n",
    "\n",
    "#X2 = np.copy(X[range(0,X.shape[0],1)])\n",
    "#y2 = np.copy(y[range(0,X.shape[0],1)])\n",
    "#nsel = np.arange(1)\n",
    "#y2=y2[:,0:nannot+nsel.shape[0]]\n",
    "\n",
    "\n",
    "from importlib import reload\n",
    "mtree = reload(mtree)\n",
    "lir=reload(lir)\n",
    "\n",
    "\n",
    "\n",
    "# ====================\n",
    "# Settings\n",
    "# ====================\n",
    "mode = \"regr\"  # \"clf\" / \"regr\"\n",
    "save_model_tree = True  # save model tree?\n",
    "save_model_tree_predictions = True  # save model tree predictions/explanations?\n",
    "cross_validation = True  # cross-validate model tree?\n",
    "\n",
    "# ====================\n",
    "# Load data\n",
    "# ====================\n",
    "\n",
    "# ['all', 'ex_inh']\n",
    "Dn = 10\n",
    "cla_n = ['ex_inh'] #,'ex1_inh1']\n",
    "ge_n = imp50.index[0:5].tolist()\n",
    "X2 = X_train0.loc[:,annot_columns_train+ge_n + cla_n]\n",
    "i_cl = np.nonzero(X2.columns.isin(cla_n))[0] - nannot\n",
    "X2=X2.iloc[0:X_train0.shape[0]:Dn,:].values\n",
    "stp_n = ['A2_20Hz']\n",
    "# 'A2_20Hz',\n",
    "#  'A3_20Hz',\n",
    "#  'A4_20Hz',\n",
    "#  'A5_20Hz',\n",
    "#  'A250_20Hz',\n",
    "#  'A1000_20Hz',\n",
    "#  'A1_50Hz',\n",
    "#  'A2_50Hz',\n",
    "#  'A3_50Hz',\n",
    "#  'A4_50Hz',\n",
    "#  'A5_50Hz',\n",
    "#  'A250_50Hz',\n",
    "#  'A1000_50Hz',\n",
    "#  'A1_10Hz',\n",
    "#  'A2_10Hz',\n",
    "#  'A3_10Hz',\n",
    "#  'A4_10Hz',\n",
    "#  'A5_10Hz',\n",
    "#  'A250_10Hz',\n",
    "#  'A1000_10Hz']\n",
    "y2 =  X_train0.loc[:,annot_columns_train+stp_n].iloc[0:X_train0.shape[0]:Dn,:].values\n",
    "#header = ge_n + cla_n + stp_n \n",
    "header = ge_n  + stp_n \n",
    "annot_columns2 = annot_columns\n",
    " \n",
    "\n",
    "if do_oob_subclasses==0:\n",
    "    ## select datasets for training and validation\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0) \n",
    "\n",
    "    #X, y = make_regression(n_features=4, n_informative=2,\n",
    "    #                        random_state=0, shuffle=False)\n",
    "\n",
    "    kf = KFold(n_splits=10, shuffle=False)\n",
    "    i0=0\n",
    "    r6=[]\n",
    "    r7=[]\n",
    "    for train_index, test_index in kf.split(X2):\n",
    "        i0=i0+1\n",
    "        print('\\n\\n '+ str(i0))\n",
    "        #print(\"TRAIN:\", train_index,\"\\n\", \"TEST:\", test_index)\n",
    "        \n",
    "        X_train, X_test = X2[train_index], X2[test_index]\n",
    "        y_train, y_test = y2[train_index], y2[test_index]\n",
    "        \n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0) \n",
    "\n",
    "\n",
    "        met=4\n",
    "        \n",
    "        if met==1:\n",
    "            regr = RandomForestRegressor(random_state=2026,max_depth=8,min_samples_leaf=1,\n",
    "                                    n_estimators=2000, oob_score=True, n_jobs=-1, max_samples=500)\n",
    "                    # RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
    "            #           max_features='auto', max_leaf_nodes=None,\n",
    "            #           min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            #           min_samples_leaf=1, min_samples_split=2,\n",
    "            #           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
    "            #           oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
    "        elif met==2:\n",
    "            if i0==1:\n",
    "                regr = sk.linear_model.MultiTaskElasticNetCV(cv=5,n_jobs=-1)\n",
    "                regr.fit(X[:,nannot:], y[:,nannot:])\n",
    "                print(\"alpha \",regr.alpha_)\n",
    "                print(\"l1_ratio \",regr.l1_ratio_)\n",
    "        \n",
    "            regr = sk.linear_model.MultiTaskElasticNet(alpha=regr.alpha_, l1_ratio=regr.l1_ratio_,\n",
    "                                                       fit_intercept=True,\n",
    "                                                       normalize=False,\n",
    "                                 max_iter=1000, copy_X=True, tol=0.0001,\n",
    "                                                       warm_start=False, positive=False, \n",
    "                                                       random_state=None, selection='cyclic')\n",
    "        \n",
    "        elif met==3:\n",
    "            regr = sk.svm.SVR(kernel='rbf', degree=3, gamma='scale',\n",
    "                    coef0=0.0, tol=0.001, C=3.0, epsilon=0.1, shrinking=True, \n",
    "                              cache_size=200, verbose=False, max_iter=-1)\n",
    "        \n",
    "        \n",
    "        elif met==4:\n",
    "            # Choose model\n",
    "            model = lir.linear_regr()\n",
    "            model.lambda_classes  = 0.1\n",
    "            model.class_index = i_cl\n",
    "\n",
    "            # Build model tree\n",
    "            model_tree = mtree.ModelTree(model, max_depth=4, min_samples_leaf=10,\n",
    "                                   search_type=\"adaptive\", n_search_grid=20)\n",
    "\n",
    "\n",
    "        if met==3:\n",
    "            #regr.fit(X_train[:,6:], y_train[:,0:].ravel())  \n",
    "            regr.fit(X_train[:,nannot:], y_train[:,nannot:].ravel())  \n",
    "            y_pred = regr.predict(X_test[:,nannot:]) \n",
    "        elif met!=4:\n",
    "            regr.fit(X_train[:,nannot:], y_train[:,nannot:])\n",
    "            y_pred = regr.predict(X_test[:,nannot:]) \n",
    "        elif met==4:\n",
    "            # ====================\n",
    "            # Train model tree\n",
    "            # ====================\n",
    "            print(\"Training model tree with '{}'...\".format(model.__class__.__name__))\n",
    "            model_tree.fit(X_train[:,nannot:], y_train[:,nannot:], verbose=True)\n",
    "            y_pred = model_tree.predict(X_test[:,nannot:])\n",
    "\n",
    "\n",
    "        \n",
    "        if (met==3)|(met==1)|(met==4):\n",
    "            y_pred = y_pred.reshape((-1,1))\n",
    "        y_test2 = y_test[:,nannot:]\n",
    "        \n",
    "        \n",
    "        yy = np.concatenate((y_test, y_pred),axis=1)\n",
    "        stpn_test=[s+'_test' for s in stp_columns]\n",
    "        stpn_pred=[s+'_pred' for s in stp_columns]\n",
    "        yy = pd.DataFrame(yy,columns=annot_columns +stpn_test[0:len(nsel)] +stpn_pred[0:len(nsel)])\n",
    "        yy.loc[:,annot_columns2] = yy.loc[:,annot_columns2].astype(str)\n",
    "        yy1 = yy.set_index(annot_columns2)\n",
    "\n",
    "        ###yy1=yy1.loc[:,stpn_test+stpn_pred] #/yy1.loc[:,'test']*100\n",
    "        yy1=yy1.astype(float).groupby(annot_columns2).mean()\n",
    "        r3=[]\n",
    "        r5=[]\n",
    "        for i in range(y_pred.shape[1]):\n",
    "            if (i==np.array([0,2,4])).sum()>0:\n",
    "                f, ax =plt.subplots(figsize=(16, 10))\n",
    "                ##f, ax = plt.figure()\n",
    "                ##ax = f.add_axes()\n",
    "                ##plt.title(stp_columns[i])\n",
    "                ax.set_title(stp_columns[i]+' '+', subclass out of bag: '+str(i0))\n",
    "                yy1.loc[:,[stpn_test[i],stpn_pred[i]]].plot(ax=ax)\n",
    "                plt.xticks(np.arange(len(yy1.index)), yy1.index, rotation=90)\n",
    "\n",
    "\n",
    "            #ytest= yy1.loc[:,stpn_test[i]]  \n",
    "            #ypred=yy1.loc[:,stpn_pred[i]]\n",
    "            \n",
    "            ##r2=1 - (ytest.iloc[istr:] - ypred.iloc[istr:]).pow(2).mean()/ytest.iloc[istr:].var()\n",
    "            if np.var(y_test2[:,i])!=0:\n",
    "                r2=1 - np.mean((y_test2[:,i] - y_pred[:,i])**2)/np.var(y_test2[:,i])\n",
    "                r4=1 - np.mean((y_test2[:,i] - y_pred[:,i])**2)/np.var(y2[:,nannot+i])\n",
    "           # print(ytest.iloc[istr:].var())\n",
    "           # print((ytest.iloc[istr:] - ypred.iloc[istr:]).var())\n",
    "           # print(str(i) +' R^2 = '+str(r2))\n",
    "            r3 = r3 + [r2]\n",
    "            r5 = r5 + [r4]\n",
    "        print('subclass out of bag: '+str(i0))\n",
    "        print('mean R^2 = '+str(np.array(r3).mean()))\n",
    "        print('all R^2 = '+str(np.array(r3)))\n",
    "        print('mean R^2 total = '+str(np.array(r5).mean()))\n",
    "        print('all R^2 total = '+str(np.array(r5)))\n",
    "\n",
    "        r6 = r6 + [np.array(r3).mean()]\n",
    "        r7 = r7 + [np.array(r5).mean()]\n",
    "        \n",
    "        if met==1:\n",
    "            print('out of bag R^2: '+str(regr.oob_score_))\n",
    "\n",
    "    rr = np.array([r6,r7])\n",
    "    rr = pd.DataFrame(rr)\n",
    "\n",
    "\n",
    "    \n",
    "t2 = time.time()\n",
    "#print(regr.feature_importances_)\n",
    "print('Elapsed time: '+str(t2-t1))\n",
    "#print(regr.predict([[0, 0, 0, 0]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#%matplotlib notebook\n",
    "#%matplotlib ipympl\n",
    "\n",
    "#import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mpld3\n",
    "mpld3.enable_notebook()\n",
    "plt.rcParams['figure.figsize']=[14,7]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "A=plt.imread('output/model_tree.png')\n",
    "#fig=plt.figure(figsize=(18, 15), facecolor='w', edgecolor='k')\n",
    "#f, ax =plt.subplots(figsize=(15, 10))\n",
    "plt.imshow(A)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines){\n",
    "      return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "#plt.plot([1,2,3])\n",
    "\n",
    "A=plt.imread('output/model_tree.png')\n",
    "fig, ax = plt.subplots() #(figsize=(5,10))\n",
    "im = ax.imshow(A)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "sb0 = np.arange(len(classes))\n",
    "mods  = []\n",
    "mods2  = []\n",
    "for i in range(classes.shape[1]):\n",
    "    sbi = classes.loc['subclasses',i]\n",
    "    print(sbi)\n",
    "        \n",
    "    subclasses3 = subclasses2[sbi]\n",
    "    X_train, y_train =  make_data(X,y,ge_data_subcl,[subclasses3,[]])   \n",
    "    regr = sk.linear_model.MultiTaskElasticNet(alpha=0.1, l1_ratio=0.5, fit_intercept=True,\n",
    "                                                   normalize=False,max_iter=1000, copy_X=True, tol=0.0001, \n",
    "                                        warm_start=False, random_state=None, selection='cyclic')\n",
    "    regr.fit(X_train[:,nannot:], y_train[:,nannot:])\n",
    "    \n",
    "    \n",
    "    X_train2, y_train2 =  make_data(X,y,ge_data_subcl,[[],subclasses3]) \n",
    "    regr2 = sk.linear_model.MultiTaskElasticNet(alpha=0.1, l1_ratio=0.5, fit_intercept=True,\n",
    "                                                   normalize=False,max_iter=1000, copy_X=True, tol=0.0001, \n",
    "                                        warm_start=False, random_state=None, selection='cyclic')\n",
    "    regr2.fit(X_train2[:,nannot:], y_train2[:,nannot:])\n",
    "    \n",
    "    mods = mods + [regr]\n",
    "    mods2 = mods2 + [regr2]\n",
    "    #X_test = X[in_subcl,:]\n",
    "    #y_test = y[in_subcl,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "i0=1\n",
    "in_subcl = ge_data_subcl.loc[:,'subclass_pre']==subclasses2[i0]\n",
    "in_subcl = in_subcl|(ge_data_subcl.loc[:,'subclass_post']==subclasses2[i0])\n",
    "\n",
    "\n",
    "\n",
    "X_train = np.delete(X, np.nonzero(in_subcl)[0],axis=0)\n",
    "y_train = np.delete(y, np.nonzero(in_subcl)[0],axis=0)\n",
    "X_test = X[in_subcl,:]\n",
    "y_test = y[in_subcl,:]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#pd.DataFrame(y_test).to_excel('y_test.xlsx')\n",
    "pd.DataFrame(X_h).to_excel('xh.xlsx')\n",
    "pd.DataFrame(X).to_excel('x.xlsx')\n",
    "ge_data_subcl.to_excel('ge_subcl.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr # elnet a=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr # elastic net a=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr # cv 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr # cv 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr # cv 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sklearn as sk\n",
    "\n",
    "import time\n",
    "t1 = time.time()\n",
    "\n",
    "do_oob_subclasses=0\n",
    "\n",
    "X2 = np.copy(X[range(0,X.shape[0],1)])\n",
    "y2 = np.copy(y[range(0,X.shape[0],1)])\n",
    "nsel = np.arange(1)\n",
    "y2=y2[:,0:nannot+nsel.shape[0]]\n",
    "\n",
    "if do_oob_subclasses==0:\n",
    "    ## select datasets for training and validation\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0) \n",
    "\n",
    "    #X, y = make_regression(n_features=4, n_informative=2,\n",
    "    #                        random_state=0, shuffle=False)\n",
    "\n",
    "    kf = KFold(n_splits=10, shuffle=False)\n",
    "    i0=0\n",
    "    r6=[]\n",
    "    r7=[]\n",
    "    for train_index, test_index in kf.split(X2):\n",
    "        i0=i0+1\n",
    "        print('\\n\\n '+ str(i0))\n",
    "        #print(\"TRAIN:\", train_index,\"\\n\", \"TEST:\", test_index)\n",
    "        \n",
    "        X_train, X_test = X2[train_index], X2[test_index]\n",
    "        y_train, y_test = y2[train_index], y2[test_index]\n",
    "        \n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0) \n",
    "\n",
    "\n",
    "        met=1\n",
    "        \n",
    "        if met==1:\n",
    "            regr = RandomForestRegressor(random_state=2026,max_depth=8,min_samples_leaf=1,\n",
    "                                    n_estimators=2000, oob_score=True, n_jobs=-1, max_samples=500)\n",
    "                    # RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
    "            #           max_features='auto', max_leaf_nodes=None,\n",
    "            #           min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            #           min_samples_leaf=1, min_samples_split=2,\n",
    "            #           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
    "            #           oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
    "        elif met==2:\n",
    "            if i0==1:\n",
    "                regr = sk.linear_model.MultiTaskElasticNetCV(cv=5,n_jobs=-1)\n",
    "                regr.fit(X[:,nannot:], y[:,nannot:])\n",
    "                print(\"alpha \",regr.alpha_)\n",
    "                print(\"l1_ratio \",regr.l1_ratio_)\n",
    "        \n",
    "            regr = sk.linear_model.MultiTaskElasticNet(alpha=regr.alpha_, l1_ratio=regr.l1_ratio_, fit_intercept=True,\n",
    "                                                       normalize=False,\n",
    "                                 max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
    "        \n",
    "        elif met==3:\n",
    "            regr = sk.svm.SVR(kernel='rbf', degree=3, gamma='scale',\n",
    "                    coef0=0.0, tol=0.001, C=3.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1)\n",
    "        \n",
    "        if met==3:\n",
    "            #regr.fit(X_train[:,6:], y_train[:,0:].ravel())  \n",
    "            regr.fit(X_train[:,nannot:], y_train[:,nannot:].ravel())  \n",
    "        else:\n",
    "            regr.fit(X_train[:,nannot:], y_train[:,nannot:])\n",
    "\n",
    "\n",
    "        y_pred = regr.predict(X_test[:,nannot:]) \n",
    "        if (met==3)|(met==1):\n",
    "            y_pred = y_pred.reshape((-1,1))\n",
    "        y_test2 = y_test[:,nannot:]\n",
    "        \n",
    "        \n",
    "        yy = np.concatenate((y_test, y_pred),axis=1)\n",
    "        stpn_test=[s+'_test' for s in stp_columns]\n",
    "        stpn_pred=[s+'_pred' for s in stp_columns]\n",
    "        yy = pd.DataFrame(yy,columns=annot_columns +stpn_test[0:len(nsel)] +stpn_pred[0:len(nsel)])\n",
    "        yy.loc[:,annot_columns2] = yy.loc[:,annot_columns2].astype(str)\n",
    "        yy1 = yy.set_index(annot_columns2)\n",
    "\n",
    "        ###yy1=yy1.loc[:,stpn_test+stpn_pred] #/yy1.loc[:,'test']*100\n",
    "        yy1=yy1.astype(float).groupby(annot_columns2).mean()\n",
    "        r3=[]\n",
    "        r5=[]\n",
    "        for i in range(y_pred.shape[1]):\n",
    "            if (i==np.array([0,2,4])).sum()>0:\n",
    "                f, ax =plt.subplots(figsize=(16, 10))\n",
    "                ##f, ax = plt.figure()\n",
    "                ##ax = f.add_axes()\n",
    "                ##plt.title(stp_columns[i])\n",
    "                ax.set_title(stp_columns[i]+' '+', subclass out of bag: '+str(i0))\n",
    "                yy1.loc[:,[stpn_test[i],stpn_pred[i]]].plot(ax=ax)\n",
    "                plt.xticks(np.arange(len(yy1.index)), yy1.index, rotation=90)\n",
    "\n",
    "\n",
    "            #ytest= yy1.loc[:,stpn_test[i]]  \n",
    "            #ypred=yy1.loc[:,stpn_pred[i]]\n",
    "            \n",
    "            ##r2=1 - (ytest.iloc[istr:] - ypred.iloc[istr:]).pow(2).mean()/ytest.iloc[istr:].var()\n",
    "            if np.var(y_test2[:,i])!=0:\n",
    "                r2=1 - np.mean((y_test2[:,i] - y_pred[:,i])**2)/np.var(y_test2[:,i])\n",
    "                r4=1 - np.mean((y_test2[:,i] - y_pred[:,i])**2)/np.var(y[:,nannot+i])\n",
    "           # print(ytest.iloc[istr:].var())\n",
    "           # print((ytest.iloc[istr:] - ypred.iloc[istr:]).var())\n",
    "           # print(str(i) +' R^2 = '+str(r2))\n",
    "            r3 = r3 + [r2]\n",
    "            r5 = r5 + [r4]\n",
    "        print('subclass out of bag: '+str(i0))\n",
    "        print('mean R^2 = '+str(np.array(r3).mean()))\n",
    "        print('all R^2 = '+str(np.array(r3)))\n",
    "        print('mean R^2 total = '+str(np.array(r5).mean()))\n",
    "        print('all R^2 total = '+str(np.array(r5)))\n",
    "\n",
    "        r6 = r6 + [np.array(r3).mean()]\n",
    "        r7 = r7 + [np.array(r5).mean()]\n",
    "        \n",
    "        if met==1:\n",
    "            print('out of bag R^2: '+str(regr.oob_score_))\n",
    "\n",
    "    rr = np.array([r6,r7])\n",
    "    rr = pd.DataFrame(rr)\n",
    "else:\n",
    "    \n",
    "    oobs = []\n",
    "    errors = []\n",
    "    annot_columns2 = annot_columns\n",
    "    \n",
    "    r6=[]\n",
    "    r7=[]\n",
    "    for i0 in range(len(subclasses2)):\n",
    "        in_subcl = ge_data_subcl.loc[:,'subclass_pre']==subclasses2[i0]\n",
    "        in_subcl = in_subcl|(ge_data_subcl.loc[:,'subclass_post']==subclasses2[i0])\n",
    "        in_subcl = np.nonzero(in_subcl.values)[0]\n",
    "\n",
    "        X_train = np.delete(X, in_subcl,axis=0)\n",
    "        y_train = np.delete(y, in_subcl,axis=0)\n",
    "        #X_train = X[:,:]\n",
    "        #y_train = y[:,:]\n",
    "        \n",
    "        X_test = X[in_subcl,:]\n",
    "        y_test = y[in_subcl,:]\n",
    "        \n",
    "        do_this = 1\n",
    "\n",
    "        X_train, X_test0, y_train, y_test0 = train_test_split(X, y, test_size=0.25, random_state=0) \n",
    "        \n",
    "        print(X_train.shape)\n",
    "        print(X_test.shape)\n",
    "        \n",
    "        if do_this==1:\n",
    "            #if i0=0:\n",
    "            regr = RandomForestRegressor(random_state=2026,\n",
    "                                    n_estimators=500, oob_score=True, n_jobs=-1, max_samples=300)\n",
    "                \n",
    "        else:\n",
    "            regr = RandomForestRegressor(random_state=2026,\n",
    "                                    n_estimators=500, oob_score=True, n_jobs=-1)\n",
    "        \n",
    "        regr.fit(X_train[:,nannot:], y_train[:,nannot:])  \n",
    "        \n",
    "        y_pred = regr.predict(X_test[:,nannot:]) \n",
    "        y_test2 = y_test[:,nannot:]\n",
    "        \n",
    "        \n",
    "        yy = np.concatenate((y_test, y_pred),axis=1)\n",
    "        stpn_test=[s+'_test' for s in stp_columns]\n",
    "        stpn_pred=[s+'_pred' for s in stp_columns]\n",
    "        yy = pd.DataFrame(yy,columns=annot_columns +stpn_test +stpn_pred)\n",
    "        yy.loc[:,annot_columns2] = yy.loc[:,annot_columns2].astype(str)\n",
    "        yy1 = yy.set_index(annot_columns2)\n",
    "\n",
    "        ###yy1=yy1.loc[:,stpn_test+stpn_pred] #/yy1.loc[:,'test']*100\n",
    "        yy1=yy1.astype(float).groupby(annot_columns2).mean()\n",
    "        r3=[]\n",
    "        r5=[]\n",
    "        for i in range(y_pred.shape[1]):\n",
    "            if (i==np.array([0,1,9])).sum()>0:\n",
    "                f, ax =plt.subplots(figsize=(16, 10))\n",
    "                ##f, ax = plt.figure()\n",
    "                ##ax = f.add_axes()\n",
    "                ##plt.title(stp_columns[i])\n",
    "                ax.set_title(stp_columns[i]+' '+', subclass out of bag: '+subclasses2[i0])\n",
    "                yy1.loc[:,[stpn_test[i],stpn_pred[i]]].plot(ax=ax)\n",
    "                plt.xticks(np.arange(len(yy1.index)), yy1.index, rotation=90)\n",
    "\n",
    "\n",
    "            #ytest= yy1.loc[:,stpn_test[i]]  \n",
    "            #ypred=yy1.loc[:,stpn_pred[i]]\n",
    "            \n",
    "            ##r2=1 - (ytest.iloc[istr:] - ypred.iloc[istr:]).pow(2).mean()/ytest.iloc[istr:].var()\n",
    "            if np.var(y_test2[:,i])!=0:\n",
    "                r2=1 - np.mean((y_test2[:,i] - y_pred[:,i])**2)/np.var(y_test2[:,i])\n",
    "                r4=1 - np.mean((y_test2[:,i] - y_pred[:,i])**2)/np.var(y[:,nannot+i])\n",
    "           # print(ytest.iloc[istr:].var())\n",
    "           # print((ytest.iloc[istr:] - ypred.iloc[istr:]).var())\n",
    "           # print(str(i) +' R^2 = '+str(r2))\n",
    "            r3 = r3 + [r2]\n",
    "            r5 = r5 + [r4]\n",
    "        print('subclass out of bag: '+subclasses2[i0])\n",
    "        print('mean R^2 = '+str(np.array(r3).mean()))\n",
    "        print('all R^2 = '+str(np.array(r3)))\n",
    "        print('mean R^2 total = '+str(np.array(r5).mean()))\n",
    "        print('all R^2 total = '+str(np.array(r5)))\n",
    "\n",
    "        r6 = r6 + [np.array(r3).mean()]\n",
    "        r7 = r7 + [np.array(r5).mean()]\n",
    "        oobs = oobs + [regr.oob_score_]\n",
    "    print('out of bag R^2: '+str(oobs))\n",
    "    \n",
    "    rr = np.array([subclasses2,r6,r7])\n",
    "    rr = pd.DataFrame(rr)\n",
    "    \n",
    "t2 = time.time()\n",
    "#print(regr.feature_importances_)\n",
    "print('Elapsed time: '+str(t2-t1))\n",
    "#print(regr.predict([[0, 0, 0, 0]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TO DO : CHECK R^2 for ca and dg, ee cases!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regr.alpha_)\n",
    "print(regr.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr # out of 10 cross val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr # all data, 300 bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr # all data, 1000 bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr # cross val, 1000 bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr # cross val, 300 bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr # cross val, all bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr  # cross val, 93 classes bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr  # all data, classes bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca1=1.6\n",
    "ca2=2\n",
    "k=2.79\n",
    "# k=1.09\n",
    "\n",
    "cc1=(k**4 + ca1**4)/ca1**4*ca2**4/(k**4+ca2**4)\n",
    "\n",
    "k=1.09\n",
    "cc2=(k**4 + ca1**4)/ca1**4*ca2**4/(k**4+ca2**4)\n",
    "\n",
    "print(str([cc1,cc2,0.5*(cc1+cc2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "#regr = RandomForestRegressor(max_depth=15, random_state=0,\n",
    "#                            n_estimators=100)\n",
    "#regr = RandomForestRegressor(random_state=2026,\n",
    "#                            n_estimators=500)\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn import linear_model\n",
    "regr2 = linear_model.Lasso(alpha=0.01)\n",
    "regr2.fit(X_train[:,nannot:], y_train[:,nannot:])  \n",
    "#Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
    "#   normalize=False, positive=False, precompute=False, random_state=None,\n",
    "#   selection='cyclic', tol=0.0001, warm_start=False)\n",
    "\n",
    "\n",
    "#regr.fit(X_train[:,6:], y_train[:,0:].ravel())  \n",
    "#regr.fit(X_train[:,nannot:], y_train[:,nannot:])  \n",
    "# RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
    "#           max_features='auto', max_leaf_nodes=None,\n",
    "#           min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#           min_samples_leaf=1, min_samples_split=2,\n",
    "#           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
    "#           oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
    "t2 = time.time()\n",
    "#print(regr.feature_importances_)\n",
    "print('Elapsed time: '+str(t2-t1))\n",
    "#print(regr.predict([[0, 0, 0, 0]]))\n",
    "\n",
    "# some analysis of results\n",
    "y_predl = regr2.predict(X_test[:,nannot:])  \n",
    "if len(y_predl.shape)==1:\n",
    "    y_predl=y_predl.reshape((len(y_predl),1))\n",
    "    \n",
    "yyl = np.concatenate((y_test, y_predl),axis=1)\n",
    "stpn_test=[s+'_test' for s in stp_columns]\n",
    "stpn_pred=[s+'_pred' for s in stp_columns]\n",
    "yyl = pd.DataFrame(yyl,columns=annot_columns +stpn_test +stpn_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# make input and output stp labels dataset with errors\n",
    "stpn_error = [s+'_error' for s in stp_columns]\n",
    "yel = pd.DataFrame(yyl.loc[:,stpn_pred].values - yyl.loc[:,stpn_test].values, columns=stpn_error)\n",
    "yyl = pd.concat([yyl, yel.abs()], axis=1)\n",
    "\n",
    "#yy.set_index(['pre','post']).groupby()\n",
    "#yy.set_index(['pre','post']).groupby(['pre','post']).mean()\n",
    "\n",
    "# plot errors\n",
    "annot_columns2 = annot_columns\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "#yy1l=yyl.set_index(annot_columns2)\n",
    "yyl.loc[:,annot_columns2] = yyl.loc[:,annot_columns2].astype(str)\n",
    "yy1l = yyl.set_index(annot_columns2)\n",
    "yy1l.loc[:,stpn_error]=yy1l.loc[:,stpn_error].values/(yy1l.loc[:,stpn_test].values+1e-15)*100\n",
    "yy1l=yy1l.astype(float).groupby(annot_columns2).mean()\n",
    "yy1l.loc[:,stpn_error].plot(ax=ax)\n",
    "plt.xticks(np.arange(len(yy1l.index)), yy1l.index, rotation=90)\n",
    "plt.title('errors of An/A1 predicted by lasso linear regrassion, %')\n",
    "plt.ylim((0,100))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot test vs train STP data\n",
    "\n",
    "1. plot all parameters\n",
    "2. additional cross validation? - better estimate of generalization error?\n",
    "3. check dependence on rf parameters\n",
    "4. dependence on % of training data\n",
    "5. lists of most significant genes\n",
    "6. check gs assignment - plot some figures\n",
    "7. project on hipp-ds -> predict stp, compare\n",
    "8. an:a1 vs TM-par\n",
    "## \n",
    "## new (sep 2019):\n",
    "9. compare with linear fit\n",
    "10. iRFcompare small gs predictions\n",
    "11. compare with factors predicting neuron types, predict n-types using small ds\n",
    "12. R^2\n",
    "13. compare with list of STP molecular factors\n",
    "14. relative errors\n",
    "15. some graphs for particular factors?\n",
    "16. iRF\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#annot_columns +stpn_test +stpn_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF case\n",
    "y_pred = regr.predict(X_test[:,nannot:]) \n",
    "if len(y_pred.shape)==1:\n",
    "    y_pred=y_pred.reshape((len(y_pred),1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "#print('Mean Absolute Error:', metrics.mean_absolute_error(y_test[:,2:].ravel(), y_pred))  \n",
    "#print('Mean Squared Error:', metrics.mean_squared_error(y_test[:,2:].ravel(), y_pred))  \n",
    "#print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test[:,2:].ravel(), y_pred))) \n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test[:,nannot:], y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test[:,nannot:], y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test[:,nannot:], y_pred))) \n",
    "\n",
    "\n",
    "#y_pred2 = y_pred.reshape((len(y_pred),nstp))\n",
    "yy = np.concatenate((y_test, y_pred),axis=1)\n",
    "stpn_test=[s+'_test' for s in stp_columns]\n",
    "stpn_pred=[s+'_pred' for s in stp_columns]\n",
    "yy = pd.DataFrame(yy,columns=annot_columns +stpn_test +stpn_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# make input and output stp labels dataset with errors\n",
    "stpn_error = [s+'_error' for s in stp_columns]\n",
    "ye = pd.DataFrame(yy.loc[:,stpn_pred].values - yy.loc[:,stpn_test].values, columns=stpn_error)\n",
    "yy = pd.concat([yy, ye.abs()], axis=1)\n",
    "\n",
    "#yy.set_index(['pre','post']).groupby()\n",
    "#yy.set_index(['pre','post']).groupby(['pre','post']).mean()\n",
    "\n",
    "# plot errors\n",
    "annot_columns2 = annot_columns\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "#yy1=yy.set_index(annot_columns2)\n",
    "yy.loc[:,annot_columns2] = yy.loc[:,annot_columns2].astype(str)\n",
    "yy1 = yy.set_index(annot_columns2)\n",
    "yy1.loc[:,stpn_error]=yy1.loc[:,stpn_error].values/(yy1.loc[:,stpn_test].values+1e-15)*100\n",
    "yy1=yy1.astype(float).groupby(annot_columns2).mean()\n",
    "yy1.loc[:,stpn_error].plot(ax=ax)\n",
    "plt.xticks(np.arange(len(yy1.index)), yy1.index, rotation=90)\n",
    "plt.title('errors of An/A1 predicted by RF, %')\n",
    "plt.ylim((0,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy.to_excel('all_tested_rf_280gs_with_PPR.xlsx')\n",
    "yy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Linear model case\n",
    "\n",
    "#plot predicted and real (test) stp data\n",
    "#plt.figure()\n",
    "#yy1l=yyl.set_index(annot_columns2)\n",
    "\n",
    "#yyl.loc[:,annot_columns2] = yyl.loc[:,annot_columns2].astype(str)\n",
    "yy1l = yyl.set_index(annot_columns2)\n",
    "#yy1=yy1.loc[:,stpn_test+stpn_pred] #/yy1.loc[:,'test']*100\n",
    "yy1l=yy1l.astype(float).groupby(annot_columns2).mean()\n",
    "\n",
    "r3=[]\n",
    "for i in range(nstp):\n",
    "    f, ax =plt.subplots(figsize=(16, 10))\n",
    "    #f, ax = plt.figure()\n",
    "    #ax = f.add_axes()\n",
    "    #plt.title(stp_columns[i])\n",
    "    ax.set_title(stp_columns[i])\n",
    "    yy1l.loc[:,[stpn_test[i],stpn_pred[i]]].plot(ax=ax)\n",
    "    plt.xticks(np.arange(len(yy1l.index)), yy1l.index, rotation=90)\n",
    "    if i==0:\n",
    "        istr=0\n",
    "    else:\n",
    "        istr=0 #11 - if ppr included\n",
    "    \n",
    "    ytest= yy1l.loc[:,stpn_test[i]]  \n",
    "    ypred=yy1l.loc[:,stpn_pred[i]]\n",
    "    \n",
    "    if ytest.iloc[istr:].var()!=0:\n",
    "        r2=1 - (ytest.iloc[istr:] - ypred.iloc[istr:]).pow(2).mean()/(ytest.iloc[istr:].var() )\n",
    "    print(str(i) +' R^2 = '+str(r2))\n",
    "    r3 = r3 + [r2]\n",
    "\n",
    "print('mean R^2 = '+str(np.array(r3).mean()))\n",
    "#print(yy1.mean())\n",
    "yy1l.loc[:,stpn_test +stpn_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RF model case\n",
    "\n",
    "#plot predicted and real (test) stp data\n",
    "#plt.figure()\n",
    "\n",
    "#yy.loc[:,annot_columns2] = yy.loc[:,annot_columns2].astype(str)\n",
    "#yy1 = yy.set_index(annot_columns2)\n",
    "\n",
    "##yy1=yy1.loc[:,stpn_test+stpn_pred] #/yy1.loc[:,'test']*100\n",
    "yy1=yy1.astype(float).groupby(annot_columns2).mean()\n",
    "r3=[]\n",
    "for i in range(0,nstp):\n",
    "    f, ax =plt.subplots(figsize=(16, 10))\n",
    "    #f, ax = plt.figure()\n",
    "    #ax = f.add_axes()\n",
    "    #plt.title(stp_columns[i])\n",
    "    ax.set_title(stp_columns[i])\n",
    "    yy1.loc[:,[stpn_test[i],stpn_pred[i]]].plot(ax=ax)\n",
    "    plt.xticks(np.arange(len(yy1.index)), yy1.index, rotation=90)\n",
    "    if i==0:\n",
    "        istr=0\n",
    "    else:\n",
    "        istr=0 # 11 - if ppr data excluded\n",
    "        \n",
    "    ytest= yy1.loc[:,stpn_test[i]]  \n",
    "    ypred=yy1.loc[:,stpn_pred[i]]\n",
    "    #r2=1 - (ytest.iloc[istr:] - ypred.iloc[istr:]).pow(2).mean()/ytest.iloc[istr:].var()\n",
    "    if ytest.iloc[istr:].var()!=0:\n",
    "        r2=1 - (ytest.iloc[istr:] - ypred.iloc[istr:]).var()/ytest.iloc[istr:].var()\n",
    "    print(ytest.iloc[istr:].var())\n",
    "    print((ytest.iloc[istr:] - ypred.iloc[istr:]).var())\n",
    "    print(str(i) +' R^2 = '+str(r2))\n",
    "    r3 = r3 + [r2]\n",
    "\n",
    "print('mean R^2 = '+str(np.array(r3).mean()))\n",
    "\n",
    "#f, ax =plt.subplots(figsize=(10, 10))\n",
    "#yy1.loc[:,[stpn_test[i],stpn_pred[i]]].plot(ax=ax)\n",
    "\n",
    "#print(yy1.mean())\n",
    "yy1.loc[:,stpn_test +stpn_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yy1.to_excel('results_RF_10gs_without_PPR.xlsx')\n",
    "yy1.to_excel('results_RF_10gs_with_PPR.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at most important genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear model case\n",
    "\n",
    "# set of most significant genes\n",
    " #fil = regr2.feature_importances_\n",
    "\n",
    "#plt.plot(fi)\n",
    "fil = regr2.coef_\n",
    "fil = np.absolute(fil)\n",
    "#fil = np.sum(fil,axis=0).transpose()\n",
    "fil = fil[0,:].transpose()\n",
    "fil.shape\n",
    "\n",
    "ifi = np.nonzero(fil)[0]\n",
    "fi2l = fil[ifi]\n",
    "ifi2 = np.argsort(fi2l)\n",
    "l=len(fi2l)\n",
    "#print(np.concatenate((ifi2.reshape((l,1)),fi2[ifi2].reshape((l,1))),axis=1))\n",
    "\n",
    "print(len(ge_columns))\n",
    "print(len(ge_data.columns))\n",
    "print(fil.shape)\n",
    "print(ge_columns[0:6])\n",
    "\n",
    "fi2l = pd.DataFrame(fil,columns=['importance'],index = ge_columns[6:])\n",
    "fi2l = pd.concat([fi2l, pd.DataFrame(np.repeat('post',len(fi2l.index)),columns = ['compartment'],index=fi2l.index)],axis=1)\n",
    "#fi2.loc[np.arange(len(fi2.index))<len(pregs),'compartment']='pre'\n",
    "fi2l.loc[fi2l.index.str.contains('pre'),'compartment']='pre'\n",
    "\n",
    "fi2l.index =  [id[5:] for id in fi2l.index ]\n",
    "\n",
    "fi2l=fi2l.sort_values('importance',ascending=False)\n",
    "best10 = fi2l.iloc[0:10,:]\n",
    "best20 = fi2l.iloc[0:20,:]\n",
    "best50 = fi2l.iloc[0:50,:]\n",
    "\n",
    "\n",
    "## linear case\n",
    "#fi2l.iloc[0:50,:] # A1:5, without averaging, 500 trees, with Yuste, 384gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi2l.iloc[0:50,:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi2l.iloc[0:50,:] # A1:5, without averaging, 500 trees,  50 best genes found from 280 gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi2l.iloc[0:50,:] # A1:5, without averaging, 500 trees, with Yuste, 20 best genes found from 280 gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF model case\n",
    "\n",
    "# set of most significant genes\n",
    "fi = regr.feature_importances_\n",
    "#plt.plot(fi)\n",
    "\n",
    "ifi = np.nonzero(fi)[0]\n",
    "fi2 = fi[ifi]\n",
    "ifi2 = np.argsort(fi2)\n",
    "l=len(fi2)\n",
    "#print(np.concatenate((ifi2.reshape((l,1)),fi2[ifi2].reshape((l,1))),axis=1))\n",
    "\n",
    "print(len(ge_columns))\n",
    "print(len(ge_data.columns))\n",
    "print(fi.shape)\n",
    "print(ge_columns[0:6])\n",
    "\n",
    "fi2 = pd.DataFrame(fi,columns=['importance'],index = ge_columns[6:])\n",
    "fi2 = pd.concat([fi2, pd.DataFrame(np.repeat('post',len(fi2.index)),columns = ['compartment'],index=fi2.index)],axis=1)\n",
    "#fi2.loc[np.arange(len(fi2.index))<len(pregs),'compartment']='pre'\n",
    "fi2.loc[fi2.index.str.contains('pre'),'compartment']='pre'\n",
    "\n",
    "fi2.index =  [id[5:] for id in fi2.index ]\n",
    "\n",
    "fi2=fi2.sort_values('importance',ascending=False)\n",
    "best10 = fi2.iloc[0:10,:]\n",
    "best20 = fi2.iloc[0:20,:]\n",
    "best50 = fi2.iloc[0:50,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi2.iloc[0:50,:] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi2.to_excel('best_genes_hipp.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with VISp trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "RF_file = open('RF_290_hipp.pickle', mode='wb')\n",
    "pickle.dump(regr,RF_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_file = open('RF_290_visp.pickle', mode='rb')\n",
    "regr_visp = pickle.load(RF_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF case\n",
    "y_pred = regr_visp.predict(X_test[:,nannot:]) \n",
    "if len(y_pred.shape)==1:\n",
    "    y_pred=y_pred.reshape((len(y_pred),1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "#print('Mean Absolute Error:', metrics.mean_absolute_error(y_test[:,2:].ravel(), y_pred))  \n",
    "#print('Mean Squared Error:', metrics.mean_squared_error(y_test[:,2:].ravel(), y_pred))  \n",
    "#print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test[:,2:].ravel(), y_pred))) \n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test[:,nannot:], y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test[:,nannot:], y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test[:,nannot:], y_pred))) \n",
    "\n",
    "\n",
    "#y_pred2 = y_pred.reshape((len(y_pred),nstp))\n",
    "yy = np.concatenate((y_test, y_pred),axis=1)\n",
    "stpn_test=[s+'_test' for s in stp_columns]\n",
    "stpn_pred=[s+'_pred' for s in stp_columns]\n",
    "yy = pd.DataFrame(yy,columns=annot_columns +stpn_test +stpn_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# make input and output stp labels dataset with errors\n",
    "stpn_error = [s+'_error' for s in stp_columns]\n",
    "ye = pd.DataFrame(yy.loc[:,stpn_pred].values - yy.loc[:,stpn_test].values, columns=stpn_error)\n",
    "yy = pd.concat([yy, ye.abs()], axis=1)\n",
    "\n",
    "#yy.set_index(['pre','post']).groupby()\n",
    "#yy.set_index(['pre','post']).groupby(['pre','post']).mean()\n",
    "\n",
    "# plot errors\n",
    "annot_columns2 = annot_columns\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 18))\n",
    "\n",
    "yy1=yy.set_index(annot_columns2)\n",
    "yy1.loc[:,stpn_error]=yy1.loc[:,stpn_error].values/(yy1.loc[:,stpn_test].values+1e-15)*100\n",
    "yy1=yy1.astype(float).groupby(annot_columns2).mean()\n",
    "yy1.loc[:,stpn_error].plot(ax=ax)\n",
    "plt.xticks(np.arange(len(yy1.index)), yy1.index, rotation=90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RF model case\n",
    "\n",
    "#plot predicted and real (test) stp data\n",
    "#plt.figure()\n",
    "yy1=yy.set_index(annot_columns2)\n",
    "#yy1=yy1.loc[:,stpn_test+stpn_pred] #/yy1.loc[:,'test']*100\n",
    "yy1=yy1.astype(float).groupby(annot_columns2).mean()\n",
    "r3=[]\n",
    "for i in range(nstp):\n",
    "    f, ax =plt.subplots(figsize=(10, 10))\n",
    "    #f, ax = plt.figure()\n",
    "    #ax = f.add_axes()\n",
    "    #plt.title(stp_columns[i])\n",
    "    ax.set_title(stp_columns[i])\n",
    "    yy1.loc[:,[stpn_test[i],stpn_pred[i]]].plot(ax=ax)\n",
    "    plt.xticks(np.arange(len(yy1.index)), yy1.index, rotation=90)\n",
    "    if i==0:\n",
    "        istr=0\n",
    "    else:\n",
    "        istr=0 # 11 - if ppr data excluded\n",
    "        \n",
    "    ytest= yy1.loc[:,stpn_test[i]]  \n",
    "    ypred=yy1.loc[:,stpn_pred[i]]\n",
    "    #r2=1 - (ytest.iloc[istr:] - ypred.iloc[istr:]).pow(2).mean()/ytest.iloc[istr:].var()\n",
    "    r2=1 - (ytest.iloc[istr:] - ypred.iloc[istr:]).var()/ytest.iloc[istr:].var()\n",
    "    print(ytest.iloc[istr:].var())\n",
    "    print((ytest.iloc[istr:] - ypred.iloc[istr:]).var())\n",
    "    print(str(i) +' R^2 = '+str(r2))\n",
    "    r3 = r3 + [r2]\n",
    "\n",
    "print('mean R^2 = '+str(np.array(r3).mean()))\n",
    "\n",
    "f, ax =plt.subplots(figsize=(10, 10))\n",
    "yy1.loc[:,[stpn_test[i],stpn_pred[i]]].plot(ax=ax)\n",
    "\n",
    "#print(yy1.mean())\n",
    "yy1.loc[:,stpn_test +stpn_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi2.iloc[0:50,:] # A1:5, without averaging, 500 trees,  10 from 280 gs R^2 = 0.86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fi2.to_excel('10_selected_from_280_gs.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START!\n",
    "## iterative RF - search for stable interations of features\n",
    "\n",
    "## run python version of iRF :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # PYTHON iRF WORKS ONLY WITH BYNARY CLASSIFICATION DATA!!! - USE R iRF2.0 for multiclasses (or 1D regression)!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "from irf import irf_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_samples = 5000\n",
    "n_features = 500\n",
    "X_train = np.random.uniform(low=0, high=1, size=(n_samples, n_features))\n",
    "y_train = np.random.choice([0, 1], size=(n_samples,), p=[.5, .5])\n",
    "X_test = np.random.uniform(low=0, high=1, size=(n_samples, n_features))\n",
    "y_test = np.random.choice([0, 1], size=(n_samples,), p=[.5, .5])\n",
    "# The second feature (which is indexed by 1) is very important\n",
    "X_train[:, 1] = X_train[:, 1] + y_train\n",
    "X_test[:, 1] = X_test[:, 1] + y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "rng = np.random.RandomState(10)  # deterministic random data\n",
    "a = np.hstack((rng.normal(size=1000),  rng.normal(loc=5, scale=2, size=1000)))\n",
    "print(a.shape)\n",
    "plt.hist(a, bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Histogram with 'auto' bins\")\n",
    "plt.show()\n",
    "\n",
    "bins =[0.45, 0.75,1.0, 1.5, 5]\n",
    "y_train[:,nannot]\n",
    "in_bins =  np.histogram(y_train[:,nannot],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "# embedding = umap.UMAP().fit_transform(digits.data)\n",
    "\n",
    "\n",
    "\n",
    "embedding = umap.UMAP(n_neighbors=5,\n",
    "                      min_dist=0.3,\n",
    "                      metric='correlation').fit_transform(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import louvain\n",
    "import pyclustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyclustering.cluster import cluster_visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy=y_train[:,nannot:]\n",
    "y_u = UMAP(spread=2).fit_transform(yy)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "y_u2 = TSNE(n_components=2).fit_transform(yy)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10), facecolor='w', edgecolor='k')\n",
    "plt.scatter(y_u[:, 0], y_u[:, 1],\n",
    "                edgecolors='none', c=\"#ff4400\", s=10)\n",
    "\n",
    "plt.axis('off')\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "fig1 = plt.figure(figsize=(10, 10), facecolor='w', edgecolor='k')\n",
    "plt.scatter(y_u2[:, 0], y_u2[:, 1],\n",
    "                edgecolors='none', c=\"#ff4400\", s=10)\n",
    "\n",
    "plt.axis('off')\n",
    "fig1.set_tight_layout(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"#991f1f\", \"#ff9999\", \"#ff4400\", \"#ff8800\", \"#664014\", \"#665c52\",\n",
    "          \"#cca300\", \"#f1ff33\", \"#b4cca3\", \"#0e6600\", \"#33ff4e\", \"#00ccbe\",\n",
    "          \"#0088ff\", \"#7aa6cc\", \"#293966\", \"#0000ff\", \"#9352cc\", \"#cca3c9\", \"#cc2996\"]\n",
    "\n",
    "fig = plt.figure(figsize=(15, 15), facecolor='w', edgecolor='k')\n",
    "for i, k in enumerate(list(set(df_aba_vis_in_c_s_vis.loc[:,'cluster_label'] ))):\n",
    "    i_clu = df_aba_vis_in_c_s_vis.index[df_aba_vis_in_c_s_vis.loc[:,'cluster_label'].isin([k])]\n",
    "    c =df_aba_vis_in_c_s_vis.loc[i_clu,'cluster_color'].iloc[0]\n",
    "    \n",
    "    plt.scatter(latent_u_vis[i_clu, 0], latent_u_vis[i_clu, 1], label=k,\n",
    "                edgecolors='none', c=c, s=15)\n",
    "    plt.legend(loc=9, borderaxespad=0, fontsize='small', markerscale=3)\n",
    "\n",
    "plt.axis('off')\n",
    "fig.set_tight_layout(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run R iRF in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "np.save('X.npy',X[:,nannot:].astype(float))\n",
    "np.save('y.npy',y[:,nannot:].astype(float))\n",
    "\n",
    "np.save('X_train.npy',X_train[:,nannot:].astype(float))\n",
    "np.save('X_test.npy',X_test[:,nannot:].astype(float))\n",
    "np.save('y_train.npy',y_train[:,nannot:].astype(float))\n",
    "np.save('y_test.npy',y_test[:,nannot:].astype(float))\n",
    "\n",
    "#np.savetxt('X_train.csv',X_train[:,nannot:].astype(),fmt='%10.5f', delimiter=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rpy2.robjects import r, pandas2ri\n",
    "import rpy2.robjects.numpy2ri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rpy2.robjects.packages import importr\n",
    "irfr = importr('iRF')\n",
    "bs = importr('base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iRFr=r('iRF')\n",
    "iRFr = irfr.iRF\n",
    "X_test_r=X_test[:,nannot:]\n",
    "X_train_r=X_train[:,nannot:]\n",
    "y_train_r = y_train[:,nannot:]\n",
    "y_test_r = y_test[:,nannot:]\n",
    "\n",
    "import rpy2.robjects as ro\n",
    "import rpy2.robjects.numpy2ri\n",
    "rpy2.robjects.numpy2ri.activate()\n",
    "\n",
    "X_test_r2 = ro.r.matrix(X_test_r, nrow=X_test_r.shape[0], ncol=X_test_r.shape[1])\n",
    "X_train_r2 = ro.r.matrix(X_train_r, nrow=X_train_r.shape[0], ncol=X_train_r.shape[1])\n",
    "y_test_r2 = ro.r.matrix(y_test_r, nrow=y_test_r.shape[0], ncol=y_test_r.shape[1])\n",
    "y_train_r2 = ro.r.matrix(y_train_r, nrow=y_train_r.shape[0], ncol=y_train_r.shape[1])\n",
    "ro.r.assign(\"X_test_r\", X_test_r2 )\n",
    "ro.r.assign(\"X_train_r\", X_train_r2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = irfr.iRF(x=X_train_r2, y=y_train_r2, xtest=X_test_r2, ytest=y_test_r2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYTHON iRF WORKS ONLY WITH BYNARY CLASSIFICATION DATA!!! - USE R iRF2.0 for multiclasses (or 1D regression)!!!\n",
    "\n",
    "from irf import irf_utils\n",
    "import time\n",
    "t1 = time.time()\n",
    "\n",
    "all_rf_weights, all_K_iter_rf_data, \\\n",
    "    all_rf_bootstrap_output, all_rit_bootstrap_output, \\\n",
    "    stability_score = irf_utils.run_iRF(X_train=X_train[:,nannot:],\n",
    "                                        X_test=X_test[:,nannot:],\n",
    "                                        y_train=y_train[:,-1],\n",
    "                                        y_test=y_test[:,-1],\n",
    "                                        K=5,                          # number of iteration\n",
    "                                        n_estimators=20,              # number of trees in the forest\n",
    "                                        B=30,\n",
    "                                        random_state_classifier=2026, # random seed\n",
    "                                        propn_n_samples=.2,\n",
    "                                        bin_class_type=1,\n",
    "                                        M=20,\n",
    "                                        max_depth=5,\n",
    "                                        noisy_split=False,\n",
    "                                        num_splits=2,\n",
    "                                        n_estimators_bootstrap=5)\n",
    "\n",
    "\n",
    "t2 = time.time()\n",
    "#print(regr.feature_importances_)\n",
    "print('Elapsed time: '+str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = iRFr(x=X_train_r, \n",
    "         y=y_train_r, \n",
    "         xtest=X_test_r, \n",
    "         ytest=y_test_r, \n",
    "         n.iter=5, \n",
    "         n.core=8,\n",
    "         select.iter = TRUE,\n",
    "         n.bootstrap=10\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run iRF in R studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.save('X.npy',X[:,nannot:].astype(float))\n",
    "np.save('y.npy',y[:,nannot:].astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HERE clustering by xmeans_STP.ipynb  should be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yc  = np.load('y_clusters.npy')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, yc, test_size=0.25, random_state=0)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(set(yc[:,-1]))\n",
    "print( len(list( set(yc[:,-1]) )))\n",
    "print(yc.shape)\n",
    "\n",
    "np.save('Xtrain.npy',X_train[:,nannot:].astype(float))\n",
    "np.save('yctrain.npy',np.rint(y_train[:,-1]).astype(float))\n",
    "np.save('Xtest.npy',X_test[:,nannot:].astype(float))\n",
    "np.save('yctest.npy',np.rint(y_test[:,-1]).astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HERE do iRF.Rmd comutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "imp  = np.load('importance.npy')\n",
    "imp = pd.DataFrame(imp)\n",
    "imp = pd.concat([imp,pd.DataFrame(ge_columns[6:])],axis=1)\n",
    "imp.columns = ['importance', 'genes']\n",
    "imp = imp.set_index('genes')\n",
    "\n",
    "imp0  = np.load('importance0.npy')\n",
    "imp0 = pd.DataFrame(imp0)\n",
    "imp0 = pd.concat([imp0,pd.DataFrame(ge_columns[6:])],axis=1)\n",
    "imp0.columns = ['importance0', 'genes']\n",
    "imp0 = imp0.set_index('genes')\n",
    "\n",
    "\n",
    "# List of important genes : iRF\n",
    "imp = pd.concat([imp,imp0],axis=1)\n",
    "\n",
    "imp50 = imp.sort_values('importance',ascending=False).iloc[0:49,:]\n",
    "plt.plot(imp.sort_values('importance',ascending=False).loc[:,['importance','importance0']].values)\n",
    "imp50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#annot_columns + ge_columns[6:]\n",
    "imp500 = imp.sort_values('importance0',ascending=False).iloc[0:50,:]\n",
    "imp500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge_columns[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### # train subclasses with best iRF subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge_data_subcl"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ge_columns_x = ge_columns[6:]\n",
    "ge_columns_x = pd.DataFrame(ge_columns_x)\n",
    "\n",
    "nsel=10\n",
    "print(imp50.index[0:nsel-1])\n",
    "selected_columns_x = ge_columns_x.isin(imp50.index[0:nsel-1])\n",
    "selected_columns_x =ge_columns_x[selected_columns_x.values].index.values \n",
    "print(selected_columns_x)\n",
    "selected_columns_x2 = np.concatenate([np.arange(nannot),selected_columns_x+ nannot])\n",
    "selected_columns_x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "t1 = time.time()\n",
    "\n",
    "ge_columns_x = ge_columns[6:]\n",
    "ge_columns_x = pd.DataFrame(ge_columns_x)\n",
    "\n",
    "nsel=25\n",
    "print(imp50.index[0:nsel-1])\n",
    "selected_columns_x = ge_columns_x.isin(imp50.index[0:nsel-1])\n",
    "selected_columns_x =ge_columns_x[selected_columns_x.values].index.values \n",
    "print(selected_columns_x)\n",
    "selected_columns_x2 = np.concatenate([np.arange(nannot),selected_columns_x+ nannot])\n",
    "X_selected = X[:,selected_columns_x2]\n",
    "\n",
    "do_oob_subclasses=1\n",
    "\n",
    "subclasses2 = pd.unique(subclasses2 )\n",
    "print(subclasses2) \n",
    "\n",
    "if do_oob_subclasses==1:\n",
    "\n",
    "    \n",
    "    oobs = []\n",
    "    errors = []\n",
    "    annot_columns2 = annot_columns\n",
    "    \n",
    "    r6=[]\n",
    "    r7=[]\n",
    "    for i0 in range(len(subclasses2)):\n",
    "        in_subcl = ge_data_subcl.loc[:,'subclass_pre']==subclasses2[i0]\n",
    "        in_subcl = in_subcl|(ge_data_subcl.loc[:,'subclass_post']==subclasses2[i0])\n",
    "        in_subcl = np.nonzero(in_subcl.values)[0]\n",
    "\n",
    "        X_train = np.delete(X_selected, in_subcl,axis=0)\n",
    "        y_train = np.delete(y, in_subcl,axis=0)\n",
    "        #X_train = X[:,:]\n",
    "        #y_train = y[:,:]\n",
    "        \n",
    "        X_test = X_selected[in_subcl,:]\n",
    "        y_test = y[in_subcl,:]\n",
    "        \n",
    "        \n",
    "\n",
    "        X_train, X_test0, y_train, y_test0 = train_test_split(X_selected, y, test_size=0.25, random_state=0) \n",
    "        \n",
    "        print(X_train.shape)\n",
    "        print(X_test.shape)\n",
    "        \n",
    "        regr = RandomForestRegressor(random_state=2026,\n",
    "                                    n_estimators=2500, oob_score=True, n_jobs=-1, max_samples=93)\n",
    "        regr.fit(X_train[:,nannot:], y_train[:,nannot:])  \n",
    "        \n",
    "        y_pred = regr.predict(X_test[:,nannot:]) \n",
    "        y_test2 = y_test[:,nannot:]\n",
    "        \n",
    "        \n",
    "        yy = np.concatenate((y_test, y_pred),axis=1)\n",
    "        stpn_test=[s+'_test' for s in stp_columns]\n",
    "        stpn_pred=[s+'_pred' for s in stp_columns]\n",
    "        yy = pd.DataFrame(yy,columns=annot_columns +stpn_test +stpn_pred)\n",
    "        yy.loc[:,annot_columns2] = yy.loc[:,annot_columns2].astype(str)\n",
    "        yy1 = yy.set_index(annot_columns2)\n",
    "\n",
    "        ###yy1=yy1.loc[:,stpn_test+stpn_pred] #/yy1.loc[:,'test']*100\n",
    "        yy1=yy1.astype(float).groupby(annot_columns2).mean()\n",
    "        r3=[]\n",
    "        r5=[]\n",
    "        for i in range(y_pred.shape[1]):\n",
    "            if (i==np.array([0,1,9])).sum()>0:\n",
    "                f, ax =plt.subplots(figsize=(16, 10))\n",
    "                ##f, ax = plt.figure()\n",
    "                ##ax = f.add_axes()\n",
    "                ##plt.title(stp_columns[i])\n",
    "                ax.set_title(stp_columns[i]+' '+', subclass out of bag: '+subclasses2[i0])\n",
    "                yy1.loc[:,[stpn_test[i],stpn_pred[i]]].plot(ax=ax)\n",
    "                plt.xticks(np.arange(len(yy1.index)), yy1.index, rotation=90)\n",
    "\n",
    "\n",
    "            #ytest= yy1.loc[:,stpn_test[i]]  \n",
    "            #ypred=yy1.loc[:,stpn_pred[i]]\n",
    "            \n",
    "            ##r2=1 - (ytest.iloc[istr:] - ypred.iloc[istr:]).pow(2).mean()/ytest.iloc[istr:].var()\n",
    "            if np.var(y_test2[:,i])!=0:\n",
    "                r2=1 - np.var(y_test2[:,i] - y_pred[:,i])/np.var(y_test2[:,i])\n",
    "                r4=1 - np.var(y_test2[:,i] - y_pred[:,i])/np.var(y[:,nannot+i])\n",
    "           # print(ytest.iloc[istr:].var())\n",
    "           # print((ytest.iloc[istr:] - ypred.iloc[istr:]).var())\n",
    "           # print(str(i) +' R^2 = '+str(r2))\n",
    "            r3 = r3 + [r2]\n",
    "            r5 = r5 + [r4]\n",
    "        print('subclass out of bag: '+subclasses2[i0])\n",
    "        print('mean R^2 = '+str(np.array(r3).mean()))\n",
    "        print('all R^2 = '+str(np.array(r3)))\n",
    "        print('mean R^2 total = '+str(np.array(r5).mean()))\n",
    "        print('all R^2 total = '+str(np.array(r5)))\n",
    "\n",
    "        r6 = r6 + [np.array(r3).mean()]\n",
    "        r7 = r7 + [np.array(r5).mean()]\n",
    "        oobs = oobs + [regr.oob_score_]\n",
    "    print('out of bag R^2: '+str(oobs))\n",
    "    \n",
    "    rr = np.array([subclasses2,r6,r7])\n",
    "    rr = pd.DataFrame(rr)\n",
    "    \n",
    "t2 = time.time()\n",
    "#print(regr.feature_importances_)\n",
    "print('Elapsed time: '+str(t2-t1))\n",
    "#print(regr.predict([[0, 0, 0, 0]]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def rfr_model(X, y):\n",
    "    # Perform Grid-Search\n",
    "    gsc = GridSearchCV(\n",
    "        estimator=RandomForestRegressor(),\n",
    "        param_grid={\n",
    "            'max_depth': range(3,7),\n",
    "            'n_estimators': (10, 50, 100, 1000),\n",
    "        },\n",
    "        cv=5, scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)\n",
    "    \n",
    "    grid_result = gsc.fit(X, y)\n",
    "    best_params = grid_result.best_params_\n",
    "    \n",
    "    rfr = RandomForestRegressor(max_depth=best_params[\"max_depth\"], n_estimators=best_params[\"n_estimators\"],                               random_state=False, verbose=False)\n",
    "    # Perform K-Fold CV\n",
    "    scores = cross_val_score(rfr, X, y, cv=10, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
